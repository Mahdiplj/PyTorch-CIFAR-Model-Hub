{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IGml3SMd6ID"
      },
      "source": [
        "# How to Train Your ResNet - 8: Bag of Tricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tac7LB2td_Cs"
      },
      "source": [
        "In the [final post of the series](https://myrtle.ai/how-to-train-your-resnet-8-bag-of-tricks/) we come full circle, speeding up our single-GPU training implementation to take on a field of multi-GPU competitors. Whilst we've been otherwise occupied - investigating [hyperparameter tuning](https://myrtle.ai/learn/how-to-train-your-resnet-5-hyperparameters/), [weight decay](https://myrtle.ai/learn/how-to-train-your-resnet-6-weight-decay/) and [batch norm](https://myrtle.ai/learn/how-to-train-your-resnet-7-batch-norm/) - our entry for training CIFAR10 to 94% test accuracy has slipped five (!) places on the DAWNBench leaderboard:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1MzB0-FR_oIPVe9PCffvRI9DB13m2YI21)\n",
        "\n",
        "The top six entries all use 9-layer ResNets which are cousins - or twins - of the network we developed  [earlier in the series](https://myrtle.ai/learn/how-to-train-your-resnet-4-architecture/). First place is a 4-GPU implementation from Kakao Brain which completes in an impressive 37s. The single-GPU version of the same comes in third with 68s, an apparent 7s improvement over our single-GPU entry from last year, although close inspection shows that these submissions are using test-time augmentation (TTA). We shall discuss the validity of this approach towards the end of the post (our conclusion is that any reasonable restriction should be based on total inference cost and that the form of mild TTA used here, along with a lightweight network, passes on that front.) Note that our earlier submission, allowing the same TTA, would achieve a time of 60s on a 19 epoch training schedule without further changes.\n",
        "\n",
        "By the end of the post our single-GPU implementation surpasses the top multi-GPU times comfortably, reclaiming the coveted DAWNBench crown with a time of 34s and achieving a 10Ã— improvement over the single-GPU state-of-the-art at the start of the series! Using the same TTA employed by the Kakao Brain submission, this drops to 26s. We achieve these times by accumulating a series of small (typically 0.1-0.3% in absolute test accuracy) improvements, which can be traded for shorter training times. These improvements are based on a collection of standard and not-so-standard tricks. \n",
        "\n",
        "Our main weapon is statistical significance. The standard deviation in test accuracy for a single training run is roughly 0.15% and when comparing between two runs we need to multiply this by $\\sqrt{2}$. This is larger than many of the effects that we are measuring. Given that training times soon drop below a minute, we can afford to run experiments 10s-100s of times to make sure that improvements are real and this allows us to make consistent progress.\n",
        "\n",
        "Sharp experimental results are essential to advancing the field but if a baseline is poorly-tuned or the number of runs too few, experimental validation holds little value. The main goal of today's post is to provide a well-tuned baseline on which to test novel techniques, allowing one to complete a statistically significant number of training runs within minutes on a single GPU. We confirm, at the end of the post, that improvements in training speed translate into improvements in final accuarcy if training is allowed to proceed towards convergence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWyjxzQb_yMA"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "7WeTS0FF4iQE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pdluser/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "#@title Lib (RUN ME) - double-click to show/hide code\n",
        "####################\n",
        "## CORE\n",
        "#####################\n",
        "\n",
        "import inspect\n",
        "from collections import namedtuple, defaultdict\n",
        "from functools import partial\n",
        "import functools\n",
        "from itertools import chain, count, islice as take\n",
        "\n",
        "#####################\n",
        "## dict utils\n",
        "#####################\n",
        "\n",
        "union = lambda *dicts: {k: v for d in dicts for (k, v) in d.items()}\n",
        "\n",
        "make_tuple = lambda path: (path,) if isinstance(path, str) else path\n",
        "\n",
        "def path_iter(nested_dict, pfx=()):\n",
        "    for name, val in nested_dict.items():\n",
        "        if isinstance(val, dict): yield from path_iter(val, pfx+make_tuple(name))\n",
        "        else: yield (pfx+make_tuple(name), val)  \n",
        "            \n",
        "map_values = lambda func, dct: {k: func(v) for k,v in dct.items()}\n",
        "\n",
        "def map_nested(func, nested_dict):\n",
        "    return {k: map_nested(func, v) if isinstance(v, dict) else func(v) for k,v in nested_dict.items()}\n",
        "\n",
        "def group_by_key(seq):\n",
        "    res = defaultdict(list)\n",
        "    for k, v in seq: \n",
        "        res[k].append(v) \n",
        "    return res\n",
        "\n",
        "reorder = lambda dct, keys: {k: dct[k] for k in keys}\n",
        "\n",
        "#####################\n",
        "## graph building\n",
        "#####################\n",
        "\n",
        "def identity(value): return value\n",
        "\n",
        "def build_graph(net, path_map='_'.join):\n",
        "    net = {path: node if len(node) is 3 else (*node, None) for path, node in path_iter(net)}\n",
        "    default_inputs = chain([('input',)], net.keys())\n",
        "    resolve_path = lambda path, pfx: pfx+path if (pfx+path in net or not pfx) else resolve_path(net, path, pfx[:-1])\n",
        "    return {path_map(path): (typ, value, ([path_map(default)] if inputs is None else [path_map(resolve_path(make_tuple(k), path[:-1])) for k in inputs])) \n",
        "            for (path, (typ, value, inputs)), default in zip(net.items(), default_inputs)}\n",
        "\n",
        "#####################\n",
        "## network visualisation (requires pydot)\n",
        "#####################\n",
        "import IPython.display\n",
        "\n",
        "class ColorMap(dict):\n",
        "    palette = (\n",
        "        'bebada,ffffb3,fb8072,8dd3c7,80b1d3,fdb462,b3de69,fccde5,bc80bd,ccebc5,ffed6f,1f78b4,33a02c,e31a1c,ff7f00,'\n",
        "        '4dddf8,e66493,b07b87,4e90e3,dea05e,d0c281,f0e189,e9e8b1,e0eb71,bbd2a4,6ed641,57eb9c,3ca4d4,92d5e7,b15928'\n",
        "    ).split(',')\n",
        " \n",
        "    def __missing__(self, key):\n",
        "        self[key] = self.palette[len(self) % len(self.palette)]\n",
        "        return self[key]\n",
        "\n",
        "def make_pydot(nodes, edges, direction='LR', sep='_', **kwargs):\n",
        "    from pydot import Dot, Cluster, Node, Edge\n",
        "    class Subgraphs(dict):\n",
        "        def __missing__(self, path):\n",
        "            *parent, label = path\n",
        "            subgraph = Cluster(sep.join(path), label=label, style='rounded, filled', fillcolor='#77777744')\n",
        "            self[tuple(parent)].add_subgraph(subgraph)\n",
        "            return subgraph\n",
        "    g = Dot(rankdir=direction, directed=True, **kwargs)\n",
        "    g.set_node_defaults(\n",
        "        shape='box', style='rounded, filled', fillcolor='#ffffff')\n",
        "    subgraphs = Subgraphs({(): g})\n",
        "    for path, attr in nodes:\n",
        "        *parent, label = path.split(sep)\n",
        "        subgraphs[tuple(parent)].add_node(\n",
        "            Node(name=path, label=label, **attr))\n",
        "    for src, dst, attr in edges:\n",
        "        g.add_edge(Edge(src, dst, **attr))\n",
        "    return g\n",
        "\n",
        "class DotGraph():\n",
        "    colors = ColorMap()   \n",
        "    def __init__(self, graph, size=15, direction='LR'):\n",
        "        self.nodes = [(k, {\n",
        "            'tooltip': '%s %.1000r' % (typ, value), \n",
        "            'fillcolor': '#'+self.colors[typ],\n",
        "        }) for k, (typ, value, inputs) in graph.items()] \n",
        "        self.edges = [(src, k, {}) for (k, (_,_,inputs)) in graph.items() for src in inputs]\n",
        "        self.size, self.direction = size, direction\n",
        "\n",
        "    def dot_graph(self, **kwargs):\n",
        "        return make_pydot(self.nodes, self.edges, size=self.size, \n",
        "                            direction=self.direction, **kwargs)\n",
        "\n",
        "    def svg(self, **kwargs):\n",
        "        return self.dot_graph(**kwargs).create(format='svg').decode('utf-8')\n",
        "\n",
        "    try:\n",
        "        import pydot\n",
        "        def _repr_svg_(self):\n",
        "            return self.svg()\n",
        "    except ImportError:\n",
        "        def __repr__(self):\n",
        "            return 'pydot is needed for network visualisation'\n",
        "\n",
        "\n",
        "#####################\n",
        "## Layers\n",
        "##################### \n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "import copy\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu = torch.device('cpu')\n",
        "    \n",
        "class Network(nn.Module):\n",
        "    def __init__(self, net, loss=None):\n",
        "        super().__init__()\n",
        "        self.graph = {path: (typ, typ(**params), inputs) for path, (typ, params, inputs) in build_graph(net).items()}\n",
        "        self.loss = loss or identity\n",
        "        for path, (_,node,_) in self.graph.items(): \n",
        "            setattr(self, path, node)\n",
        "    \n",
        "    def nodes(self):\n",
        "        return (node for _,node,_ in self.graph.values())\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        outputs = dict(inputs)\n",
        "        for k, (_, node, ins) in self.graph.items():\n",
        "            outputs[k] = node(*[outputs[x] for x in ins])\n",
        "        return outputs\n",
        "    \n",
        "    def half(self):\n",
        "        for node in self.nodes():\n",
        "            if isinstance(node, nn.Module) and not isinstance(node, nn.BatchNorm2d):\n",
        "                node.half()\n",
        "        return self\n",
        "\n",
        "build_model = lambda network, loss: Network(network, loss).half().to(device)\n",
        "show = lambda network, size=15: display(DotGraph(network.graph if isinstance(network, Network) else build_graph(network), size=size))\n",
        "    \n",
        "class Add(namedtuple('Add', [])):\n",
        "    def __call__(self, x, y): return x + y \n",
        "    \n",
        "class AddWeighted(namedtuple('AddWeighted', ['wx', 'wy'])):\n",
        "    def __call__(self, x, y): return self.wx*x + self.wy*y \n",
        "    \n",
        "class Identity(namedtuple('Identity', [])):\n",
        "    def __call__(self, x): return x\n",
        "\n",
        "class BatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight=True, bias=True):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
        "        self.weight.data.fill_(1.0)\n",
        "        self.bias.data.fill_(0.0)\n",
        "        self.weight.requires_grad = weight\n",
        "        self.bias.requires_grad = bias\n",
        "\n",
        "class GhostBatchNorm(BatchNorm):\n",
        "    def __init__(self, num_features, num_splits, **kw):\n",
        "        super().__init__(num_features, **kw)\n",
        "        self.num_splits = num_splits\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features*self.num_splits))\n",
        "        self.register_buffer('running_var', torch.ones(num_features*self.num_splits))\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        if (self.training is True) and (mode is False): #lazily collate stats when we are going to use them\n",
        "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
        "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(self.num_splits)\n",
        "        return super().train(mode)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        N, C, H, W = input.shape\n",
        "        if self.training or not self.track_running_stats:\n",
        "            return F.batch_norm(\n",
        "                input.view(-1, C*self.num_splits, H, W), self.running_mean, self.running_var, \n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(self.num_splits),\n",
        "                True, self.momentum, self.eps).view(N, C, H, W) \n",
        "        else:\n",
        "            return F.batch_norm(\n",
        "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features], \n",
        "                self.weight, self.bias, False, self.momentum, self.eps)\n",
        "        \n",
        "class Mul(nn.Module):\n",
        "    def __init__(self, weight):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "    def __call__(self, x): \n",
        "        return x*self.weight\n",
        "    \n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x): \n",
        "        return x.view(x.size(0), x.size(1))\n",
        "\n",
        "# Losses\n",
        "class CrossEntropyLoss(namedtuple('CrossEntropyLoss', [])):\n",
        "    def __call__(self, log_probs, target):\n",
        "        return torch.nn.functional.nll_loss(log_probs, target, reduction='none')\n",
        "    \n",
        "class KLLoss(namedtuple('KLLoss', [])):        \n",
        "    def __call__(self, log_probs):\n",
        "        return -log_probs.mean(dim=1)\n",
        "\n",
        "class Correct(namedtuple('Correct', [])):\n",
        "    def __call__(self, classifier, target):\n",
        "        return classifier.max(dim = 1)[1] == target\n",
        "\n",
        "class LogSoftmax(namedtuple('LogSoftmax', ['dim'])):\n",
        "    def __call__(self, x):\n",
        "        return torch.nn.functional.log_softmax(x, self.dim, _stacklevel=5)\n",
        "\n",
        "    \n",
        "# node definitions   \n",
        "from inspect import signature    \n",
        "empty_signature = inspect.Signature()\n",
        "\n",
        "class node_def(namedtuple('node_def', ['type'])):\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return (self.type, dict(signature(self.type).bind(*args, **kwargs).arguments))\n",
        "\n",
        "conv = node_def(nn.Conv2d)\n",
        "linear = node_def(nn.Linear)\n",
        "batch_norm = node_def(BatchNorm)\n",
        "pool = node_def(nn.MaxPool2d)\n",
        "relu = node_def(nn.ReLU)\n",
        "    \n",
        "def map_types(mapping, net):\n",
        "    def f(node):\n",
        "        typ, *rest = node\n",
        "        return (mapping.get(typ, typ), *rest)\n",
        "    return map_nested(f, net) \n",
        "\n",
        "#####################\n",
        "## Compat\n",
        "##################### \n",
        "\n",
        "def to_numpy(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.detach().cpu().numpy()  \n",
        "    return x\n",
        "  \n",
        "def flip_lr(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return torch.flip(x, [-1]) \n",
        "    return x[..., ::-1].copy()\n",
        "  \n",
        "trainable_params = lambda model: {k:p for k,p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "#####################\n",
        "## Optimisers\n",
        "##################### \n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def nesterov_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    dw.add_(weight_decay, w).mul_(-lr)\n",
        "    v.mul_(momentum).add_(dw)\n",
        "    w.add_(dw.add_(momentum, v))\n",
        "\n",
        "norm = lambda x: torch.norm(x.reshape(x.size(0),-1).float(), dim=1)[:,None,None,None]\n",
        "\n",
        "def LARS_update(w, dw, v, lr, weight_decay, momentum):\n",
        "    nesterov_update(w, dw, v, lr*(norm(w)/(norm(dw)+1e-2)).to(w.dtype), weight_decay, momentum)\n",
        "\n",
        "def zeros_like(weights):\n",
        "    return [torch.zeros_like(w) for w in weights]\n",
        "\n",
        "def optimiser(weights, param_schedule, update, state_init):\n",
        "    weights = list(weights)\n",
        "    return {'update': update, 'param_schedule': param_schedule, 'step_number': 0, 'weights': weights,  'opt_state': state_init(weights)}\n",
        "\n",
        "def opt_step(update, param_schedule, step_number, weights, opt_state):\n",
        "    step_number += 1\n",
        "    param_values = {k: f(step_number) for k, f in param_schedule.items()}\n",
        "    for w, v in zip(weights, opt_state):\n",
        "        if w.requires_grad:\n",
        "            update(w.data, w.grad.data, v, **param_values)\n",
        "    return {'update': update, 'param_schedule': param_schedule, 'step_number': step_number, 'weights': weights,  'opt_state': opt_state}\n",
        "\n",
        "LARS = partial(optimiser, update=LARS_update, state_init=zeros_like)\n",
        "SGD = partial(optimiser, update=nesterov_update, state_init=zeros_like)\n",
        "  \n",
        "class PiecewiseLinear(namedtuple('PiecewiseLinear', ('knots', 'vals'))):\n",
        "    def __call__(self, t):\n",
        "        return np.interp([t], self.knots, self.vals)[0]\n",
        "     \n",
        "class Const(namedtuple('Const', ['val'])):\n",
        "    def __call__(self, x):\n",
        "        return self.val\n",
        "\n",
        "#####################\n",
        "## DATA\n",
        "##################### \n",
        "\n",
        "import torchvision\n",
        "from functools import lru_cache as cache\n",
        "\n",
        "@cache(None)\n",
        "def cifar10(root='./data'):\n",
        "    download = lambda train: torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
        "    return {k: {'data': torch.tensor(v.data), 'targets': torch.tensor(v.targets)} \n",
        "            for k,v in [('train', download(True)), ('valid', download(False))]}\n",
        "  \n",
        "cifar10_mean, cifar10_std = [\n",
        "    (125.31, 122.95, 113.87), # equals np.mean(cifar10()['train']['data'], axis=(0,1,2)) \n",
        "    (62.99, 62.09, 66.70), # equals np.std(cifar10()['train']['data'], axis=(0,1,2))\n",
        "]\n",
        "cifar10_classes= 'airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck'.split(', ')\n",
        "\n",
        "#####################\n",
        "## data preprocessing\n",
        "#####################\n",
        "mean, std = [torch.tensor(x, device=device, dtype=torch.float16) for x in (cifar10_mean, cifar10_std)]\n",
        "\n",
        "normalise = lambda data, mean=mean, std=std: (data - mean)/std\n",
        "unnormalise = lambda data, mean=mean, std=std: data*std + mean\n",
        "pad = lambda data, border: nn.ReflectionPad2d(border)(data)\n",
        "transpose = lambda x, source='NHWC', target='NCHW': x.permute([source.index(d) for d in target]) \n",
        "to = lambda *args, **kwargs: (lambda x: x.to(*args, **kwargs))\n",
        "\n",
        "def preprocess(dataset, transforms):\n",
        "    dataset = copy.copy(dataset)\n",
        "    for transform in reversed(transforms):\n",
        "        dataset['data'] = transform(dataset['data'])\n",
        "    return dataset\n",
        "\n",
        "#####################\n",
        "## Data augmentation\n",
        "#####################\n",
        "\n",
        "chunks = lambda data, splits: (data[start:end] for (start, end) in zip(splits, splits[1:]))\n",
        "\n",
        "even_splits = lambda N, num_chunks: np.cumsum([0] + [(N//num_chunks)+1]*(N % num_chunks)  + [N//num_chunks]*(num_chunks - (N % num_chunks)))\n",
        "\n",
        "def shuffled(xs, inplace=False):\n",
        "    xs = xs if inplace else copy.copy(xs) \n",
        "    np.random.shuffle(xs)\n",
        "    return xs\n",
        "\n",
        "def transformed(data, targets, transform, max_options=None, unshuffle=False):\n",
        "    i = torch.randperm(len(data), device=device)\n",
        "    data = data[i]\n",
        "    options = shuffled(transform.options(data.shape), inplace=True)[:max_options]\n",
        "    data = torch.cat([transform.apply(x, **choice) for choice, x in zip(options, chunks(data, even_splits(len(data), len(options))))])\n",
        "    return (data[torch.argsort(i)], targets) if unshuffle else (data, targets[i])\n",
        "\n",
        "class Batches():\n",
        "    def __init__(self, batch_size, transforms=(), dataset=None, shuffle=True, drop_last=False, max_options=None):\n",
        "        self.dataset, self.transforms, self.shuffle, self.max_options = dataset, transforms, shuffle, max_options\n",
        "        N = len(dataset['data'])\n",
        "        self.splits = list(range(0, N+1, batch_size))\n",
        "        if not drop_last and self.splits[-1] != N:\n",
        "            self.splits.append(N)\n",
        "     \n",
        "    def __iter__(self):\n",
        "        data, targets = self.dataset['data'], self.dataset['targets']\n",
        "        for transform in self.transforms:\n",
        "            data, targets = transformed(data, targets, transform, max_options=self.max_options, unshuffle=not self.shuffle)\n",
        "        if self.shuffle:\n",
        "            i = torch.randperm(len(data), device=device)\n",
        "            data, targets = data[i], targets[i]\n",
        "        return ({'input': x.clone(), 'target': y} for (x, y) in zip(chunks(data, self.splits), chunks(targets, self.splits)))\n",
        "    \n",
        "    def __len__(self): \n",
        "        return len(self.splits) - 1\n",
        "    \n",
        "#####################\n",
        "## Augmentations\n",
        "#####################\n",
        "\n",
        "class Crop(namedtuple('Crop', ('h', 'w'))):\n",
        "    def apply(self, x, x0, y0):\n",
        "        return x[..., y0:y0+self.h, x0:x0+self.w] \n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]\n",
        "    \n",
        "class FlipLR(namedtuple('FlipLR', ())):\n",
        "    def apply(self, x, choice):\n",
        "        return flip_lr(x) if choice else x \n",
        "        \n",
        "    def options(self, shape):\n",
        "        return [{'choice': b} for b in [True, False]]\n",
        "\n",
        "class Cutout(namedtuple('Cutout', ('h', 'w'))):\n",
        "    def apply(self, x, x0, y0):\n",
        "        x[..., y0:y0+self.h, x0:x0+self.w] = 0.0\n",
        "        return x\n",
        "\n",
        "    def options(self, shape):\n",
        "        *_, H, W = shape\n",
        "        return [{'x0': x0, 'y0': y0} for x0 in range(W+1-self.w) for y0 in range(H+1-self.h)]  \n",
        "\n",
        "#####################\n",
        "## TRAINING\n",
        "#####################\n",
        "\n",
        "import time\n",
        "\n",
        "class Timer():\n",
        "    def __init__(self, synch=None):\n",
        "        self.synch = synch or (lambda: None)\n",
        "        self.synch()\n",
        "        self.times = [time.perf_counter()]\n",
        "        self.total_time = 0.0\n",
        "\n",
        "    def __call__(self, update_total=True):\n",
        "        self.synch()\n",
        "        self.times.append(time.perf_counter())\n",
        "        delta_t = self.times[-1] - self.times[-2]\n",
        "        if update_total:\n",
        "            self.total_time += delta_t\n",
        "        return delta_t\n",
        "\n",
        "default_table_formats = {float: '{:{w}.4f}', str: '{:>{w}s}', 'default': '{:{w}}', 'title': '{:>{w}s}'}\n",
        "\n",
        "def table_formatter(val, is_title=False, col_width=12, formats=None):\n",
        "    formats = formats or default_table_formats\n",
        "    type_ = lambda val: float if isinstance(val, (float, np.float)) else type(val)\n",
        "    return (formats['title'] if is_title else formats.get(type_(val), formats['default'])).format(val, w=col_width)\n",
        "\n",
        "every = lambda n, col: (lambda data: data[col] % n == 0)\n",
        "\n",
        "class Table():\n",
        "    def __init__(self, keys=None, report=(lambda data: True), formatter=table_formatter):\n",
        "        self.keys, self.report, self.formatter = keys, report, formatter\n",
        "        self.log = []\n",
        "        \n",
        "    def append(self, data):\n",
        "        self.log.append(data)\n",
        "        data = {' '.join(p): v for p,v in path_iter(data)}\n",
        "        self.keys = self.keys or data.keys()\n",
        "        if len(self.log) is 1:\n",
        "            print(*(self.formatter(k, True) for k in self.keys))\n",
        "        if self.report(data):\n",
        "            print(*(self.formatter(data[k]) for k in self.keys))\n",
        "            \n",
        "    def df(self):\n",
        "        return pd.DataFrame([{'_'.join(p): v for p,v in path_iter(row)} for row in self.log])     \n",
        "            \n",
        "def reduce(batches, state, steps):\n",
        "    #state: is a dictionary\n",
        "    #steps: are functions that take (batch, state)\n",
        "    #and return a dictionary of updates to the state (or None)\n",
        "    \n",
        "    for batch in chain(batches, [None]): \n",
        "    #we send an extra batch=None at the end for steps that \n",
        "    #need to do some tidying-up (e.g. log_activations)\n",
        "        for step in steps:\n",
        "            updates = step(batch, state)\n",
        "            if updates:\n",
        "                for k,v in updates.items():\n",
        "                    state[k] = v                  \n",
        "    return state\n",
        "  \n",
        "#define keys in the state dict as constants\n",
        "MODEL = 'model'\n",
        "VALID_MODEL = 'valid_model'\n",
        "OUTPUT = 'output'\n",
        "OPTS = 'optimisers'\n",
        "ACT_LOG = 'activation_log'\n",
        "WEIGHT_LOG = 'weight_log'\n",
        "\n",
        "#step definitions\n",
        "def forward(training_mode):\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        model = state[MODEL] if training_mode or (VALID_MODEL not in state) else state[VALID_MODEL]\n",
        "        if model.training != training_mode: #without the guard it's slow!\n",
        "            model.train(training_mode)\n",
        "        return {OUTPUT: model.loss(model(batch))}\n",
        "    return step\n",
        "\n",
        "def forward_tta(tta_transforms):\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        model = state[MODEL] if (VALID_MODEL not in state) else state[VALID_MODEL]\n",
        "        if model.training:\n",
        "            model.train(False)\n",
        "        logits = torch.mean(torch.stack([model({'input': transform(batch['input'].clone())})['logits'].detach() for transform in tta_transforms], dim=0), dim=0)\n",
        "        return {OUTPUT: model.loss(dict(batch, logits=logits))}\n",
        "    return step\n",
        "\n",
        "def backward(dtype=torch.float16):\n",
        "    def step(batch, state):\n",
        "        state[MODEL].zero_grad()\n",
        "        if not batch: return\n",
        "        state[OUTPUT]['loss'].to(dtype).sum().backward()\n",
        "    return step\n",
        "\n",
        "def opt_steps(batch, state):\n",
        "    if not batch: return\n",
        "    return {OPTS: [opt_step(**opt) for opt in state[OPTS]]}\n",
        "\n",
        "def log_activations(node_names=('loss', 'acc')):\n",
        "    logs = []\n",
        "    def step(batch, state):\n",
        "        if batch:\n",
        "            logs.extend((k, state[OUTPUT][k].detach()) for k in node_names)\n",
        "        else:\n",
        "            res = map_values((lambda xs: to_numpy(torch.cat(xs)).astype(np.float)), group_by_key(logs))\n",
        "            logs.clear()\n",
        "            return {ACT_LOG: res}\n",
        "    return step\n",
        "\n",
        "def update_ema(momentum, update_freq=1):\n",
        "    n = iter(count())\n",
        "    rho = momentum**update_freq\n",
        "    def step(batch, state):\n",
        "        if not batch: return\n",
        "        if (next(n) % update_freq) != 0: return\n",
        "        for v, ema_v in zip(state[MODEL].state_dict().values(), state[VALID_MODEL].state_dict().values()):\n",
        "            ema_v *= rho\n",
        "            ema_v += (1-rho)*v\n",
        "    return step\n",
        "\n",
        "train_steps = (forward(training_mode=True), log_activations(('loss', 'acc')), backward(), opt_steps)\n",
        "valid_steps = (forward(training_mode=False), log_activations(('loss', 'acc')))\n",
        "\n",
        "epoch_stats = lambda state: {k: np.mean(v) for k, v in state[ACT_LOG].items()}\n",
        "\n",
        "def train_epoch(state, timer, train_batches, valid_batches, train_steps=train_steps, valid_steps=valid_steps, on_epoch_end=identity):\n",
        "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
        "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(update_total=False) #DAWNBench rules\n",
        "    return {\n",
        "        'train': union({'time': train_time}, train_summary), \n",
        "        'valid': union({'time': valid_time}, valid_summary), \n",
        "        'total time': timer.total_time\n",
        "    }\n",
        "\n",
        "summary = lambda logs, cols=['valid_acc']: logs.df().query('epoch==epoch.max()')[cols].describe().transpose().astype({'count': int})[\n",
        "    ['count', 'mean', 'min', 'max', 'std']]\n",
        "\n",
        "#on_epoch_end\n",
        "def log_weights(state, weights):\n",
        "    state[WEIGHT_LOG] = state.get(WEIGHT_LOG, [])\n",
        "    state[WEIGHT_LOG].append({k: to_numpy(v.data) for k,v in weights.items()})\n",
        "    return state\n",
        "\n",
        "def fine_tune_bn_stats(state, batches, model_key=VALID_MODEL):\n",
        "    reduce(batches, {MODEL: state[model_key]}, [forward(True)])\n",
        "    return state\n",
        "\n",
        "#misc\n",
        "def warmup_cudnn(model, batch):\n",
        "    #run forward and backward pass of the model\n",
        "    #to allow benchmarking of cudnn kernels \n",
        "    reduce([batch], {MODEL: model}, [forward(True), backward()])\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "#####################\n",
        "## Plotting\n",
        "#####################\n",
        "\n",
        "# import altair as alt\n",
        "# alt.renderers.enable('colab')\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import SVG\n",
        "\n",
        "def empty_plot(ax, **kw):\n",
        "    ax.axis('off')\n",
        "    return ax\n",
        "\n",
        "def image_plot(ax, img, title):\n",
        "    ax.imshow(to_numpy(unnormalise(transpose(img, 'CHW', 'HWC'))).astype(np.int))\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "def layout(figures, sharex=False, sharey=False, figure_title=None, col_width=4, row_height = 3.25, **kw):\n",
        "    nrows, ncols = np.array(figures).shape\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey, figsize=(col_width*ncols, row_height*nrows))\n",
        "    axs = [figure(ax, **kw) for row in zip(np.array(axs).reshape(nrows, ncols), figures) for ax, figure in zip(*row)]\n",
        "    fig.suptitle(figure_title)\n",
        "    return fig, axs\n",
        "\n",
        "#####################\n",
        "## Network\n",
        "#####################\n",
        "\n",
        "conv_block = lambda c_in, c_out: {\n",
        "    'conv': conv(in_channels=c_in, out_channels=c_out, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), \n",
        "    'norm': batch_norm(c_out), \n",
        "    'act':  relu(),\n",
        "}\n",
        "\n",
        "conv_pool_block = lambda c_in, c_out: dict(conv_block(c_in, c_out), pool=pool(2))\n",
        "conv_pool_block_pre = lambda c_in, c_out: reorder(conv_pool_block(c_in, c_out), ('conv', 'pool', 'norm', 'act'))\n",
        "\n",
        "residual = lambda c, conv_block: {\n",
        "    'in': (Identity, {}),\n",
        "    'res1': conv_block(c, c),\n",
        "    'res2': conv_block(c, c),\n",
        "    'out': (Identity, {}),\n",
        "    'add': (Add, {}, ['in', 'out']),\n",
        "}\n",
        "\n",
        "def build_network(channels, extra_layers, res_layers, scale, conv_block=conv_block, \n",
        "                  prep_block=conv_block, conv_pool_block=conv_pool_block, types=None): \n",
        "    net = {\n",
        "        'prep': prep_block(3, channels['prep']),\n",
        "        'layer1': conv_pool_block(channels['prep'], channels['layer1']),\n",
        "        'layer2': conv_pool_block(channels['layer1'], channels['layer2']),\n",
        "        'layer3': conv_pool_block(channels['layer2'], channels['layer3']),\n",
        "        'pool': pool(4),\n",
        "        'classifier': {\n",
        "            'flatten': (Flatten, {}),\n",
        "            'conv': linear(channels['layer3'], 10, bias=False),\n",
        "            'scale': (Mul, {'weight': scale}),\n",
        "        },\n",
        "        'logits': (Identity, {}),\n",
        "    }\n",
        "    for layer in res_layers:\n",
        "        net[layer]['residual'] = residual(channels[layer], conv_block)\n",
        "    for layer in extra_layers:\n",
        "        net[layer]['extra'] = conv_block(channels[layer], channels[layer])     \n",
        "    if types: net = map_types(types, net)\n",
        "    return net\n",
        "\n",
        "channels={'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
        "network = partial(build_network, channels=channels, extra_layers=(), res_layers=('layer1', 'layer3'), scale=1/8)   \n",
        "\n",
        "x_ent_loss = Network({\n",
        "  'loss':  (nn.CrossEntropyLoss, {'reduction': 'none'}, ['logits', 'target']),\n",
        "  'acc': (Correct, {}, ['logits', 'target'])\n",
        "})\n",
        "\n",
        "label_smoothing_loss = lambda alpha: Network({\n",
        "        'logprobs': (LogSoftmax, {'dim': 1}, ['logits']),\n",
        "        'KL':  (KLLoss, {}, ['logprobs']),\n",
        "        'xent':  (CrossEntropyLoss, {}, ['logprobs', 'target']),\n",
        "        'loss': (AddWeighted, {'wx': 1-alpha, 'wy': alpha}, ['xent', 'KL']),\n",
        "        'acc': (Correct, {}, ['logits', 'target']),\n",
        "    })\n",
        "\n",
        "#####################\n",
        "## Misc\n",
        "#####################\n",
        "\n",
        "lr_schedule = lambda knots, vals, batch_size: PiecewiseLinear(np.array(knots)*len(train_batches(batch_size)), np.array(vals)/batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "izQpvJHoBnAx"
      },
      "outputs": [],
      "source": [
        "#####################\n",
        "## Config\n",
        "#####################\n",
        "\n",
        "N_RUNS = 5 #number of times to run each experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tofkr8ecBqgF"
      },
      "source": [
        "### Baseline (75s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ_zmZhTyOSN"
      },
      "source": [
        "### Recommended: running on GCP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUJEUs3ND0eq"
      },
      "source": [
        "You can run this notebook using a free Colab GPU instance (Tesla T4) but timings will be considerably slower than on a V100. \n",
        "If you have a GCP account and want to  use a faster V100 GPU you can follow the instructions [here](https://blog.kovalevskyi.com/gce-deeplearning-images-as-a-backend-for-google-colaboratory-bc4903d24947) to use that as an alternative Colab backend. Don't forget to shut down the GCP instance (not just the Colab notebook) once you've finished!\n",
        "\n",
        "FWIW, I use the following (preemptible) instance type:\n",
        "\n",
        "```\n",
        "export IMAGE_FAMILY=\"pytorch-latest-gpu\"\n",
        "export ZONE=\"europe-west4-a\"\n",
        "export INSTANCE_NAME=\"pytorch-colab-backend\"\n",
        "\n",
        "gcloud compute instances create $INSTANCE_NAME \\\n",
        "  --zone $ZONE \\\n",
        "  --machine-type n1-standard-4 \\\n",
        "  --accelerator type=nvidia-tesla-v100,count=1 \\\n",
        "  --image-family $IMAGE_FAMILY \\\n",
        "  --image-project=deeplearning-platform-release \\\n",
        "  --metadata install-nvidia-driver=True \\\n",
        "  --maintenance-policy TERMINATE \\\n",
        "  --preemptible \n",
        "  ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjOIoRO9ZNE2"
      },
      "source": [
        "On GCP you will need to install altair for plotting. This is pre-installed on Colab instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m1LCN_bZIrs"
      },
      "outputs": [],
      "source": [
        "#GCP only\n",
        "!python -m pip install -q vega altair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns4mKklxZGIT"
      },
      "source": [
        "You can check the details of your setup (free Colab GPU or V100 on GCP) with this nice utility from fastai:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "OpxpGgl_yJ8r",
        "outputId": "81784a4d-6135-49af-fdb0-e7fbb2875338"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "```text\n",
            "=== Software === \n",
            "python        : 3.7.3\n",
            "fastai        : 1.0.55\n",
            "fastprogress  : 0.1.21\n",
            "torch         : 1.1.0\n",
            "nvidia driver : 410.104\n",
            "torch cuda    : 10.0.130 / is available\n",
            "torch cudnn   : 7501 / is enabled\n",
            "\n",
            "=== Hardware === \n",
            "nvidia gpus   : 1\n",
            "torch devices : 1\n",
            "  - gpu0      : 16130MB | Tesla V100-SXM2-16GB\n",
            "\n",
            "=== Environment === \n",
            "platform      : Linux-4.9.0-9-amd64-x86_64-with-debian-9.9\n",
            "distro        : #1 SMP Debian 4.9.168-1+deb9u4 (2019-07-19)\n",
            "conda env     : base\n",
            "python        : /opt/anaconda3/bin/python\n",
            "sys.path      : /home/jupyter\n",
            "/opt/anaconda3/lib/python37.zip\n",
            "/opt/anaconda3/lib/python3.7\n",
            "/opt/anaconda3/lib/python3.7/lib-dynload\n",
            "\n",
            "/opt/anaconda3/lib/python3.7/site-packages\n",
            "/opt/anaconda3/lib/python3.7/site-packages/IPython/extensions\n",
            "/home/jupyter/.ipython\n",
            "```\n",
            "\n",
            "Please make sure to include opening/closing ``` when you paste into forums/github to make the reports appear formatted as code sections.\n",
            "\n",
            "Optional package(s) to enhance the diagnostics can be installed with:\n",
            "pip install distro\n",
            "Once installed, re-run this utility to get the additional information\n"
          ]
        }
      ],
      "source": [
        "# from fastai.utils.show_install import show_install \n",
        "# show_install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kngp4RTwBKNl"
      },
      "source": [
        "First we should check that timings haven't changed since November and our submission still runs in 75 seconds (note that this requires a V100 GPU, see instructions at the top of the notebook for how to use one from Colab.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "n___bs94Rvm2",
        "outputId": "975cee77-3b99-4a75-ffb1-ef463a33a5ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/pdluser/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
            "  warnings.warn(msg, UserWarning)\n",
            "Downloading datasets\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Warming up cudnn on random inputs\n",
            "Starting timer\n",
            "Preprocessing training data\n",
            "Finished in 1.8 seconds\n",
            "Preprocessing test data\n",
            "Finished in 0.072 seconds\n",
            "/home/pdluser/project/pytorch-cifar-model-zoo/cifar10-fast/torch_backend.py:243: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  dw.add_(weight_decay, w).mul_(-lr)\n",
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       5.2092       1.6403       0.4084       0.3151       1.4289       0.5195       7.0703\n",
            "           2       5.2051       0.9485       0.6612       0.3154       0.7875       0.7237      12.2754\n",
            "           3       5.2335       0.7309       0.7450       0.3176       0.7860       0.7386      17.5089\n",
            "           4       5.2466       0.6145       0.7868       0.3188       0.7708       0.7258      22.7554\n",
            "           5       5.2665       0.5607       0.8033       0.3213       0.7567       0.7373      28.0219\n",
            "           6       5.2780       0.5045       0.8278       0.3202       0.5870       0.7975      33.2999\n",
            "           7       5.2780       0.4454       0.8471       0.3190       0.4710       0.8367      38.5778\n",
            "           8       5.2784       0.4115       0.8619       0.3214       0.4961       0.8370      43.8563\n",
            "           9       5.2876       0.3881       0.8679       0.3205       0.5094       0.8205      49.1439\n",
            "          10       5.2971       0.3644       0.8769       0.3203       0.4384       0.8522      54.4410\n",
            "          11       5.2982       0.3403       0.8850       0.3206       0.4157       0.8584      59.7392\n",
            "          12       5.3058       0.3203       0.8919       0.3258       0.3756       0.8750      65.0450\n",
            "          13       5.3013       0.3042       0.8970       0.3210       0.3910       0.8673      70.3463\n",
            "          14       5.3003       0.2903       0.9029       0.3333       0.5402       0.8226      75.6466\n",
            "          15       5.3043       0.2690       0.9094       0.3205       0.4943       0.8356      80.9509\n",
            "          16       5.3014       0.2507       0.9153       0.3209       0.3517       0.8798      86.2522\n",
            "          17       5.3151       0.2304       0.9230       0.3207       0.3145       0.8937      91.5673\n",
            "          18       5.3058       0.2119       0.9298       0.3203       0.3075       0.8985      96.8731\n",
            "          19       5.3057       0.1904       0.9369       0.3206       0.3300       0.8913     102.1788\n",
            "          20       5.3062       0.1657       0.9467       0.3207       0.2873       0.9049     107.4850\n",
            "          21       5.3083       0.1444       0.9522       0.3215       0.2316       0.9223     112.7933\n",
            "          22       5.3089       0.1150       0.9643       0.3206       0.2126       0.9288     118.1022\n",
            "          23       5.3087       0.0958       0.9709       0.3245       0.1871       0.9367     123.4109\n",
            "          24       5.3056       0.0754       0.9784       0.3212       0.1763       0.9429     128.7166\n"
          ]
        }
      ],
      "source": [
        "# !git clone -q https://github.com/davidcpage/cifar10-fast.git\n",
        "!cd cifar10-fast && python -m dawn --data_dir=~/data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6woN6tKcCLLy"
      },
      "source": [
        "Training indeed reaches ~94% test accuracy and completes in about 75s. We will need to cut this in half and then a bit more....\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOPAnn6orgLn"
      },
      "source": [
        "### Preprocessing on the GPU (70s)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyIgaE0Yr6wl"
      },
      "source": [
        "We start with the practical matter of some code optimisation. The logs above show three seconds wasted on data preprocessing, which counts towards training time. Recall that we are normalising, transposing and padding the dataset before training to avoid repeating the work at each epoch.\n",
        "\n",
        "We can do better by transferring the data to the GPU, preprocessing there and then transferring back to the CPU for random data augmentation and batching:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "W-y1gWp1zsWA",
        "outputId": "6652754d-bbb4-4d8e-b4fe-941b82b8037a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Starting timer\n",
            "Transfer to GPU:\t0.075s\n",
            "Data preprocessing:\t0.013s\n",
            "Transfer to CPU:\t0.306s\n"
          ]
        }
      ],
      "source": [
        "#####################\n",
        "## timings\n",
        "#####################\n",
        "dataset = cifar10() #downloads dataset\n",
        "print('Starting timer')\n",
        "t = Timer(synch=torch.cuda.synchronize)\n",
        "dataset = map_nested(to(device), dataset)\n",
        "print(f'Transfer to GPU:\\t{t():.3f}s')\n",
        "train_set = preprocess(dataset['train'], [partial(pad, border=4), transpose, normalise, to(torch.float16)])\n",
        "valid_set = preprocess(dataset['valid'], [transpose, normalise, to(torch.float16)])\n",
        "print(f'Data preprocessing:\\t{t():.3f}s')\n",
        "map_nested(to(cpu), {'train': train_set, 'valid': valid_set})\n",
        "print(f'Transfer to CPU:\\t{t():.3f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDVW85kFex8c"
      },
      "source": [
        "Not bad! We've reduced the preprocessing time to about half a second. Actual preprocessing now takes a negligible amount of time and the bulk of time is spent transferring data back to the CPU. This is a bit silly, since the data will need to cross to the GPU again after batching and augmentation, incurring a further delay at each training step. Can we remove this by doing data augmentation on the GPU?\n",
        "\n",
        "The answer is yes, but it requires a little care. If we naively apply augmentation to individual training examples, as on the CPU, we will incur substantial overhead launching multiple GPU kernels to process each item. We can avoid this by applying the same augmentation to groups of examples and we can preserve randomness by shuffling the data beforehand.\n",
        "\n",
        "For example, consider applying 8Ã—8 cutout augmentation to CIFAR10 images. There are 625 possible 8Ã—8 cutout regions in a  32Ã—32 image, so we can achieve random augmentation by shuffling the dataset and splitting into 625 groups, one for each of the possible cutout regions. If we choose evenly-sized groups, this is not quite the same as making a random choice for each example (which leads to irregular group sizes) but it's close enough. As a further optimisation, if the number of groups for an augmentation becomes too large, we can consider capping it at a reasonable limit - say 200 randomly selected groups per epoch.\n",
        "\n",
        "Our basic implementation takes about 35 lines of code and doesn't use Pytorch DataLoaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oNz7-dE7fE5J"
      },
      "outputs": [],
      "source": [
        "train_batches = partial(Batches, dataset=train_set, shuffle=True,  drop_last=True, max_options=200)\n",
        "valid_batches = partial(Batches, dataset=valid_set, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwHDoIF807rV"
      },
      "source": [
        "As a sanity check that we're doing things correctly - here are two random augmentations of the same 8 images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "4hF3Grkd1OVW",
        "outputId": "c97aee8f-700b-402a-fb15-94b7b1cf4394"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x144 with 8 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1152x144 with 8 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "batches = train_batches(batch_size=8, transforms=(Crop(32, 32), FlipLR(), Cutout(8, 8)), shuffle=False)\n",
        "\n",
        "layout([[partial(image_plot, img=x, title=cifar10_classes[y]) for x,y in zip(*next(iter(batches)).values())]], col_width=2, row_height=2)\n",
        "layout([[partial(image_plot, img=x, title=cifar10_classes[y]) for x,y in zip(*next(iter(batches)).values())]], col_width=2, row_height=2);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvhwlXlDtEwQ"
      },
      "source": [
        "Now let's see how fast it is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S0YBtX02q8T6",
        "outputId": "d4bedf88-ee09-483e-f53c-7ee3a7f9aa59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.153s\n"
          ]
        }
      ],
      "source": [
        "t = Timer(synch=torch.cuda.synchronize)\n",
        "batches = train_batches(batch_size=512, transforms=(Crop(32, 32), FlipLR(), Cutout(8, 8)))\n",
        "\n",
        "for epoch in range(240):\n",
        "    for batch in batches:\n",
        "        pass\n",
        "print(f'{t():.3f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-mg89jmtg1A"
      },
      "source": [
        "This is great! We're able to iterate through 24 epochs of training data, applying data augmentation and shuffling in less than the time taken to transfer the dataset once to the CPU! Moreover, since we're no longer racing CPU preprocessing queues against the GPU, we can stop worrying about dataloading altogether, even if training gets faster. Note: we are relying on the fact that the dataset is small enough to store and manipulate as a whole in GPU memory, but a more sophisticated implementation could work around this - or one could switch to an industrial strength solution such as [Nvidia DALI](https://github.com/NVIDIA/DALI).\n",
        "\n",
        "Let's see if this gives us a speedup on our baseline. The code we use here is essentially equivalent to the DAWNBench submission, apart from the GPU data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "hNFoYZEjH_w8",
        "outputId": "e47debaf-d181-4838-ea70-754e85371f15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pydot is needed for network visualisation"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "baseline_net=network()\n",
        "show(baseline_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3v3IsSpij-PL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pdluser/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:267: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       5.1485       1.6349       0.4108       0.5503       1.4526       0.5022       5.1485\n",
            "           2       5.1678       0.9433       0.6665       0.3004       0.9719       0.6840      10.3164\n",
            "           3       5.1783       0.7265       0.7461       0.3009       0.7759       0.7298      15.4946\n",
            "           4       5.1936       0.6410       0.7760       0.3019       1.5162       0.5607      20.6882\n",
            "           5       5.2047       0.5606       0.8049       0.3021       1.1775       0.6162      25.8929\n",
            "           6       5.2060       0.5124       0.8228       0.3027       0.5503       0.8108      31.0989\n",
            "           7       5.2088       0.4473       0.8458       0.3029       0.4928       0.8297      36.3077\n",
            "           8       5.2110       0.4221       0.8547       0.3026       0.6115       0.7967      41.5187\n",
            "           9       5.2142       0.3852       0.8693       0.3029       0.4430       0.8479      46.7329\n",
            "          10       5.2305       0.3699       0.8729       0.3039       0.4591       0.8408      51.9634\n",
            "          11       5.2320       0.3440       0.8844       0.3037       0.3796       0.8696      57.1953\n",
            "          12       5.2338       0.3267       0.8901       0.3035       0.4589       0.8465      62.4291\n",
            "          13       5.2340       0.3125       0.8936       0.3036       0.4232       0.8563      67.6631\n",
            "          14       5.2327       0.2928       0.9004       0.3038       0.4539       0.8473      72.8959\n",
            "          15       5.2330       0.2720       0.9074       0.3037       0.4362       0.8525      78.1289\n",
            "          16       5.2350       0.2587       0.9119       0.3038       0.3470       0.8771      83.3639\n",
            "          17       5.2364       0.2347       0.9207       0.3038       0.4485       0.8533      88.6002\n",
            "          18       5.2392       0.2106       0.9310       0.3041       0.3413       0.8848      93.8394\n",
            "          19       5.2380       0.1959       0.9334       0.3044       0.3102       0.8965      99.0775\n",
            "          20       5.2403       0.1705       0.9444       0.3038       0.2544       0.9144     104.3177\n",
            "          21       5.2387       0.1420       0.9541       0.3036       0.2338       0.9212     109.5564\n",
            "          22       5.2408       0.1211       0.9620       0.3042       0.2115       0.9320     114.7972\n",
            "          23       5.2391       0.0963       0.9709       0.3043       0.2005       0.9335     120.0363\n",
            "          24       5.2377       0.0753       0.9784       0.3042       0.1805       0.9408     125.2740\n"
          ]
        }
      ],
      "source": [
        "epochs, batch_size = 24, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, 5, epochs], [0.0, 0.4, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "model = build_model(baseline_net, x_ent_loss)  \n",
        "warmup_cudnn(model, next(iter(train_batches(batch_size, transforms))))\n",
        "logs, state, timer = Table(), {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize)\n",
        "for epoch in range(epochs):\n",
        "    logs.append(union({'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10lxhHspp79n"
      },
      "source": [
        "Total training time (including the negligible time spent on preprocessing) is under 70s, moving us up two places to fourth on the leaderboard!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiaIqUQKWHgT"
      },
      "source": [
        "#### Aside: mixed precision training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FoPeZCcWmYR"
      },
      "source": [
        "In our original DAWNBench submission and in the code above, we simply converted the model to float16 without all the niceties of so-called [mixed precision training](https://arxiv.org/abs/1710.03740) although we include a basic sort of 'loss scaling' by summing rather than averaging losses in a batch. It is straightforward to implement proper mixed precision training but this adds about a second to overall training time and we found it to have no effect on final accuracy, so we continue to do without it below. For completeness, here is a simple implementation which might be useful elsewhere: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qvk0e9AfWMjp"
      },
      "outputs": [],
      "source": [
        "FP32_PARAMS = 'fp32_params'\n",
        "\n",
        "def copy_grads_to_master(batch, state):\n",
        "    if not batch: return\n",
        "    for float_param, param in zip(state[FP32_PARAMS], trainable_params(state[MODEL]).values()):\n",
        "        if float_param.grad is None: \n",
        "            float_param.grad = float_param.data.new(*float_param.data.size())\n",
        "        float_param.grad.data.copy_(param.grad.data)    \n",
        "    \n",
        "def update_params_from_master(batch, state):\n",
        "    if not batch: return\n",
        "    for float_param, param in zip(state[FP32_PARAMS], trainable_params(state[MODEL]).values()):\n",
        "        param.data.copy_(float_param)\n",
        "    \n",
        "train_steps_mixed_precision = (forward(training_mode=True),  log_activations(('loss', 'acc')), backward(torch.float32), copy_grads_to_master, opt_steps, update_params_from_master)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7Q9-IXXGYJFL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pdluser/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       5.1978       1.6362       0.4110       0.2959       1.5309       0.4806       5.1978\n",
            "           2       5.1746       0.9530       0.6580       0.2971       0.8653       0.7125      10.3724\n",
            "           3       5.2047       0.7285       0.7463       0.2992       0.8478       0.7218      15.5771\n",
            "           4       5.2185       0.6191       0.7845       0.3000       0.5850       0.8029      20.7955\n",
            "           5       5.2370       0.5669       0.8039       0.3007       0.6038       0.7933      26.0325\n",
            "           6       5.2412       0.4941       0.8302       0.3016       0.6205       0.7925      31.2737\n",
            "           7       5.2598       0.4492       0.8448       0.3020       0.6233       0.7911      36.5335\n",
            "           8       5.2659       0.4101       0.8588       0.3020       0.5517       0.8091      41.7994\n",
            "           9       5.2686       0.3797       0.8715       0.3023       0.4771       0.8393      47.0679\n",
            "          10       5.2744       0.3626       0.8770       0.3023       0.4159       0.8637      52.3424\n",
            "          11       5.2772       0.3416       0.8842       0.3025       0.4369       0.8485      57.6195\n",
            "          12       5.2744       0.3283       0.8887       0.3021       0.3480       0.8825      62.8939\n",
            "          13       5.2774       0.3065       0.8956       0.3029       0.4327       0.8570      68.1713\n",
            "          14       5.2799       0.2857       0.9034       0.3030       0.4062       0.8625      73.4513\n",
            "          15       5.2801       0.2697       0.9090       0.3027       0.5729       0.8167      78.7314\n",
            "          16       5.2933       0.2460       0.9168       0.3035       0.3389       0.8878      84.0246\n",
            "          17       5.2913       0.2326       0.9213       0.3035       0.3544       0.8793      89.3159\n",
            "          18       5.2930       0.2104       0.9288       0.3039       0.2948       0.9006      94.6090\n",
            "          19       5.2910       0.1840       0.9381       0.3040       0.3501       0.8791      99.9000\n",
            "          20       5.2953       0.1616       0.9475       0.3042       0.2749       0.9102     105.1953\n",
            "          21       5.2973       0.1422       0.9535       0.3040       0.2736       0.9100     110.4926\n",
            "          22       5.2949       0.1148       0.9630       0.3039       0.2180       0.9298     115.7875\n",
            "          23       5.2957       0.0962       0.9708       0.3039       0.1893       0.9370     121.0831\n",
            "          24       5.2961       0.0747       0.9788       0.3039       0.1791       0.9413     126.3792\n"
          ]
        }
      ],
      "source": [
        "epochs, batch_size =24, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, 5, epochs], [0.0, 0.4, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "model = build_model(network(), x_ent_loss)\n",
        "fp32_params = [v.clone().to(torch.float32) for v in trainable_params(model).values()]\n",
        "logs, state, timer = Table(), {MODEL: model, OPTS: [SGD(fp32_params, opt_params)], FP32_PARAMS: fp32_params}, Timer(torch.cuda.synchronize)\n",
        "for epoch in range(epochs):\n",
        "    logs.append(union({'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size), train_steps=train_steps_mixed_precision)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RpWD1rHwNoz"
      },
      "source": [
        "### Moving max-pool layers (64s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTwu1qXPwVCI"
      },
      "source": [
        "Max-pooling commutes with a monotonic-increasing activation function such as ReLU. It should be more efficient to apply pooling first. This is the sort of thing a friendly compiler might do for you, but for now let's switch the order by hand:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "vHSpmXRd-Acj",
        "outputId": "f5e673a2-23e9-411f-bff5-e454588428a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pydot is needed for network visualisation"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "conv_pool_block_opt = lambda c_in, c_out: reorder(conv_pool_block(c_in, c_out), ('conv', 'norm', 'pool', 'act'))\n",
        "show(network(conv_pool_block=conv_pool_block_opt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uCx6niO29qtZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
            "           1       5.0569       1.6416       0.4109       0.2889       1.2009       0.5575       5.0569\n",
            "           2       5.0423       0.9398       0.6649       0.2895       1.0367       0.6596      10.0992\n",
            "           3       5.0598       0.7274       0.7454       0.2903       0.6625       0.7770      15.1590\n",
            "           4       5.0651       0.6230       0.7838       0.2913       0.7818       0.7328      20.2241\n",
            "           5       5.0701       0.5519       0.8079       0.2912       0.6476       0.7792      25.2942\n",
            "           6       5.0714       0.4902       0.8306       0.2916       0.6296       0.7888      30.3656\n",
            "           7       5.0860       0.4452       0.8469       0.2921       0.5398       0.8160      35.4517\n",
            "           8       5.0907       0.4091       0.8600       0.2922       0.5207       0.8191      40.5423\n",
            "           9       5.0908       0.3844       0.8683       0.2922       0.5333       0.8155      45.6331\n",
            "          10       5.0961       0.3623       0.8765       0.2923       0.4316       0.8513      50.7292\n",
            "          11       5.0926       0.3437       0.8834       0.2924       0.4913       0.8333      55.8218\n",
            "          12       5.1003       0.3261       0.8897       0.2919       0.4233       0.8555      60.9221\n",
            "          13       5.0942       0.3083       0.8956       0.2926       0.4534       0.8493      66.0163\n",
            "          14       5.0987       0.2881       0.9021       0.2926       0.4875       0.8337      71.1150\n",
            "          15       5.0994       0.2711       0.9083       0.2928       0.4704       0.8479      76.2144\n",
            "          16       5.0983       0.2491       0.9166       0.2926       0.3649       0.8770      81.3127\n",
            "          17       5.1046       0.2285       0.9231       0.2927       0.3737       0.8757      86.4173\n",
            "          18       5.1003       0.2093       0.9312       0.2921       0.3630       0.8809      91.5176\n",
            "          19       5.1027       0.1880       0.9361       0.2926       0.2859       0.9057      96.6203\n",
            "          20       5.0967       0.1651       0.9453       0.2921       0.2837       0.9066     101.7171\n",
            "          21       5.0973       0.1410       0.9543       0.2926       0.2618       0.9168     106.8144\n",
            "          22       5.0977       0.1156       0.9642       0.2924       0.2271       0.9260     111.9121\n",
            "          23       5.0987       0.0930       0.9725       0.2926       0.1922       0.9354     117.0108\n",
            "          24       5.1014       0.0762       0.9780       0.2924       0.1785       0.9407     122.1122\n"
          ]
        }
      ],
      "source": [
        "epochs, batch_size = 24, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, 5, epochs], [0.0, 0.4, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "model = build_model(network(conv_pool_block=conv_pool_block_opt), x_ent_loss)\n",
        "logs, state, timer = Table(), {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize)\n",
        "for epoch in range(epochs):\n",
        "    logs.append(union({'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpB6j6de7keL"
      },
      "source": [
        "A further 3s off! \n",
        "\n",
        "Perhaps we should try something more radical and move max-pooling before batch norm. This will achieve a further efficiency gain but will change the network \n",
        "so we need to test the effect on training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "YfDw9V35-1Ng",
        "outputId": "7e314bfa-b3c0-4a01-c738-f38fa1a6ddd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pydot is needed for network visualisation"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "conv_pool_block_pre = lambda c_in, c_out: reorder(conv_pool_block(c_in, c_out), ('conv', 'pool', 'norm', 'act'))\n",
        "\n",
        "maxpool_pre_net = network(conv_pool_block=conv_pool_block_pre)\n",
        "show(maxpool_pre_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eblbuB2_vCh"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size = 24, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, 5, epochs], [0.0, 0.4, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(maxpool_pre_net, x_ent_loss)\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))       \n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5elIHYXi7AVz"
      },
      "source": [
        "The results show a small negative effect on test accuracy to 94.0% (mean of 50 runs) compared to our baseline of 94.1%. They also show a substantial 5s reduction in training time! We can restore the previous accuracy by adding an extra epoch to training. This is the only time in the post that we select an 'improvement' that leads to worse accuracy! The 5s gain from a more efficient network more than compensates the 2.5s loss from the extra training epoch. The net effect brings our time to 64s, up to third place on the leaderboard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7ecJCjI1SJV"
      },
      "source": [
        "### Label smoothing (59s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NqB3aXtWoel"
      },
      "source": [
        "[Label smoothing](https://arxiv.org/abs/1512.00567) is a well-established trick for improving the training speed and generalization of neural nets in classification problems. It involves blending the one-hot target probabilities with a uniform distribution over class labels inside the cross entropy loss. This helps to stabilise the output distribution and prevents the network from making overconfident predictions which might inhibit further training. Let's give it a try - the label smoothing parameter of 0.2 has been very roughly hand-optimised but the result is not too sensitive to a range of choices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "s8UjTsDLul0D",
        "outputId": "2ffce5c6-e64a-439f-d59c-2b74838e5312"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pydot is needed for network visualisation"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show(label_smoothing_loss(alpha=0.2), size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ofLeJMTpbJVc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         run        epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6998557aa37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e82a95c2c12e>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(state, timer, train_batches, valid_batches, train_steps, valid_steps, on_epoch_end)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m     \u001b[0mtrain_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m     \u001b[0mvalid_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_total\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#DAWNBench rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     return {\n",
            "\u001b[0;32m<ipython-input-1-e82a95c2c12e>\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(batches, state, steps)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;31m#need to do some tidying-up (e.g. log_activations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-e82a95c2c12e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(batch, state)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs, batch_size = 25, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.4, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(maxpool_pre_net, label_smoothing_loss(0.2))\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc5w9r2HSof1"
      },
      "source": [
        "Test accuracy improves to 94.2% (mean of 50 runs.) We can trade this for training time by reducing the number of epochs. As a rule of thumb, we reduce training by one epoch for each 0.1% of test accuracy improvement, since this roughly tracks the gain from an extra epoch of training. We reduce the warmup period - during which learning rates increase linearly - in proportion to the overall number of epochs. \n",
        "\n",
        "Accuracy for 23 epochs of training is 94.1% and training time has dipped under a minute!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ug1Qk7-YyJW"
      },
      "source": [
        "### CELU activations (52s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyAlk-aa9l8E"
      },
      "source": [
        "We might hope to help the optimisation process by using a smooth activation function, rather than ReLU with its delta-function of curvature at the origin. This may also help generalisation since smoothed functions lead to a less expressive function class - in the large smoothing limit we recover a linear network. \n",
        "\n",
        "We are otherwise happy with ReLU so we're going to pick a simple smoothed-out alternative. Our choice is the Continuously Differentiable Exponential Linear Unit or [CELU](https://arxiv.org/abs/1704.07483) activation since it is smooth (unlike ELU) and the PyTorch implementation is faster than that of the otherwise perfectly adequate Softplus activation. In addition to smoothing, CELU applies an x- and y-shift to ReLU as shown below, but these are largely irrelevant given our use of batch norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "K7g9dIFu1jB_",
        "outputId": "c9238162-2a23-47f6-88ec-7aad73f62283"
      },
      "outputs": [],
      "source": [
        "alpha = 0.075\n",
        "\n",
        "funcs = {\n",
        "    'CELU(x; Î±)': nn.CELU(alpha),\n",
        "    'ReLU(x+Î±)-Î±': lambda x: torch.nn.functional.relu(x+alpha) - alpha,\n",
        "}\n",
        "\n",
        "xs = torch.tensor(np.arange(-1, 1, 0.01))\n",
        "data = pd.DataFrame([{'func': k, 'x': float(to_numpy(x)), 'y': float(to_numpy(f(x)))} for x in xs for k,f in funcs.items()])\n",
        "# alt.Chart(data).mark_line().encode(x='x', y='y', color='func')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-28Zj2A3chKd"
      },
      "source": [
        "Let's see how it works  - as usual the smoothing parameter $\\alpha$ has been roughly manually tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "x8KwIQjrGQlR",
        "outputId": "ce717079-d602-40c3-c2f3-b43f34685f24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pydot is needed for network visualisation"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "celu_net = network(conv_pool_block=conv_pool_block_pre, types={nn.ReLU: partial(nn.CELU, alpha=0.075)})\n",
        "show(celu_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cH7GrPu5dWna"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         run        epoch   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n"
          ]
        }
      ],
      "source": [
        "epochs, batch_size = 23, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.4, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(celu_net, label_smoothing_loss(0.2))\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uu5xtfMc1gK"
      },
      "source": [
        "This gives an impressive improvement to 94.3% test accuracy (mean of 50 runs) allowing a further 3 epoch reduction in training and a 20 epoch time of 52s for 94.1% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HoNA_JwSkC_"
      },
      "source": [
        "### Ghost batch norm (46s)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1F8M7LfV8dE"
      },
      "source": [
        "Batch norm seems to work best with batch size of around 32. The reasons presumably have to do with noise in the batch statistics and specifically a balance between a beneficial regularising effect at intermediate batch sizes and an excess of noise at small batches.\n",
        "\n",
        "Our batches are of size 512 and we can't afford to reduce them without taking a serious hit on training times, but we can apply batch norm separately to subsets of a training batch. This technique, known as 'ghost' batch norm, is usually used in a distributed setting but is just as useful when using large batches on a single node. It isn't supported directly in PyTorch but we can roll our own easily enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "HwmZ3fJZT6sP",
        "outputId": "faff87b8-5481-4e62-e0df-f989b799ff7d"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"1080pt\" height=\"59pt\"\n viewBox=\"0.00 0.00 1080.00 59.24\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.310167 0.310167) rotate(0) translate(4 187)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-187 3478,-187 3478,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\"><title>cluster_prep</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M94,-48C94,-48 320,-48 320,-48 326,-48 332,-54 332,-60 332,-60 332,-111 332,-111 332,-117 326,-123 320,-123 320,-123 94,-123 94,-123 88,-123 82,-117 82,-111 82,-111 82,-60 82,-60 82,-54 88,-48 94,-48\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-107.8\" font-family=\"Times,serif\" font-size=\"14.00\">prep</text>\n</g>\n<g id=\"clust2\" class=\"cluster\"><title>cluster_layer1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M364,-12C364,-12 1498,-12 1498,-12 1504,-12 1510,-18 1510,-24 1510,-24 1510,-163 1510,-163 1510,-169 1504,-175 1498,-175 1498,-175 364,-175 364,-175 358,-175 352,-169 352,-163 352,-163 352,-24 352,-24 352,-18 358,-12 364,-12\"/>\n<text text-anchor=\"middle\" x=\"931\" y=\"-159.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer1</text>\n</g>\n<g id=\"clust3\" class=\"cluster\"><title>cluster_layer1_residual</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M724,-20C724,-20 1490,-20 1490,-20 1496,-20 1502,-26 1502,-32 1502,-32 1502,-132 1502,-132 1502,-138 1496,-144 1490,-144 1490,-144 724,-144 724,-144 718,-144 712,-138 712,-132 712,-132 712,-32 712,-32 712,-26 718,-20 724,-20\"/>\n<text text-anchor=\"middle\" x=\"1107\" y=\"-128.8\" font-family=\"Times,serif\" font-size=\"14.00\">residual</text>\n</g>\n<g id=\"clust4\" class=\"cluster\"><title>cluster_layer1_residual_res1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M814,-28C814,-28 1040,-28 1040,-28 1046,-28 1052,-34 1052,-40 1052,-40 1052,-91 1052,-91 1052,-97 1046,-103 1040,-103 1040,-103 814,-103 814,-103 808,-103 802,-97 802,-91 802,-91 802,-40 802,-40 802,-34 808,-28 814,-28\"/>\n<text text-anchor=\"middle\" x=\"927\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">res1</text>\n</g>\n<g id=\"clust5\" class=\"cluster\"><title>cluster_layer1_residual_res2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1084,-28C1084,-28 1310,-28 1310,-28 1316,-28 1322,-34 1322,-40 1322,-40 1322,-91 1322,-91 1322,-97 1316,-103 1310,-103 1310,-103 1084,-103 1084,-103 1078,-103 1072,-97 1072,-91 1072,-91 1072,-40 1072,-40 1072,-34 1078,-28 1084,-28\"/>\n<text text-anchor=\"middle\" x=\"1197\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">res2</text>\n</g>\n<g id=\"clust6\" class=\"cluster\"><title>cluster_layer2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1534,-53C1534,-53 1850,-53 1850,-53 1856,-53 1862,-59 1862,-65 1862,-65 1862,-116 1862,-116 1862,-122 1856,-128 1850,-128 1850,-128 1534,-128 1534,-128 1528,-128 1522,-122 1522,-116 1522,-116 1522,-65 1522,-65 1522,-59 1528,-53 1534,-53\"/>\n<text text-anchor=\"middle\" x=\"1692\" y=\"-112.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer2</text>\n</g>\n<g id=\"clust7\" class=\"cluster\"><title>cluster_layer3</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1894,-8C1894,-8 3028,-8 3028,-8 3034,-8 3040,-14 3040,-20 3040,-20 3040,-159 3040,-159 3040,-165 3034,-171 3028,-171 3028,-171 1894,-171 1894,-171 1888,-171 1882,-165 1882,-159 1882,-159 1882,-20 1882,-20 1882,-14 1888,-8 1894,-8\"/>\n<text text-anchor=\"middle\" x=\"2461\" y=\"-155.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer3</text>\n</g>\n<g id=\"clust8\" class=\"cluster\"><title>cluster_layer3_residual</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2254,-16C2254,-16 3020,-16 3020,-16 3026,-16 3032,-22 3032,-28 3032,-28 3032,-128 3032,-128 3032,-134 3026,-140 3020,-140 3020,-140 2254,-140 2254,-140 2248,-140 2242,-134 2242,-128 2242,-128 2242,-28 2242,-28 2242,-22 2248,-16 2254,-16\"/>\n<text text-anchor=\"middle\" x=\"2637\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">residual</text>\n</g>\n<g id=\"clust9\" class=\"cluster\"><title>cluster_layer3_residual_res1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2344,-24C2344,-24 2570,-24 2570,-24 2576,-24 2582,-30 2582,-36 2582,-36 2582,-87 2582,-87 2582,-93 2576,-99 2570,-99 2570,-99 2344,-99 2344,-99 2338,-99 2332,-93 2332,-87 2332,-87 2332,-36 2332,-36 2332,-30 2338,-24 2344,-24\"/>\n<text text-anchor=\"middle\" x=\"2457\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\">res1</text>\n</g>\n<g id=\"clust10\" class=\"cluster\"><title>cluster_layer3_residual_res2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2614,-24C2614,-24 2840,-24 2840,-24 2846,-24 2852,-30 2852,-36 2852,-36 2852,-87 2852,-87 2852,-93 2846,-99 2840,-99 2840,-99 2614,-99 2614,-99 2608,-99 2602,-93 2602,-87 2602,-87 2602,-36 2602,-36 2602,-30 2608,-24 2614,-24\"/>\n<text text-anchor=\"middle\" x=\"2727\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\">res2</text>\n</g>\n<g id=\"clust11\" class=\"cluster\"><title>cluster_classifier</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M3154,-49C3154,-49 3380,-49 3380,-49 3386,-49 3392,-55 3392,-61 3392,-61 3392,-112 3392,-112 3392,-118 3386,-124 3380,-124 3380,-124 3154,-124 3154,-124 3148,-124 3142,-118 3142,-112 3142,-112 3142,-61 3142,-61 3142,-55 3148,-49 3154,-49\"/>\n<text text-anchor=\"middle\" x=\"3267\" y=\"-108.8\" font-family=\"Times,serif\" font-size=\"14.00\">classifier</text>\n</g>\n<!-- prep_conv -->\n<g id=\"node1\" class=\"node\"><title>prep_conv</title>\n<g id=\"a_node1\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 3, &#39;out_channels&#39;: 64, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M132,-92C132,-92 102,-92 102,-92 96,-92 90,-86 90,-80 90,-80 90,-68 90,-68 90,-62 96,-56 102,-56 102,-56 132,-56 132,-56 138,-56 144,-62 144,-68 144,-68 144,-80 144,-80 144,-86 138,-92 132,-92\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- prep_norm -->\n<g id=\"node2\" class=\"node\"><title>prep_norm</title>\n<g id=\"a_node2\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16) {&#39;num_features&#39;: 64}\">\n<path fill=\"#4dddf8\" stroke=\"black\" d=\"M222,-92C222,-92 192,-92 192,-92 186,-92 180,-86 180,-80 180,-80 180,-68 180,-68 180,-62 186,-56 192,-56 192,-56 222,-56 222,-56 228,-56 234,-62 234,-68 234,-68 234,-80 234,-80 234,-86 228,-92 222,-92\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- prep_conv&#45;&gt;prep_norm -->\n<g id=\"edge2\" class=\"edge\"><title>prep_conv&#45;&gt;prep_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M144.403,-74C152.393,-74 161.311,-74 169.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"169.919,-77.5001 179.919,-74 169.919,-70.5001 169.919,-77.5001\"/>\n</g>\n<!-- prep_act -->\n<g id=\"node3\" class=\"node\"><title>prep_act</title>\n<g id=\"a_node3\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.075) {}\">\n<path fill=\"#e66493\" stroke=\"black\" d=\"M312,-92C312,-92 282,-92 282,-92 276,-92 270,-86 270,-80 270,-80 270,-68 270,-68 270,-62 276,-56 282,-56 282,-56 312,-56 312,-56 318,-56 324,-62 324,-68 324,-68 324,-80 324,-80 324,-86 318,-92 312,-92\"/>\n<text text-anchor=\"middle\" x=\"297\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- prep_norm&#45;&gt;prep_act -->\n<g id=\"edge3\" class=\"edge\"><title>prep_norm&#45;&gt;prep_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M234.403,-74C242.393,-74 251.311,-74 259.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"259.919,-77.5001 269.919,-74 259.919,-70.5001 259.919,-77.5001\"/>\n</g>\n<!-- layer1_conv -->\n<g id=\"node4\" class=\"node\"><title>layer1_conv</title>\n<g id=\"a_node4\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 64, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M402,-92C402,-92 372,-92 372,-92 366,-92 360,-86 360,-80 360,-80 360,-68 360,-68 360,-62 366,-56 372,-56 372,-56 402,-56 402,-56 408,-56 414,-62 414,-68 414,-68 414,-80 414,-80 414,-86 408,-92 402,-92\"/>\n<text text-anchor=\"middle\" x=\"387\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- prep_act&#45;&gt;layer1_conv -->\n<g id=\"edge4\" class=\"edge\"><title>prep_act&#45;&gt;layer1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M324.403,-74C332.393,-74 341.311,-74 349.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"349.919,-77.5001 359.919,-74 349.919,-70.5001 349.919,-77.5001\"/>\n</g>\n<!-- layer1_pool -->\n<g id=\"node5\" class=\"node\"><title>layer1_pool</title>\n<g id=\"a_node5\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M492,-92C492,-92 462,-92 462,-92 456,-92 450,-86 450,-80 450,-80 450,-68 450,-68 450,-62 456,-56 462,-56 462,-56 492,-56 492,-56 498,-56 504,-62 504,-68 504,-68 504,-80 504,-80 504,-86 498,-92 492,-92\"/>\n<text text-anchor=\"middle\" x=\"477\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer1_conv&#45;&gt;layer1_pool -->\n<g id=\"edge5\" class=\"edge\"><title>layer1_conv&#45;&gt;layer1_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M414.403,-74C422.393,-74 431.311,-74 439.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"439.919,-77.5001 449.919,-74 439.919,-70.5001 439.919,-77.5001\"/>\n</g>\n<!-- layer1_norm -->\n<g id=\"node6\" class=\"node\"><title>layer1_norm</title>\n<g id=\"a_node6\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16) {&#39;num_features&#39;: 128}\">\n<path fill=\"#4dddf8\" stroke=\"black\" d=\"M582,-92C582,-92 552,-92 552,-92 546,-92 540,-86 540,-80 540,-80 540,-68 540,-68 540,-62 546,-56 552,-56 552,-56 582,-56 582,-56 588,-56 594,-62 594,-68 594,-68 594,-80 594,-80 594,-86 588,-92 582,-92\"/>\n<text text-anchor=\"middle\" x=\"567\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_pool&#45;&gt;layer1_norm -->\n<g id=\"edge6\" class=\"edge\"><title>layer1_pool&#45;&gt;layer1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M504.403,-74C512.393,-74 521.311,-74 529.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"529.919,-77.5001 539.919,-74 529.919,-70.5001 529.919,-77.5001\"/>\n</g>\n<!-- layer1_act -->\n<g id=\"node7\" class=\"node\"><title>layer1_act</title>\n<g id=\"a_node7\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.075) {}\">\n<path fill=\"#e66493\" stroke=\"black\" d=\"M672,-92C672,-92 642,-92 642,-92 636,-92 630,-86 630,-80 630,-80 630,-68 630,-68 630,-62 636,-56 642,-56 642,-56 672,-56 672,-56 678,-56 684,-62 684,-68 684,-68 684,-80 684,-80 684,-86 678,-92 672,-92\"/>\n<text text-anchor=\"middle\" x=\"657\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_norm&#45;&gt;layer1_act -->\n<g id=\"edge7\" class=\"edge\"><title>layer1_norm&#45;&gt;layer1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M594.403,-74C602.393,-74 611.311,-74 619.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"619.919,-77.5001 629.919,-74 619.919,-70.5001 619.919,-77.5001\"/>\n</g>\n<!-- layer1_residual_in -->\n<g id=\"node8\" class=\"node\"><title>layer1_residual_in</title>\n<g id=\"a_node8\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M762,-92C762,-92 732,-92 732,-92 726,-92 720,-86 720,-80 720,-80 720,-68 720,-68 720,-62 726,-56 732,-56 732,-56 762,-56 762,-56 768,-56 774,-62 774,-68 774,-68 774,-80 774,-80 774,-86 768,-92 762,-92\"/>\n<text text-anchor=\"middle\" x=\"747\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">in</text>\n</a>\n</g>\n</g>\n<!-- layer1_act&#45;&gt;layer1_residual_in -->\n<g id=\"edge8\" class=\"edge\"><title>layer1_act&#45;&gt;layer1_residual_in</title>\n<path fill=\"none\" stroke=\"black\" d=\"M684.403,-74C692.393,-74 701.311,-74 709.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"709.919,-77.5001 719.919,-74 709.919,-70.5001 709.919,-77.5001\"/>\n</g>\n<!-- layer1_residual_res1_conv -->\n<g id=\"node9\" class=\"node\"><title>layer1_residual_res1_conv</title>\n<g id=\"a_node9\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M852,-72C852,-72 822,-72 822,-72 816,-72 810,-66 810,-60 810,-60 810,-48 810,-48 810,-42 816,-36 822,-36 822,-36 852,-36 852,-36 858,-36 864,-42 864,-48 864,-48 864,-60 864,-60 864,-66 858,-72 852,-72\"/>\n<text text-anchor=\"middle\" x=\"837\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_in&#45;&gt;layer1_residual_res1_conv -->\n<g id=\"edge9\" class=\"edge\"><title>layer1_residual_in&#45;&gt;layer1_residual_res1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M774.403,-67.9993C782.481,-66.1634 791.507,-64.1121 800.105,-62.158\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"800.943,-65.5568 809.919,-59.9275 799.392,-58.7308 800.943,-65.5568\"/>\n</g>\n<!-- layer1_residual_add -->\n<g id=\"node16\" class=\"node\"><title>layer1_residual_add</title>\n<g id=\"a_node16\"><a xlink:title=\"&lt;class &#39;__main__.Add&#39;&gt; {}\">\n<path fill=\"#fdb462\" stroke=\"black\" d=\"M1482,-97C1482,-97 1452,-97 1452,-97 1446,-97 1440,-91 1440,-85 1440,-85 1440,-73 1440,-73 1440,-67 1446,-61 1452,-61 1452,-61 1482,-61 1482,-61 1488,-61 1494,-67 1494,-73 1494,-73 1494,-85 1494,-85 1494,-91 1488,-97 1482,-97\"/>\n<text text-anchor=\"middle\" x=\"1467\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">add</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_in&#45;&gt;layer1_residual_add -->\n<g id=\"edge16\" class=\"edge\"><title>layer1_residual_in&#45;&gt;layer1_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M771.737,-92.1018C780.735,-97.9639 791.381,-103.78 802,-107 854.782,-123.006 870.844,-112 926,-112 926,-112 926,-112 1288,-112 1337.77,-112 1394.08,-99.3711 1429.8,-89.7384\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1431.04,-93.0265 1439.74,-86.9884 1429.17,-86.2796 1431.04,-93.0265\"/>\n</g>\n<!-- layer1_residual_res1_norm -->\n<g id=\"node10\" class=\"node\"><title>layer1_residual_res1_norm</title>\n<g id=\"a_node10\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16) {&#39;num_features&#39;: 128}\">\n<path fill=\"#4dddf8\" stroke=\"black\" d=\"M942,-72C942,-72 912,-72 912,-72 906,-72 900,-66 900,-60 900,-60 900,-48 900,-48 900,-42 906,-36 912,-36 912,-36 942,-36 942,-36 948,-36 954,-42 954,-48 954,-48 954,-60 954,-60 954,-66 948,-72 942,-72\"/>\n<text text-anchor=\"middle\" x=\"927\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_conv&#45;&gt;layer1_residual_res1_norm -->\n<g id=\"edge10\" class=\"edge\"><title>layer1_residual_res1_conv&#45;&gt;layer1_residual_res1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M864.403,-54C872.393,-54 881.311,-54 889.824,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"889.919,-57.5001 899.919,-54 889.919,-50.5001 889.919,-57.5001\"/>\n</g>\n<!-- layer1_residual_res1_act -->\n<g id=\"node11\" class=\"node\"><title>layer1_residual_res1_act</title>\n<g id=\"a_node11\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.075) {}\">\n<path fill=\"#e66493\" stroke=\"black\" d=\"M1032,-72C1032,-72 1002,-72 1002,-72 996,-72 990,-66 990,-60 990,-60 990,-48 990,-48 990,-42 996,-36 1002,-36 1002,-36 1032,-36 1032,-36 1038,-36 1044,-42 1044,-48 1044,-48 1044,-60 1044,-60 1044,-66 1038,-72 1032,-72\"/>\n<text text-anchor=\"middle\" x=\"1017\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_norm&#45;&gt;layer1_residual_res1_act -->\n<g id=\"edge11\" class=\"edge\"><title>layer1_residual_res1_norm&#45;&gt;layer1_residual_res1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M954.403,-54C962.393,-54 971.311,-54 979.824,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"979.919,-57.5001 989.919,-54 979.919,-50.5001 979.919,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_conv -->\n<g id=\"node12\" class=\"node\"><title>layer1_residual_res2_conv</title>\n<g id=\"a_node12\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M1122,-72C1122,-72 1092,-72 1092,-72 1086,-72 1080,-66 1080,-60 1080,-60 1080,-48 1080,-48 1080,-42 1086,-36 1092,-36 1092,-36 1122,-36 1122,-36 1128,-36 1134,-42 1134,-48 1134,-48 1134,-60 1134,-60 1134,-66 1128,-72 1122,-72\"/>\n<text text-anchor=\"middle\" x=\"1107\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_act&#45;&gt;layer1_residual_res2_conv -->\n<g id=\"edge12\" class=\"edge\"><title>layer1_residual_res1_act&#45;&gt;layer1_residual_res2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1044.4,-54C1052.39,-54 1061.31,-54 1069.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1069.92,-57.5001 1079.92,-54 1069.92,-50.5001 1069.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_norm -->\n<g id=\"node13\" class=\"node\"><title>layer1_residual_res2_norm</title>\n<g id=\"a_node13\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16) {&#39;num_features&#39;: 128}\">\n<path fill=\"#4dddf8\" stroke=\"black\" d=\"M1212,-72C1212,-72 1182,-72 1182,-72 1176,-72 1170,-66 1170,-60 1170,-60 1170,-48 1170,-48 1170,-42 1176,-36 1182,-36 1182,-36 1212,-36 1212,-36 1218,-36 1224,-42 1224,-48 1224,-48 1224,-60 1224,-60 1224,-66 1218,-72 1212,-72\"/>\n<text text-anchor=\"middle\" x=\"1197\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_conv&#45;&gt;layer1_residual_res2_norm -->\n<g id=\"edge13\" class=\"edge\"><title>layer1_residual_res2_conv&#45;&gt;layer1_residual_res2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1134.4,-54C1142.39,-54 1151.31,-54 1159.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1159.92,-57.5001 1169.92,-54 1159.92,-50.5001 1159.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_act -->\n<g id=\"node14\" class=\"node\"><title>layer1_residual_res2_act</title>\n<g id=\"a_node14\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.075) {}\">\n<path fill=\"#e66493\" stroke=\"black\" d=\"M1302,-72C1302,-72 1272,-72 1272,-72 1266,-72 1260,-66 1260,-60 1260,-60 1260,-48 1260,-48 1260,-42 1266,-36 1272,-36 1272,-36 1302,-36 1302,-36 1308,-36 1314,-42 1314,-48 1314,-48 1314,-60 1314,-60 1314,-66 1308,-72 1302,-72\"/>\n<text text-anchor=\"middle\" x=\"1287\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_norm&#45;&gt;layer1_residual_res2_act -->\n<g id=\"edge14\" class=\"edge\"><title>layer1_residual_res2_norm&#45;&gt;layer1_residual_res2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1224.4,-54C1232.39,-54 1241.31,-54 1249.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1249.92,-57.5001 1259.92,-54 1249.92,-50.5001 1249.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_out -->\n<g id=\"node15\" class=\"node\"><title>layer1_residual_out</title>\n<g id=\"a_node15\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M1392,-82C1392,-82 1362,-82 1362,-82 1356,-82 1350,-76 1350,-70 1350,-70 1350,-58 1350,-58 1350,-52 1356,-46 1362,-46 1362,-46 1392,-46 1392,-46 1398,-46 1404,-52 1404,-58 1404,-58 1404,-70 1404,-70 1404,-76 1398,-82 1392,-82\"/>\n<text text-anchor=\"middle\" x=\"1377\" y=\"-60.3\" font-family=\"Times,serif\" font-size=\"14.00\">out</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_act&#45;&gt;layer1_residual_out -->\n<g id=\"edge15\" class=\"edge\"><title>layer1_residual_res2_act&#45;&gt;layer1_residual_out</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1314.4,-57.0003C1322.39,-57.9083 1331.31,-58.9217 1339.82,-59.8891\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1339.59,-63.3847 1349.92,-61.0362 1340.38,-56.4294 1339.59,-63.3847\"/>\n</g>\n<!-- layer1_residual_out&#45;&gt;layer1_residual_add -->\n<g id=\"edge17\" class=\"edge\"><title>layer1_residual_out&#45;&gt;layer1_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1404.4,-68.5005C1412.39,-69.8625 1421.31,-71.3825 1429.82,-72.8336\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1429.47,-76.3242 1439.92,-74.5544 1430.65,-69.4237 1429.47,-76.3242\"/>\n</g>\n<!-- layer2_conv -->\n<g id=\"node17\" class=\"node\"><title>layer2_conv</title>\n<g id=\"a_node17\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 256, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M1572,-97C1572,-97 1542,-97 1542,-97 1536,-97 1530,-91 1530,-85 1530,-85 1530,-73 1530,-73 1530,-67 1536,-61 1542,-61 1542,-61 1572,-61 1572,-61 1578,-61 1584,-67 1584,-73 1584,-73 1584,-85 1584,-85 1584,-91 1578,-97 1572,-97\"/>\n<text text-anchor=\"middle\" x=\"1557\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_add&#45;&gt;layer2_conv -->\n<g id=\"edge18\" class=\"edge\"><title>layer1_residual_add&#45;&gt;layer2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1494.4,-79C1502.39,-79 1511.31,-79 1519.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1519.92,-82.5001 1529.92,-79 1519.92,-75.5001 1519.92,-82.5001\"/>\n</g>\n<!-- layer2_pool -->\n<g id=\"node18\" class=\"node\"><title>layer2_pool</title>\n<g id=\"a_node18\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M1662,-97C1662,-97 1632,-97 1632,-97 1626,-97 1620,-91 1620,-85 1620,-85 1620,-73 1620,-73 1620,-67 1626,-61 1632,-61 1632,-61 1662,-61 1662,-61 1668,-61 1674,-67 1674,-73 1674,-73 1674,-85 1674,-85 1674,-91 1668,-97 1662,-97\"/>\n<text text-anchor=\"middle\" x=\"1647\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer2_conv&#45;&gt;layer2_pool -->\n<g id=\"edge19\" class=\"edge\"><title>layer2_conv&#45;&gt;layer2_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1584.4,-79C1592.39,-79 1601.31,-79 1609.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1609.92,-82.5001 1619.92,-79 1609.92,-75.5001 1609.92,-82.5001\"/>\n</g>\n<!-- layer2_norm -->\n<g id=\"node19\" class=\"node\"><title>layer2_norm</title>\n<g id=\"a_node19\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16) {&#39;num_features&#39;: 256}\">\n<path fill=\"#4dddf8\" stroke=\"black\" d=\"M1752,-97C1752,-97 1722,-97 1722,-97 1716,-97 1710,-91 1710,-85 1710,-85 1710,-73 1710,-73 1710,-67 1716,-61 1722,-61 1722,-61 1752,-61 1752,-61 1758,-61 1764,-67 1764,-73 1764,-73 1764,-85 1764,-85 1764,-91 1758,-97 1752,-97\"/>\n<text text-anchor=\"middle\" x=\"1737\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer2_pool&#45;&gt;layer2_norm -->\n<g id=\"edge20\" class=\"edge\"><title>layer2_pool&#45;&gt;layer2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1674.4,-79C1682.39,-79 1691.31,-79 1699.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1699.92,-82.5001 1709.92,-79 1699.92,-75.5001 1699.92,-82.5001\"/>\n</g>\n<!-- layer2_act -->\n<g id=\"node20\" class=\"node\"><title>layer2_act</title>\n<g id=\"a_node20\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.075) {}\">\n<path fill=\"#e66493\" stroke=\"black\" d=\"M1842,-97C1842,-97 1812,-97 1812,-97 1806,-97 1800,-91 1800,-85 1800,-85 1800,-73 1800,-73 1800,-67 1806,-61 1812,-61 1812,-61 1842,-61 1842,-61 1848,-61 1854,-67 1854,-73 1854,-73 1854,-85 1854,-85 1854,-91 1848,-97 1842,-97\"/>\n<text text-anchor=\"middle\" x=\"1827\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer2_norm&#45;&gt;layer2_act -->\n<g id=\"edge21\" class=\"edge\"><title>layer2_norm&#45;&gt;layer2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1764.4,-79C1772.39,-79 1781.31,-79 1789.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1789.92,-82.5001 1799.92,-79 1789.92,-75.5001 1789.92,-82.5001\"/>\n</g>\n<!-- layer3_conv -->\n<g id=\"node21\" class=\"node\"><title>layer3_conv</title>\n<g id=\"a_node21\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 256, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M1932,-97C1932,-97 1902,-97 1902,-97 1896,-97 1890,-91 1890,-85 1890,-85 1890,-73 1890,-73 1890,-67 1896,-61 1902,-61 1902,-61 1932,-61 1932,-61 1938,-61 1944,-67 1944,-73 1944,-73 1944,-85 1944,-85 1944,-91 1938,-97 1932,-97\"/>\n<text text-anchor=\"middle\" x=\"1917\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer2_act&#45;&gt;layer3_conv -->\n<g id=\"edge22\" class=\"edge\"><title>layer2_act&#45;&gt;layer3_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1854.4,-79C1862.39,-79 1871.31,-79 1879.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1879.92,-82.5001 1889.92,-79 1879.92,-75.5001 1879.92,-82.5001\"/>\n</g>\n<!-- layer3_pool -->\n<g id=\"node22\" class=\"node\"><title>layer3_pool</title>\n<g id=\"a_node22\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M2022,-97C2022,-97 1992,-97 1992,-97 1986,-97 1980,-91 1980,-85 1980,-85 1980,-73 1980,-73 1980,-67 1986,-61 1992,-61 1992,-61 2022,-61 2022,-61 2028,-61 2034,-67 2034,-73 2034,-73 2034,-85 2034,-85 2034,-91 2028,-97 2022,-97\"/>\n<text text-anchor=\"middle\" x=\"2007\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer3_conv&#45;&gt;layer3_pool -->\n<g id=\"edge23\" class=\"edge\"><title>layer3_conv&#45;&gt;layer3_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1944.4,-79C1952.39,-79 1961.31,-79 1969.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1969.92,-82.5001 1979.92,-79 1969.92,-75.5001 1969.92,-82.5001\"/>\n</g>\n<!-- layer3_norm -->\n<g id=\"node23\" class=\"node\"><title>layer3_norm</title>\n<g id=\"a_node23\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16) {&#39;num_features&#39;: 512}\">\n<path fill=\"#4dddf8\" stroke=\"black\" d=\"M2112,-97C2112,-97 2082,-97 2082,-97 2076,-97 2070,-91 2070,-85 2070,-85 2070,-73 2070,-73 2070,-67 2076,-61 2082,-61 2082,-61 2112,-61 2112,-61 2118,-61 2124,-67 2124,-73 2124,-73 2124,-85 2124,-85 2124,-91 2118,-97 2112,-97\"/>\n<text text-anchor=\"middle\" x=\"2097\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_pool&#45;&gt;layer3_norm -->\n<g id=\"edge24\" class=\"edge\"><title>layer3_pool&#45;&gt;layer3_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2034.4,-79C2042.39,-79 2051.31,-79 2059.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2059.92,-82.5001 2069.92,-79 2059.92,-75.5001 2059.92,-82.5001\"/>\n</g>\n<!-- layer3_act -->\n<g id=\"node24\" class=\"node\"><title>layer3_act</title>\n<g id=\"a_node24\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.075) {}\">\n<path fill=\"#e66493\" stroke=\"black\" d=\"M2202,-97C2202,-97 2172,-97 2172,-97 2166,-97 2160,-91 2160,-85 2160,-85 2160,-73 2160,-73 2160,-67 2166,-61 2172,-61 2172,-61 2202,-61 2202,-61 2208,-61 2214,-67 2214,-73 2214,-73 2214,-85 2214,-85 2214,-91 2208,-97 2202,-97\"/>\n<text text-anchor=\"middle\" x=\"2187\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_norm&#45;&gt;layer3_act -->\n<g id=\"edge25\" class=\"edge\"><title>layer3_norm&#45;&gt;layer3_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2124.4,-79C2132.39,-79 2141.31,-79 2149.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2149.92,-82.5001 2159.92,-79 2149.92,-75.5001 2149.92,-82.5001\"/>\n</g>\n<!-- layer3_residual_in -->\n<g id=\"node25\" class=\"node\"><title>layer3_residual_in</title>\n<g id=\"a_node25\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M2292,-97C2292,-97 2262,-97 2262,-97 2256,-97 2250,-91 2250,-85 2250,-85 2250,-73 2250,-73 2250,-67 2256,-61 2262,-61 2262,-61 2292,-61 2292,-61 2298,-61 2304,-67 2304,-73 2304,-73 2304,-85 2304,-85 2304,-91 2298,-97 2292,-97\"/>\n<text text-anchor=\"middle\" x=\"2277\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">in</text>\n</a>\n</g>\n</g>\n<!-- layer3_act&#45;&gt;layer3_residual_in -->\n<g id=\"edge26\" class=\"edge\"><title>layer3_act&#45;&gt;layer3_residual_in</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2214.4,-79C2222.39,-79 2231.31,-79 2239.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2239.92,-82.5001 2249.92,-79 2239.92,-75.5001 2239.92,-82.5001\"/>\n</g>\n<!-- layer3_residual_res1_conv -->\n<g id=\"node26\" class=\"node\"><title>layer3_residual_res1_conv</title>\n<g id=\"a_node26\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 512, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M2382,-68C2382,-68 2352,-68 2352,-68 2346,-68 2340,-62 2340,-56 2340,-56 2340,-44 2340,-44 2340,-38 2346,-32 2352,-32 2352,-32 2382,-32 2382,-32 2388,-32 2394,-38 2394,-44 2394,-44 2394,-56 2394,-56 2394,-62 2388,-68 2382,-68\"/>\n<text text-anchor=\"middle\" x=\"2367\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_in&#45;&gt;layer3_residual_res1_conv -->\n<g id=\"edge27\" class=\"edge\"><title>layer3_residual_in&#45;&gt;layer3_residual_res1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2304.4,-70.299C2312.57,-67.608 2321.7,-64.5979 2330.38,-61.7368\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2331.52,-65.049 2339.92,-58.5949 2329.33,-58.4007 2331.52,-65.049\"/>\n</g>\n<!-- layer3_residual_add -->\n<g id=\"node33\" class=\"node\"><title>layer3_residual_add</title>\n<g id=\"a_node33\"><a xlink:title=\"&lt;class &#39;__main__.Add&#39;&gt; {}\">\n<path fill=\"#fdb462\" stroke=\"black\" d=\"M3012,-93C3012,-93 2982,-93 2982,-93 2976,-93 2970,-87 2970,-81 2970,-81 2970,-69 2970,-69 2970,-63 2976,-57 2982,-57 2982,-57 3012,-57 3012,-57 3018,-57 3024,-63 3024,-69 3024,-69 3024,-81 3024,-81 3024,-87 3018,-93 3012,-93\"/>\n<text text-anchor=\"middle\" x=\"2997\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">add</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_in&#45;&gt;layer3_residual_add -->\n<g id=\"edge34\" class=\"edge\"><title>layer3_residual_in&#45;&gt;layer3_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2304.22,-93.0359C2312.83,-96.9904 2322.61,-100.808 2332,-103 2385.71,-115.535 2400.84,-108 2456,-108 2456,-108 2456,-108 2818,-108 2867.77,-108 2924.08,-95.3711 2959.8,-85.7384\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2961.04,-89.0265 2969.74,-82.9884 2959.17,-82.2796 2961.04,-89.0265\"/>\n</g>\n<!-- layer3_residual_res1_norm -->\n<g id=\"node27\" class=\"node\"><title>layer3_residual_res1_norm</title>\n<g id=\"a_node27\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16) {&#39;num_features&#39;: 512}\">\n<path fill=\"#4dddf8\" stroke=\"black\" d=\"M2472,-68C2472,-68 2442,-68 2442,-68 2436,-68 2430,-62 2430,-56 2430,-56 2430,-44 2430,-44 2430,-38 2436,-32 2442,-32 2442,-32 2472,-32 2472,-32 2478,-32 2484,-38 2484,-44 2484,-44 2484,-56 2484,-56 2484,-62 2478,-68 2472,-68\"/>\n<text text-anchor=\"middle\" x=\"2457\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_conv&#45;&gt;layer3_residual_res1_norm -->\n<g id=\"edge28\" class=\"edge\"><title>layer3_residual_res1_conv&#45;&gt;layer3_residual_res1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2394.4,-50C2402.39,-50 2411.31,-50 2419.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2419.92,-53.5001 2429.92,-50 2419.92,-46.5001 2419.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res1_act -->\n<g id=\"node28\" class=\"node\"><title>layer3_residual_res1_act</title>\n<g id=\"a_node28\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.075) {}\">\n<path fill=\"#e66493\" stroke=\"black\" d=\"M2562,-68C2562,-68 2532,-68 2532,-68 2526,-68 2520,-62 2520,-56 2520,-56 2520,-44 2520,-44 2520,-38 2526,-32 2532,-32 2532,-32 2562,-32 2562,-32 2568,-32 2574,-38 2574,-44 2574,-44 2574,-56 2574,-56 2574,-62 2568,-68 2562,-68\"/>\n<text text-anchor=\"middle\" x=\"2547\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_norm&#45;&gt;layer3_residual_res1_act -->\n<g id=\"edge29\" class=\"edge\"><title>layer3_residual_res1_norm&#45;&gt;layer3_residual_res1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2484.4,-50C2492.39,-50 2501.31,-50 2509.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2509.92,-53.5001 2519.92,-50 2509.92,-46.5001 2509.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_conv -->\n<g id=\"node29\" class=\"node\"><title>layer3_residual_res2_conv</title>\n<g id=\"a_node29\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 512, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M2652,-68C2652,-68 2622,-68 2622,-68 2616,-68 2610,-62 2610,-56 2610,-56 2610,-44 2610,-44 2610,-38 2616,-32 2622,-32 2622,-32 2652,-32 2652,-32 2658,-32 2664,-38 2664,-44 2664,-44 2664,-56 2664,-56 2664,-62 2658,-68 2652,-68\"/>\n<text text-anchor=\"middle\" x=\"2637\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_act&#45;&gt;layer3_residual_res2_conv -->\n<g id=\"edge30\" class=\"edge\"><title>layer3_residual_res1_act&#45;&gt;layer3_residual_res2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2574.4,-50C2582.39,-50 2591.31,-50 2599.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2599.92,-53.5001 2609.92,-50 2599.92,-46.5001 2599.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_norm -->\n<g id=\"node30\" class=\"node\"><title>layer3_residual_res2_norm</title>\n<g id=\"a_node30\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16) {&#39;num_features&#39;: 512}\">\n<path fill=\"#4dddf8\" stroke=\"black\" d=\"M2742,-68C2742,-68 2712,-68 2712,-68 2706,-68 2700,-62 2700,-56 2700,-56 2700,-44 2700,-44 2700,-38 2706,-32 2712,-32 2712,-32 2742,-32 2742,-32 2748,-32 2754,-38 2754,-44 2754,-44 2754,-56 2754,-56 2754,-62 2748,-68 2742,-68\"/>\n<text text-anchor=\"middle\" x=\"2727\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_conv&#45;&gt;layer3_residual_res2_norm -->\n<g id=\"edge31\" class=\"edge\"><title>layer3_residual_res2_conv&#45;&gt;layer3_residual_res2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2664.4,-50C2672.39,-50 2681.31,-50 2689.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2689.92,-53.5001 2699.92,-50 2689.92,-46.5001 2689.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_act -->\n<g id=\"node31\" class=\"node\"><title>layer3_residual_res2_act</title>\n<g id=\"a_node31\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.075) {}\">\n<path fill=\"#e66493\" stroke=\"black\" d=\"M2832,-68C2832,-68 2802,-68 2802,-68 2796,-68 2790,-62 2790,-56 2790,-56 2790,-44 2790,-44 2790,-38 2796,-32 2802,-32 2802,-32 2832,-32 2832,-32 2838,-32 2844,-38 2844,-44 2844,-44 2844,-56 2844,-56 2844,-62 2838,-68 2832,-68\"/>\n<text text-anchor=\"middle\" x=\"2817\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_norm&#45;&gt;layer3_residual_res2_act -->\n<g id=\"edge32\" class=\"edge\"><title>layer3_residual_res2_norm&#45;&gt;layer3_residual_res2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2754.4,-50C2762.39,-50 2771.31,-50 2779.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2779.92,-53.5001 2789.92,-50 2779.92,-46.5001 2779.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_out -->\n<g id=\"node32\" class=\"node\"><title>layer3_residual_out</title>\n<g id=\"a_node32\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M2922,-78C2922,-78 2892,-78 2892,-78 2886,-78 2880,-72 2880,-66 2880,-66 2880,-54 2880,-54 2880,-48 2886,-42 2892,-42 2892,-42 2922,-42 2922,-42 2928,-42 2934,-48 2934,-54 2934,-54 2934,-66 2934,-66 2934,-72 2928,-78 2922,-78\"/>\n<text text-anchor=\"middle\" x=\"2907\" y=\"-56.3\" font-family=\"Times,serif\" font-size=\"14.00\">out</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_act&#45;&gt;layer3_residual_out -->\n<g id=\"edge33\" class=\"edge\"><title>layer3_residual_res2_act&#45;&gt;layer3_residual_out</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2844.4,-53.0003C2852.39,-53.9083 2861.31,-54.9217 2869.82,-55.8891\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2869.59,-59.3847 2879.92,-57.0362 2870.38,-52.4294 2869.59,-59.3847\"/>\n</g>\n<!-- layer3_residual_out&#45;&gt;layer3_residual_add -->\n<g id=\"edge35\" class=\"edge\"><title>layer3_residual_out&#45;&gt;layer3_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2934.4,-64.5005C2942.39,-65.8625 2951.31,-67.3825 2959.82,-68.8336\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2959.47,-72.3242 2969.92,-70.5544 2960.65,-65.4237 2959.47,-72.3242\"/>\n</g>\n<!-- pool -->\n<g id=\"node34\" class=\"node\"><title>pool</title>\n<g id=\"a_node34\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 4}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M3102,-93C3102,-93 3072,-93 3072,-93 3066,-93 3060,-87 3060,-81 3060,-81 3060,-69 3060,-69 3060,-63 3066,-57 3072,-57 3072,-57 3102,-57 3102,-57 3108,-57 3114,-63 3114,-69 3114,-69 3114,-81 3114,-81 3114,-87 3108,-93 3102,-93\"/>\n<text text-anchor=\"middle\" x=\"3087\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_add&#45;&gt;pool -->\n<g id=\"edge36\" class=\"edge\"><title>layer3_residual_add&#45;&gt;pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3024.4,-75C3032.39,-75 3041.31,-75 3049.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3049.92,-78.5001 3059.92,-75 3049.92,-71.5001 3049.92,-78.5001\"/>\n</g>\n<!-- classifier_flatten -->\n<g id=\"node35\" class=\"node\"><title>classifier_flatten</title>\n<g id=\"a_node35\"><a xlink:title=\"&lt;class &#39;__main__.Flatten&#39;&gt; {}\">\n<path fill=\"#b3de69\" stroke=\"black\" d=\"M3192,-93C3192,-93 3162,-93 3162,-93 3156,-93 3150,-87 3150,-81 3150,-81 3150,-69 3150,-69 3150,-63 3156,-57 3162,-57 3162,-57 3192,-57 3192,-57 3198,-57 3204,-63 3204,-69 3204,-69 3204,-81 3204,-81 3204,-87 3198,-93 3192,-93\"/>\n<text text-anchor=\"middle\" x=\"3177\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">flatten</text>\n</a>\n</g>\n</g>\n<!-- pool&#45;&gt;classifier_flatten -->\n<g id=\"edge37\" class=\"edge\"><title>pool&#45;&gt;classifier_flatten</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3114.4,-75C3122.39,-75 3131.31,-75 3139.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3139.92,-78.5001 3149.92,-75 3139.92,-71.5001 3139.92,-78.5001\"/>\n</g>\n<!-- classifier_conv -->\n<g id=\"node36\" class=\"node\"><title>classifier_conv</title>\n<g id=\"a_node36\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.linear.Linear&#39;&gt; {&#39;in_features&#39;: 512, &#39;out_features&#39;: 10, &#39;bias&#39;: False}\">\n<path fill=\"#fccde5\" stroke=\"black\" d=\"M3282,-93C3282,-93 3252,-93 3252,-93 3246,-93 3240,-87 3240,-81 3240,-81 3240,-69 3240,-69 3240,-63 3246,-57 3252,-57 3252,-57 3282,-57 3282,-57 3288,-57 3294,-63 3294,-69 3294,-69 3294,-81 3294,-81 3294,-87 3288,-93 3282,-93\"/>\n<text text-anchor=\"middle\" x=\"3267\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- classifier_flatten&#45;&gt;classifier_conv -->\n<g id=\"edge38\" class=\"edge\"><title>classifier_flatten&#45;&gt;classifier_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3204.4,-75C3212.39,-75 3221.31,-75 3229.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3229.92,-78.5001 3239.92,-75 3229.92,-71.5001 3229.92,-78.5001\"/>\n</g>\n<!-- classifier_scale -->\n<g id=\"node37\" class=\"node\"><title>classifier_scale</title>\n<g id=\"a_node37\"><a xlink:title=\"&lt;class &#39;__main__.Mul&#39;&gt; {&#39;weight&#39;: 0.125}\">\n<path fill=\"#bc80bd\" stroke=\"black\" d=\"M3372,-93C3372,-93 3342,-93 3342,-93 3336,-93 3330,-87 3330,-81 3330,-81 3330,-69 3330,-69 3330,-63 3336,-57 3342,-57 3342,-57 3372,-57 3372,-57 3378,-57 3384,-63 3384,-69 3384,-69 3384,-81 3384,-81 3384,-87 3378,-93 3372,-93\"/>\n<text text-anchor=\"middle\" x=\"3357\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">scale</text>\n</a>\n</g>\n</g>\n<!-- classifier_conv&#45;&gt;classifier_scale -->\n<g id=\"edge39\" class=\"edge\"><title>classifier_conv&#45;&gt;classifier_scale</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3294.4,-75C3302.39,-75 3311.31,-75 3319.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3319.92,-78.5001 3329.92,-75 3319.92,-71.5001 3319.92,-78.5001\"/>\n</g>\n<!-- logits -->\n<g id=\"node38\" class=\"node\"><title>logits</title>\n<g id=\"a_node38\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M3462,-93C3462,-93 3432,-93 3432,-93 3426,-93 3420,-87 3420,-81 3420,-81 3420,-69 3420,-69 3420,-63 3426,-57 3432,-57 3432,-57 3462,-57 3462,-57 3468,-57 3474,-63 3474,-69 3474,-69 3474,-81 3474,-81 3474,-87 3468,-93 3462,-93\"/>\n<text text-anchor=\"middle\" x=\"3447\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">logits</text>\n</a>\n</g>\n</g>\n<!-- classifier_scale&#45;&gt;logits -->\n<g id=\"edge40\" class=\"edge\"><title>classifier_scale&#45;&gt;logits</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3384.4,-75C3392.39,-75 3401.31,-75 3409.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3409.92,-78.5001 3419.92,-75 3409.92,-71.5001 3409.92,-78.5001\"/>\n</g>\n<!-- input -->\n<g id=\"node39\" class=\"node\"><title>input</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M42,-92C42,-92 12,-92 12,-92 6,-92 0,-86 0,-80 0,-80 0,-68 0,-68 0,-62 6,-56 12,-56 12,-56 42,-56 42,-56 48,-56 54,-62 54,-68 54,-68 54,-80 54,-80 54,-86 48,-92 42,-92\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">input</text>\n</g>\n<!-- input&#45;&gt;prep_conv -->\n<g id=\"edge1\" class=\"edge\"><title>input&#45;&gt;prep_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.4029,-74C62.3932,-74 71.3106,-74 79.8241,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"79.919,-77.5001 89.919,-74 79.919,-70.5001 79.919,-77.5001\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<__main__.DotGraph at 0x7f4140a9ba58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "ghost_bn_net = network(conv_pool_block=conv_pool_block_pre, types={\n",
        "    nn.ReLU: partial(nn.CELU, 0.075), \n",
        "    BatchNorm: partial(GhostBatchNorm, num_splits=16)\n",
        "})\n",
        "show(ghost_bn_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAUyBrtqN7gj"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size = 20, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.4, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs, 'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(ghost_bn_net, label_smoothing_loss(0.2))\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyvPyD2xc77U"
      },
      "source": [
        "This gives a healthy boost to the 20 epoch test accuracy of 94.2%. As training becomes ever shorter, it is occasionally helpful to increase the learning rate to compensate. If we raise the max learning rate by 50% we can achieve 94.1% accuracy in 18 epochs and a training time of 46s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-gEHrSYp94Z"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size = 18, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.6, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs, 'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(ghost_bn_net, label_smoothing_loss(0.2))\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRrXu8g5eoQ1"
      },
      "source": [
        "### Frozen batch norm scales (43s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbafQdo_er-7"
      },
      "source": [
        "Batch norm standardises the mean and variance of each channel but is followed by a learnable scale and bias. Our batch norm layers are succeeded by ReLUs, so the learnable biases could allow the network to optimise the level of sparsity per channel. On the other hand, if channel scales vary substantially this might reduce the effective number of channels and introduce a bottleneck. Let's have a look at the dynamics of these parameters during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KbCvLxqq8VI"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size = 18, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.6, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "model = build_model(ghost_bn_net, label_smoothing_loss(0.2))\n",
        "state, timer, logs = {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize), Table(report=every(epochs, 'epoch'))\n",
        "for epoch in range(epochs):\n",
        "    logs.append(union({'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size), \n",
        "                                                      on_epoch_end=partial(log_weights, weights={k: v for k,v in trainable_params(model).items() if 'norm' in k}))))\n",
        "data = pd.DataFrame([{'epoch': epoch, 'type': weight.split('.')[1], 'layer': weight.split('_norm')[0], 'channel': channel, 'value': value} \n",
        "                     for epoch, epoch_vals in enumerate(state[WEIGHT_LOG], 1) \n",
        "                     for weight, weight_vals in epoch_vals.items() \n",
        "                     for channel, value in enumerate(weight_vals[:8], 1)])\n",
        "\n",
        "alt.Chart(data).mark_line(opacity=0.7).encode(x='epoch', y='value', color=alt.Color('layer', sort=['prep']), detail='channel', column='type').resolve_scale(y='independent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMvagTTwx1wO"
      },
      "source": [
        "There's a lot going on in these plots, but one thing that sticks out is that the scales are not doing much learning and evolve largely under the control of weight decay. Let's try freezing these at a constant value of $1/4$ - roughly their average at the midpoint of training. The learnable scale for the final layer is somewhat larger but we can adjust the scaling of the network output to compensate. \n",
        "\n",
        "Actually we can fix the batch norm scales to $1$ instead if we rescale the $\\alpha$ parameter of CELU by a compensating factor of $4$ and the learning rate and weight decay for the batch norm biases by $4^2$ and $1/4^2$ respectively. We prefer to do things this way since it makes the impact of the channel scales on the learning rate dynamics of the biases more explicit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "lkSuchyvLSa4",
        "outputId": "10f9be49-cf0a-4d94-b740-0b846d436c85"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"1080pt\" height=\"59pt\"\n viewBox=\"0.00 0.00 1080.00 59.24\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.310167 0.310167) rotate(0) translate(4 187)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-187 3478,-187 3478,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\"><title>cluster_prep</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M94,-48C94,-48 320,-48 320,-48 326,-48 332,-54 332,-60 332,-60 332,-111 332,-111 332,-117 326,-123 320,-123 320,-123 94,-123 94,-123 88,-123 82,-117 82,-111 82,-111 82,-60 82,-60 82,-54 88,-48 94,-48\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-107.8\" font-family=\"Times,serif\" font-size=\"14.00\">prep</text>\n</g>\n<g id=\"clust2\" class=\"cluster\"><title>cluster_layer1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M364,-12C364,-12 1498,-12 1498,-12 1504,-12 1510,-18 1510,-24 1510,-24 1510,-163 1510,-163 1510,-169 1504,-175 1498,-175 1498,-175 364,-175 364,-175 358,-175 352,-169 352,-163 352,-163 352,-24 352,-24 352,-18 358,-12 364,-12\"/>\n<text text-anchor=\"middle\" x=\"931\" y=\"-159.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer1</text>\n</g>\n<g id=\"clust3\" class=\"cluster\"><title>cluster_layer1_residual</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M724,-20C724,-20 1490,-20 1490,-20 1496,-20 1502,-26 1502,-32 1502,-32 1502,-132 1502,-132 1502,-138 1496,-144 1490,-144 1490,-144 724,-144 724,-144 718,-144 712,-138 712,-132 712,-132 712,-32 712,-32 712,-26 718,-20 724,-20\"/>\n<text text-anchor=\"middle\" x=\"1107\" y=\"-128.8\" font-family=\"Times,serif\" font-size=\"14.00\">residual</text>\n</g>\n<g id=\"clust4\" class=\"cluster\"><title>cluster_layer1_residual_res1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M814,-28C814,-28 1040,-28 1040,-28 1046,-28 1052,-34 1052,-40 1052,-40 1052,-91 1052,-91 1052,-97 1046,-103 1040,-103 1040,-103 814,-103 814,-103 808,-103 802,-97 802,-91 802,-91 802,-40 802,-40 802,-34 808,-28 814,-28\"/>\n<text text-anchor=\"middle\" x=\"927\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">res1</text>\n</g>\n<g id=\"clust5\" class=\"cluster\"><title>cluster_layer1_residual_res2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1084,-28C1084,-28 1310,-28 1310,-28 1316,-28 1322,-34 1322,-40 1322,-40 1322,-91 1322,-91 1322,-97 1316,-103 1310,-103 1310,-103 1084,-103 1084,-103 1078,-103 1072,-97 1072,-91 1072,-91 1072,-40 1072,-40 1072,-34 1078,-28 1084,-28\"/>\n<text text-anchor=\"middle\" x=\"1197\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">res2</text>\n</g>\n<g id=\"clust6\" class=\"cluster\"><title>cluster_layer2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1534,-53C1534,-53 1850,-53 1850,-53 1856,-53 1862,-59 1862,-65 1862,-65 1862,-116 1862,-116 1862,-122 1856,-128 1850,-128 1850,-128 1534,-128 1534,-128 1528,-128 1522,-122 1522,-116 1522,-116 1522,-65 1522,-65 1522,-59 1528,-53 1534,-53\"/>\n<text text-anchor=\"middle\" x=\"1692\" y=\"-112.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer2</text>\n</g>\n<g id=\"clust7\" class=\"cluster\"><title>cluster_layer3</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1894,-8C1894,-8 3028,-8 3028,-8 3034,-8 3040,-14 3040,-20 3040,-20 3040,-159 3040,-159 3040,-165 3034,-171 3028,-171 3028,-171 1894,-171 1894,-171 1888,-171 1882,-165 1882,-159 1882,-159 1882,-20 1882,-20 1882,-14 1888,-8 1894,-8\"/>\n<text text-anchor=\"middle\" x=\"2461\" y=\"-155.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer3</text>\n</g>\n<g id=\"clust8\" class=\"cluster\"><title>cluster_layer3_residual</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2254,-16C2254,-16 3020,-16 3020,-16 3026,-16 3032,-22 3032,-28 3032,-28 3032,-128 3032,-128 3032,-134 3026,-140 3020,-140 3020,-140 2254,-140 2254,-140 2248,-140 2242,-134 2242,-128 2242,-128 2242,-28 2242,-28 2242,-22 2248,-16 2254,-16\"/>\n<text text-anchor=\"middle\" x=\"2637\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">residual</text>\n</g>\n<g id=\"clust9\" class=\"cluster\"><title>cluster_layer3_residual_res1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2344,-24C2344,-24 2570,-24 2570,-24 2576,-24 2582,-30 2582,-36 2582,-36 2582,-87 2582,-87 2582,-93 2576,-99 2570,-99 2570,-99 2344,-99 2344,-99 2338,-99 2332,-93 2332,-87 2332,-87 2332,-36 2332,-36 2332,-30 2338,-24 2344,-24\"/>\n<text text-anchor=\"middle\" x=\"2457\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\">res1</text>\n</g>\n<g id=\"clust10\" class=\"cluster\"><title>cluster_layer3_residual_res2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2614,-24C2614,-24 2840,-24 2840,-24 2846,-24 2852,-30 2852,-36 2852,-36 2852,-87 2852,-87 2852,-93 2846,-99 2840,-99 2840,-99 2614,-99 2614,-99 2608,-99 2602,-93 2602,-87 2602,-87 2602,-36 2602,-36 2602,-30 2608,-24 2614,-24\"/>\n<text text-anchor=\"middle\" x=\"2727\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\">res2</text>\n</g>\n<g id=\"clust11\" class=\"cluster\"><title>cluster_classifier</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M3154,-49C3154,-49 3380,-49 3380,-49 3386,-49 3392,-55 3392,-61 3392,-61 3392,-112 3392,-112 3392,-118 3386,-124 3380,-124 3380,-124 3154,-124 3154,-124 3148,-124 3142,-118 3142,-112 3142,-112 3142,-61 3142,-61 3142,-55 3148,-49 3154,-49\"/>\n<text text-anchor=\"middle\" x=\"3267\" y=\"-108.8\" font-family=\"Times,serif\" font-size=\"14.00\">classifier</text>\n</g>\n<!-- prep_conv -->\n<g id=\"node1\" class=\"node\"><title>prep_conv</title>\n<g id=\"a_node1\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 3, &#39;out_channels&#39;: 64, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M132,-92C132,-92 102,-92 102,-92 96,-92 90,-86 90,-80 90,-80 90,-68 90,-68 90,-62 96,-56 102,-56 102,-56 132,-56 132,-56 138,-56 144,-62 144,-68 144,-68 144,-80 144,-80 144,-86 138,-92 132,-92\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- prep_norm -->\n<g id=\"node2\" class=\"node\"><title>prep_norm</title>\n<g id=\"a_node2\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 64}\">\n<path fill=\"#b07b87\" stroke=\"black\" d=\"M222,-92C222,-92 192,-92 192,-92 186,-92 180,-86 180,-80 180,-80 180,-68 180,-68 180,-62 186,-56 192,-56 192,-56 222,-56 222,-56 228,-56 234,-62 234,-68 234,-68 234,-80 234,-80 234,-86 228,-92 222,-92\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- prep_conv&#45;&gt;prep_norm -->\n<g id=\"edge2\" class=\"edge\"><title>prep_conv&#45;&gt;prep_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M144.403,-74C152.393,-74 161.311,-74 169.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"169.919,-77.5001 179.919,-74 169.919,-70.5001 169.919,-77.5001\"/>\n</g>\n<!-- prep_act -->\n<g id=\"node3\" class=\"node\"><title>prep_act</title>\n<g id=\"a_node3\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#4e90e3\" stroke=\"black\" d=\"M312,-92C312,-92 282,-92 282,-92 276,-92 270,-86 270,-80 270,-80 270,-68 270,-68 270,-62 276,-56 282,-56 282,-56 312,-56 312,-56 318,-56 324,-62 324,-68 324,-68 324,-80 324,-80 324,-86 318,-92 312,-92\"/>\n<text text-anchor=\"middle\" x=\"297\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- prep_norm&#45;&gt;prep_act -->\n<g id=\"edge3\" class=\"edge\"><title>prep_norm&#45;&gt;prep_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M234.403,-74C242.393,-74 251.311,-74 259.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"259.919,-77.5001 269.919,-74 259.919,-70.5001 259.919,-77.5001\"/>\n</g>\n<!-- layer1_conv -->\n<g id=\"node4\" class=\"node\"><title>layer1_conv</title>\n<g id=\"a_node4\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 64, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M402,-92C402,-92 372,-92 372,-92 366,-92 360,-86 360,-80 360,-80 360,-68 360,-68 360,-62 366,-56 372,-56 372,-56 402,-56 402,-56 408,-56 414,-62 414,-68 414,-68 414,-80 414,-80 414,-86 408,-92 402,-92\"/>\n<text text-anchor=\"middle\" x=\"387\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- prep_act&#45;&gt;layer1_conv -->\n<g id=\"edge4\" class=\"edge\"><title>prep_act&#45;&gt;layer1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M324.403,-74C332.393,-74 341.311,-74 349.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"349.919,-77.5001 359.919,-74 349.919,-70.5001 349.919,-77.5001\"/>\n</g>\n<!-- layer1_pool -->\n<g id=\"node5\" class=\"node\"><title>layer1_pool</title>\n<g id=\"a_node5\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M492,-92C492,-92 462,-92 462,-92 456,-92 450,-86 450,-80 450,-80 450,-68 450,-68 450,-62 456,-56 462,-56 462,-56 492,-56 492,-56 498,-56 504,-62 504,-68 504,-68 504,-80 504,-80 504,-86 498,-92 492,-92\"/>\n<text text-anchor=\"middle\" x=\"477\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer1_conv&#45;&gt;layer1_pool -->\n<g id=\"edge5\" class=\"edge\"><title>layer1_conv&#45;&gt;layer1_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M414.403,-74C422.393,-74 431.311,-74 439.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"439.919,-77.5001 449.919,-74 439.919,-70.5001 439.919,-77.5001\"/>\n</g>\n<!-- layer1_norm -->\n<g id=\"node6\" class=\"node\"><title>layer1_norm</title>\n<g id=\"a_node6\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 128}\">\n<path fill=\"#b07b87\" stroke=\"black\" d=\"M582,-92C582,-92 552,-92 552,-92 546,-92 540,-86 540,-80 540,-80 540,-68 540,-68 540,-62 546,-56 552,-56 552,-56 582,-56 582,-56 588,-56 594,-62 594,-68 594,-68 594,-80 594,-80 594,-86 588,-92 582,-92\"/>\n<text text-anchor=\"middle\" x=\"567\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_pool&#45;&gt;layer1_norm -->\n<g id=\"edge6\" class=\"edge\"><title>layer1_pool&#45;&gt;layer1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M504.403,-74C512.393,-74 521.311,-74 529.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"529.919,-77.5001 539.919,-74 529.919,-70.5001 529.919,-77.5001\"/>\n</g>\n<!-- layer1_act -->\n<g id=\"node7\" class=\"node\"><title>layer1_act</title>\n<g id=\"a_node7\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#4e90e3\" stroke=\"black\" d=\"M672,-92C672,-92 642,-92 642,-92 636,-92 630,-86 630,-80 630,-80 630,-68 630,-68 630,-62 636,-56 642,-56 642,-56 672,-56 672,-56 678,-56 684,-62 684,-68 684,-68 684,-80 684,-80 684,-86 678,-92 672,-92\"/>\n<text text-anchor=\"middle\" x=\"657\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_norm&#45;&gt;layer1_act -->\n<g id=\"edge7\" class=\"edge\"><title>layer1_norm&#45;&gt;layer1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M594.403,-74C602.393,-74 611.311,-74 619.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"619.919,-77.5001 629.919,-74 619.919,-70.5001 619.919,-77.5001\"/>\n</g>\n<!-- layer1_residual_in -->\n<g id=\"node8\" class=\"node\"><title>layer1_residual_in</title>\n<g id=\"a_node8\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M762,-92C762,-92 732,-92 732,-92 726,-92 720,-86 720,-80 720,-80 720,-68 720,-68 720,-62 726,-56 732,-56 732,-56 762,-56 762,-56 768,-56 774,-62 774,-68 774,-68 774,-80 774,-80 774,-86 768,-92 762,-92\"/>\n<text text-anchor=\"middle\" x=\"747\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">in</text>\n</a>\n</g>\n</g>\n<!-- layer1_act&#45;&gt;layer1_residual_in -->\n<g id=\"edge8\" class=\"edge\"><title>layer1_act&#45;&gt;layer1_residual_in</title>\n<path fill=\"none\" stroke=\"black\" d=\"M684.403,-74C692.393,-74 701.311,-74 709.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"709.919,-77.5001 719.919,-74 709.919,-70.5001 709.919,-77.5001\"/>\n</g>\n<!-- layer1_residual_res1_conv -->\n<g id=\"node9\" class=\"node\"><title>layer1_residual_res1_conv</title>\n<g id=\"a_node9\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M852,-72C852,-72 822,-72 822,-72 816,-72 810,-66 810,-60 810,-60 810,-48 810,-48 810,-42 816,-36 822,-36 822,-36 852,-36 852,-36 858,-36 864,-42 864,-48 864,-48 864,-60 864,-60 864,-66 858,-72 852,-72\"/>\n<text text-anchor=\"middle\" x=\"837\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_in&#45;&gt;layer1_residual_res1_conv -->\n<g id=\"edge9\" class=\"edge\"><title>layer1_residual_in&#45;&gt;layer1_residual_res1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M774.403,-67.9993C782.481,-66.1634 791.507,-64.1121 800.105,-62.158\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"800.943,-65.5568 809.919,-59.9275 799.392,-58.7308 800.943,-65.5568\"/>\n</g>\n<!-- layer1_residual_add -->\n<g id=\"node16\" class=\"node\"><title>layer1_residual_add</title>\n<g id=\"a_node16\"><a xlink:title=\"&lt;class &#39;__main__.Add&#39;&gt; {}\">\n<path fill=\"#fdb462\" stroke=\"black\" d=\"M1482,-97C1482,-97 1452,-97 1452,-97 1446,-97 1440,-91 1440,-85 1440,-85 1440,-73 1440,-73 1440,-67 1446,-61 1452,-61 1452,-61 1482,-61 1482,-61 1488,-61 1494,-67 1494,-73 1494,-73 1494,-85 1494,-85 1494,-91 1488,-97 1482,-97\"/>\n<text text-anchor=\"middle\" x=\"1467\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">add</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_in&#45;&gt;layer1_residual_add -->\n<g id=\"edge16\" class=\"edge\"><title>layer1_residual_in&#45;&gt;layer1_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M771.737,-92.1018C780.735,-97.9639 791.381,-103.78 802,-107 854.782,-123.006 870.844,-112 926,-112 926,-112 926,-112 1288,-112 1337.77,-112 1394.08,-99.3711 1429.8,-89.7384\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1431.04,-93.0265 1439.74,-86.9884 1429.17,-86.2796 1431.04,-93.0265\"/>\n</g>\n<!-- layer1_residual_res1_norm -->\n<g id=\"node10\" class=\"node\"><title>layer1_residual_res1_norm</title>\n<g id=\"a_node10\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 128}\">\n<path fill=\"#b07b87\" stroke=\"black\" d=\"M942,-72C942,-72 912,-72 912,-72 906,-72 900,-66 900,-60 900,-60 900,-48 900,-48 900,-42 906,-36 912,-36 912,-36 942,-36 942,-36 948,-36 954,-42 954,-48 954,-48 954,-60 954,-60 954,-66 948,-72 942,-72\"/>\n<text text-anchor=\"middle\" x=\"927\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_conv&#45;&gt;layer1_residual_res1_norm -->\n<g id=\"edge10\" class=\"edge\"><title>layer1_residual_res1_conv&#45;&gt;layer1_residual_res1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M864.403,-54C872.393,-54 881.311,-54 889.824,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"889.919,-57.5001 899.919,-54 889.919,-50.5001 889.919,-57.5001\"/>\n</g>\n<!-- layer1_residual_res1_act -->\n<g id=\"node11\" class=\"node\"><title>layer1_residual_res1_act</title>\n<g id=\"a_node11\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#4e90e3\" stroke=\"black\" d=\"M1032,-72C1032,-72 1002,-72 1002,-72 996,-72 990,-66 990,-60 990,-60 990,-48 990,-48 990,-42 996,-36 1002,-36 1002,-36 1032,-36 1032,-36 1038,-36 1044,-42 1044,-48 1044,-48 1044,-60 1044,-60 1044,-66 1038,-72 1032,-72\"/>\n<text text-anchor=\"middle\" x=\"1017\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_norm&#45;&gt;layer1_residual_res1_act -->\n<g id=\"edge11\" class=\"edge\"><title>layer1_residual_res1_norm&#45;&gt;layer1_residual_res1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M954.403,-54C962.393,-54 971.311,-54 979.824,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"979.919,-57.5001 989.919,-54 979.919,-50.5001 979.919,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_conv -->\n<g id=\"node12\" class=\"node\"><title>layer1_residual_res2_conv</title>\n<g id=\"a_node12\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M1122,-72C1122,-72 1092,-72 1092,-72 1086,-72 1080,-66 1080,-60 1080,-60 1080,-48 1080,-48 1080,-42 1086,-36 1092,-36 1092,-36 1122,-36 1122,-36 1128,-36 1134,-42 1134,-48 1134,-48 1134,-60 1134,-60 1134,-66 1128,-72 1122,-72\"/>\n<text text-anchor=\"middle\" x=\"1107\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_act&#45;&gt;layer1_residual_res2_conv -->\n<g id=\"edge12\" class=\"edge\"><title>layer1_residual_res1_act&#45;&gt;layer1_residual_res2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1044.4,-54C1052.39,-54 1061.31,-54 1069.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1069.92,-57.5001 1079.92,-54 1069.92,-50.5001 1069.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_norm -->\n<g id=\"node13\" class=\"node\"><title>layer1_residual_res2_norm</title>\n<g id=\"a_node13\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 128}\">\n<path fill=\"#b07b87\" stroke=\"black\" d=\"M1212,-72C1212,-72 1182,-72 1182,-72 1176,-72 1170,-66 1170,-60 1170,-60 1170,-48 1170,-48 1170,-42 1176,-36 1182,-36 1182,-36 1212,-36 1212,-36 1218,-36 1224,-42 1224,-48 1224,-48 1224,-60 1224,-60 1224,-66 1218,-72 1212,-72\"/>\n<text text-anchor=\"middle\" x=\"1197\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_conv&#45;&gt;layer1_residual_res2_norm -->\n<g id=\"edge13\" class=\"edge\"><title>layer1_residual_res2_conv&#45;&gt;layer1_residual_res2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1134.4,-54C1142.39,-54 1151.31,-54 1159.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1159.92,-57.5001 1169.92,-54 1159.92,-50.5001 1159.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_act -->\n<g id=\"node14\" class=\"node\"><title>layer1_residual_res2_act</title>\n<g id=\"a_node14\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#4e90e3\" stroke=\"black\" d=\"M1302,-72C1302,-72 1272,-72 1272,-72 1266,-72 1260,-66 1260,-60 1260,-60 1260,-48 1260,-48 1260,-42 1266,-36 1272,-36 1272,-36 1302,-36 1302,-36 1308,-36 1314,-42 1314,-48 1314,-48 1314,-60 1314,-60 1314,-66 1308,-72 1302,-72\"/>\n<text text-anchor=\"middle\" x=\"1287\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_norm&#45;&gt;layer1_residual_res2_act -->\n<g id=\"edge14\" class=\"edge\"><title>layer1_residual_res2_norm&#45;&gt;layer1_residual_res2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1224.4,-54C1232.39,-54 1241.31,-54 1249.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1249.92,-57.5001 1259.92,-54 1249.92,-50.5001 1249.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_out -->\n<g id=\"node15\" class=\"node\"><title>layer1_residual_out</title>\n<g id=\"a_node15\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M1392,-82C1392,-82 1362,-82 1362,-82 1356,-82 1350,-76 1350,-70 1350,-70 1350,-58 1350,-58 1350,-52 1356,-46 1362,-46 1362,-46 1392,-46 1392,-46 1398,-46 1404,-52 1404,-58 1404,-58 1404,-70 1404,-70 1404,-76 1398,-82 1392,-82\"/>\n<text text-anchor=\"middle\" x=\"1377\" y=\"-60.3\" font-family=\"Times,serif\" font-size=\"14.00\">out</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_act&#45;&gt;layer1_residual_out -->\n<g id=\"edge15\" class=\"edge\"><title>layer1_residual_res2_act&#45;&gt;layer1_residual_out</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1314.4,-57.0003C1322.39,-57.9083 1331.31,-58.9217 1339.82,-59.8891\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1339.59,-63.3847 1349.92,-61.0362 1340.38,-56.4294 1339.59,-63.3847\"/>\n</g>\n<!-- layer1_residual_out&#45;&gt;layer1_residual_add -->\n<g id=\"edge17\" class=\"edge\"><title>layer1_residual_out&#45;&gt;layer1_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1404.4,-68.5005C1412.39,-69.8625 1421.31,-71.3825 1429.82,-72.8336\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1429.47,-76.3242 1439.92,-74.5544 1430.65,-69.4237 1429.47,-76.3242\"/>\n</g>\n<!-- layer2_conv -->\n<g id=\"node17\" class=\"node\"><title>layer2_conv</title>\n<g id=\"a_node17\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 256, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M1572,-97C1572,-97 1542,-97 1542,-97 1536,-97 1530,-91 1530,-85 1530,-85 1530,-73 1530,-73 1530,-67 1536,-61 1542,-61 1542,-61 1572,-61 1572,-61 1578,-61 1584,-67 1584,-73 1584,-73 1584,-85 1584,-85 1584,-91 1578,-97 1572,-97\"/>\n<text text-anchor=\"middle\" x=\"1557\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_add&#45;&gt;layer2_conv -->\n<g id=\"edge18\" class=\"edge\"><title>layer1_residual_add&#45;&gt;layer2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1494.4,-79C1502.39,-79 1511.31,-79 1519.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1519.92,-82.5001 1529.92,-79 1519.92,-75.5001 1519.92,-82.5001\"/>\n</g>\n<!-- layer2_pool -->\n<g id=\"node18\" class=\"node\"><title>layer2_pool</title>\n<g id=\"a_node18\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M1662,-97C1662,-97 1632,-97 1632,-97 1626,-97 1620,-91 1620,-85 1620,-85 1620,-73 1620,-73 1620,-67 1626,-61 1632,-61 1632,-61 1662,-61 1662,-61 1668,-61 1674,-67 1674,-73 1674,-73 1674,-85 1674,-85 1674,-91 1668,-97 1662,-97\"/>\n<text text-anchor=\"middle\" x=\"1647\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer2_conv&#45;&gt;layer2_pool -->\n<g id=\"edge19\" class=\"edge\"><title>layer2_conv&#45;&gt;layer2_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1584.4,-79C1592.39,-79 1601.31,-79 1609.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1609.92,-82.5001 1619.92,-79 1609.92,-75.5001 1609.92,-82.5001\"/>\n</g>\n<!-- layer2_norm -->\n<g id=\"node19\" class=\"node\"><title>layer2_norm</title>\n<g id=\"a_node19\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 256}\">\n<path fill=\"#b07b87\" stroke=\"black\" d=\"M1752,-97C1752,-97 1722,-97 1722,-97 1716,-97 1710,-91 1710,-85 1710,-85 1710,-73 1710,-73 1710,-67 1716,-61 1722,-61 1722,-61 1752,-61 1752,-61 1758,-61 1764,-67 1764,-73 1764,-73 1764,-85 1764,-85 1764,-91 1758,-97 1752,-97\"/>\n<text text-anchor=\"middle\" x=\"1737\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer2_pool&#45;&gt;layer2_norm -->\n<g id=\"edge20\" class=\"edge\"><title>layer2_pool&#45;&gt;layer2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1674.4,-79C1682.39,-79 1691.31,-79 1699.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1699.92,-82.5001 1709.92,-79 1699.92,-75.5001 1699.92,-82.5001\"/>\n</g>\n<!-- layer2_act -->\n<g id=\"node20\" class=\"node\"><title>layer2_act</title>\n<g id=\"a_node20\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#4e90e3\" stroke=\"black\" d=\"M1842,-97C1842,-97 1812,-97 1812,-97 1806,-97 1800,-91 1800,-85 1800,-85 1800,-73 1800,-73 1800,-67 1806,-61 1812,-61 1812,-61 1842,-61 1842,-61 1848,-61 1854,-67 1854,-73 1854,-73 1854,-85 1854,-85 1854,-91 1848,-97 1842,-97\"/>\n<text text-anchor=\"middle\" x=\"1827\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer2_norm&#45;&gt;layer2_act -->\n<g id=\"edge21\" class=\"edge\"><title>layer2_norm&#45;&gt;layer2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1764.4,-79C1772.39,-79 1781.31,-79 1789.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1789.92,-82.5001 1799.92,-79 1789.92,-75.5001 1789.92,-82.5001\"/>\n</g>\n<!-- layer3_conv -->\n<g id=\"node21\" class=\"node\"><title>layer3_conv</title>\n<g id=\"a_node21\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 256, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M1932,-97C1932,-97 1902,-97 1902,-97 1896,-97 1890,-91 1890,-85 1890,-85 1890,-73 1890,-73 1890,-67 1896,-61 1902,-61 1902,-61 1932,-61 1932,-61 1938,-61 1944,-67 1944,-73 1944,-73 1944,-85 1944,-85 1944,-91 1938,-97 1932,-97\"/>\n<text text-anchor=\"middle\" x=\"1917\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer2_act&#45;&gt;layer3_conv -->\n<g id=\"edge22\" class=\"edge\"><title>layer2_act&#45;&gt;layer3_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1854.4,-79C1862.39,-79 1871.31,-79 1879.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1879.92,-82.5001 1889.92,-79 1879.92,-75.5001 1879.92,-82.5001\"/>\n</g>\n<!-- layer3_pool -->\n<g id=\"node22\" class=\"node\"><title>layer3_pool</title>\n<g id=\"a_node22\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M2022,-97C2022,-97 1992,-97 1992,-97 1986,-97 1980,-91 1980,-85 1980,-85 1980,-73 1980,-73 1980,-67 1986,-61 1992,-61 1992,-61 2022,-61 2022,-61 2028,-61 2034,-67 2034,-73 2034,-73 2034,-85 2034,-85 2034,-91 2028,-97 2022,-97\"/>\n<text text-anchor=\"middle\" x=\"2007\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer3_conv&#45;&gt;layer3_pool -->\n<g id=\"edge23\" class=\"edge\"><title>layer3_conv&#45;&gt;layer3_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1944.4,-79C1952.39,-79 1961.31,-79 1969.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1969.92,-82.5001 1979.92,-79 1969.92,-75.5001 1969.92,-82.5001\"/>\n</g>\n<!-- layer3_norm -->\n<g id=\"node23\" class=\"node\"><title>layer3_norm</title>\n<g id=\"a_node23\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 512}\">\n<path fill=\"#b07b87\" stroke=\"black\" d=\"M2112,-97C2112,-97 2082,-97 2082,-97 2076,-97 2070,-91 2070,-85 2070,-85 2070,-73 2070,-73 2070,-67 2076,-61 2082,-61 2082,-61 2112,-61 2112,-61 2118,-61 2124,-67 2124,-73 2124,-73 2124,-85 2124,-85 2124,-91 2118,-97 2112,-97\"/>\n<text text-anchor=\"middle\" x=\"2097\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_pool&#45;&gt;layer3_norm -->\n<g id=\"edge24\" class=\"edge\"><title>layer3_pool&#45;&gt;layer3_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2034.4,-79C2042.39,-79 2051.31,-79 2059.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2059.92,-82.5001 2069.92,-79 2059.92,-75.5001 2059.92,-82.5001\"/>\n</g>\n<!-- layer3_act -->\n<g id=\"node24\" class=\"node\"><title>layer3_act</title>\n<g id=\"a_node24\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#4e90e3\" stroke=\"black\" d=\"M2202,-97C2202,-97 2172,-97 2172,-97 2166,-97 2160,-91 2160,-85 2160,-85 2160,-73 2160,-73 2160,-67 2166,-61 2172,-61 2172,-61 2202,-61 2202,-61 2208,-61 2214,-67 2214,-73 2214,-73 2214,-85 2214,-85 2214,-91 2208,-97 2202,-97\"/>\n<text text-anchor=\"middle\" x=\"2187\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_norm&#45;&gt;layer3_act -->\n<g id=\"edge25\" class=\"edge\"><title>layer3_norm&#45;&gt;layer3_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2124.4,-79C2132.39,-79 2141.31,-79 2149.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2149.92,-82.5001 2159.92,-79 2149.92,-75.5001 2149.92,-82.5001\"/>\n</g>\n<!-- layer3_residual_in -->\n<g id=\"node25\" class=\"node\"><title>layer3_residual_in</title>\n<g id=\"a_node25\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M2292,-97C2292,-97 2262,-97 2262,-97 2256,-97 2250,-91 2250,-85 2250,-85 2250,-73 2250,-73 2250,-67 2256,-61 2262,-61 2262,-61 2292,-61 2292,-61 2298,-61 2304,-67 2304,-73 2304,-73 2304,-85 2304,-85 2304,-91 2298,-97 2292,-97\"/>\n<text text-anchor=\"middle\" x=\"2277\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">in</text>\n</a>\n</g>\n</g>\n<!-- layer3_act&#45;&gt;layer3_residual_in -->\n<g id=\"edge26\" class=\"edge\"><title>layer3_act&#45;&gt;layer3_residual_in</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2214.4,-79C2222.39,-79 2231.31,-79 2239.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2239.92,-82.5001 2249.92,-79 2239.92,-75.5001 2239.92,-82.5001\"/>\n</g>\n<!-- layer3_residual_res1_conv -->\n<g id=\"node26\" class=\"node\"><title>layer3_residual_res1_conv</title>\n<g id=\"a_node26\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 512, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M2382,-68C2382,-68 2352,-68 2352,-68 2346,-68 2340,-62 2340,-56 2340,-56 2340,-44 2340,-44 2340,-38 2346,-32 2352,-32 2352,-32 2382,-32 2382,-32 2388,-32 2394,-38 2394,-44 2394,-44 2394,-56 2394,-56 2394,-62 2388,-68 2382,-68\"/>\n<text text-anchor=\"middle\" x=\"2367\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_in&#45;&gt;layer3_residual_res1_conv -->\n<g id=\"edge27\" class=\"edge\"><title>layer3_residual_in&#45;&gt;layer3_residual_res1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2304.4,-70.299C2312.57,-67.608 2321.7,-64.5979 2330.38,-61.7368\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2331.52,-65.049 2339.92,-58.5949 2329.33,-58.4007 2331.52,-65.049\"/>\n</g>\n<!-- layer3_residual_add -->\n<g id=\"node33\" class=\"node\"><title>layer3_residual_add</title>\n<g id=\"a_node33\"><a xlink:title=\"&lt;class &#39;__main__.Add&#39;&gt; {}\">\n<path fill=\"#fdb462\" stroke=\"black\" d=\"M3012,-93C3012,-93 2982,-93 2982,-93 2976,-93 2970,-87 2970,-81 2970,-81 2970,-69 2970,-69 2970,-63 2976,-57 2982,-57 2982,-57 3012,-57 3012,-57 3018,-57 3024,-63 3024,-69 3024,-69 3024,-81 3024,-81 3024,-87 3018,-93 3012,-93\"/>\n<text text-anchor=\"middle\" x=\"2997\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">add</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_in&#45;&gt;layer3_residual_add -->\n<g id=\"edge34\" class=\"edge\"><title>layer3_residual_in&#45;&gt;layer3_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2304.22,-93.0359C2312.83,-96.9904 2322.61,-100.808 2332,-103 2385.71,-115.535 2400.84,-108 2456,-108 2456,-108 2456,-108 2818,-108 2867.77,-108 2924.08,-95.3711 2959.8,-85.7384\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2961.04,-89.0265 2969.74,-82.9884 2959.17,-82.2796 2961.04,-89.0265\"/>\n</g>\n<!-- layer3_residual_res1_norm -->\n<g id=\"node27\" class=\"node\"><title>layer3_residual_res1_norm</title>\n<g id=\"a_node27\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 512}\">\n<path fill=\"#b07b87\" stroke=\"black\" d=\"M2472,-68C2472,-68 2442,-68 2442,-68 2436,-68 2430,-62 2430,-56 2430,-56 2430,-44 2430,-44 2430,-38 2436,-32 2442,-32 2442,-32 2472,-32 2472,-32 2478,-32 2484,-38 2484,-44 2484,-44 2484,-56 2484,-56 2484,-62 2478,-68 2472,-68\"/>\n<text text-anchor=\"middle\" x=\"2457\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_conv&#45;&gt;layer3_residual_res1_norm -->\n<g id=\"edge28\" class=\"edge\"><title>layer3_residual_res1_conv&#45;&gt;layer3_residual_res1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2394.4,-50C2402.39,-50 2411.31,-50 2419.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2419.92,-53.5001 2429.92,-50 2419.92,-46.5001 2419.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res1_act -->\n<g id=\"node28\" class=\"node\"><title>layer3_residual_res1_act</title>\n<g id=\"a_node28\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#4e90e3\" stroke=\"black\" d=\"M2562,-68C2562,-68 2532,-68 2532,-68 2526,-68 2520,-62 2520,-56 2520,-56 2520,-44 2520,-44 2520,-38 2526,-32 2532,-32 2532,-32 2562,-32 2562,-32 2568,-32 2574,-38 2574,-44 2574,-44 2574,-56 2574,-56 2574,-62 2568,-68 2562,-68\"/>\n<text text-anchor=\"middle\" x=\"2547\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_norm&#45;&gt;layer3_residual_res1_act -->\n<g id=\"edge29\" class=\"edge\"><title>layer3_residual_res1_norm&#45;&gt;layer3_residual_res1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2484.4,-50C2492.39,-50 2501.31,-50 2509.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2509.92,-53.5001 2519.92,-50 2509.92,-46.5001 2509.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_conv -->\n<g id=\"node29\" class=\"node\"><title>layer3_residual_res2_conv</title>\n<g id=\"a_node29\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 512, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M2652,-68C2652,-68 2622,-68 2622,-68 2616,-68 2610,-62 2610,-56 2610,-56 2610,-44 2610,-44 2610,-38 2616,-32 2622,-32 2622,-32 2652,-32 2652,-32 2658,-32 2664,-38 2664,-44 2664,-44 2664,-56 2664,-56 2664,-62 2658,-68 2652,-68\"/>\n<text text-anchor=\"middle\" x=\"2637\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_act&#45;&gt;layer3_residual_res2_conv -->\n<g id=\"edge30\" class=\"edge\"><title>layer3_residual_res1_act&#45;&gt;layer3_residual_res2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2574.4,-50C2582.39,-50 2591.31,-50 2599.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2599.92,-53.5001 2609.92,-50 2599.92,-46.5001 2599.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_norm -->\n<g id=\"node30\" class=\"node\"><title>layer3_residual_res2_norm</title>\n<g id=\"a_node30\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 512}\">\n<path fill=\"#b07b87\" stroke=\"black\" d=\"M2742,-68C2742,-68 2712,-68 2712,-68 2706,-68 2700,-62 2700,-56 2700,-56 2700,-44 2700,-44 2700,-38 2706,-32 2712,-32 2712,-32 2742,-32 2742,-32 2748,-32 2754,-38 2754,-44 2754,-44 2754,-56 2754,-56 2754,-62 2748,-68 2742,-68\"/>\n<text text-anchor=\"middle\" x=\"2727\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_conv&#45;&gt;layer3_residual_res2_norm -->\n<g id=\"edge31\" class=\"edge\"><title>layer3_residual_res2_conv&#45;&gt;layer3_residual_res2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2664.4,-50C2672.39,-50 2681.31,-50 2689.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2689.92,-53.5001 2699.92,-50 2689.92,-46.5001 2689.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_act -->\n<g id=\"node31\" class=\"node\"><title>layer3_residual_res2_act</title>\n<g id=\"a_node31\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#4e90e3\" stroke=\"black\" d=\"M2832,-68C2832,-68 2802,-68 2802,-68 2796,-68 2790,-62 2790,-56 2790,-56 2790,-44 2790,-44 2790,-38 2796,-32 2802,-32 2802,-32 2832,-32 2832,-32 2838,-32 2844,-38 2844,-44 2844,-44 2844,-56 2844,-56 2844,-62 2838,-68 2832,-68\"/>\n<text text-anchor=\"middle\" x=\"2817\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_norm&#45;&gt;layer3_residual_res2_act -->\n<g id=\"edge32\" class=\"edge\"><title>layer3_residual_res2_norm&#45;&gt;layer3_residual_res2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2754.4,-50C2762.39,-50 2771.31,-50 2779.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2779.92,-53.5001 2789.92,-50 2779.92,-46.5001 2779.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_out -->\n<g id=\"node32\" class=\"node\"><title>layer3_residual_out</title>\n<g id=\"a_node32\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M2922,-78C2922,-78 2892,-78 2892,-78 2886,-78 2880,-72 2880,-66 2880,-66 2880,-54 2880,-54 2880,-48 2886,-42 2892,-42 2892,-42 2922,-42 2922,-42 2928,-42 2934,-48 2934,-54 2934,-54 2934,-66 2934,-66 2934,-72 2928,-78 2922,-78\"/>\n<text text-anchor=\"middle\" x=\"2907\" y=\"-56.3\" font-family=\"Times,serif\" font-size=\"14.00\">out</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_act&#45;&gt;layer3_residual_out -->\n<g id=\"edge33\" class=\"edge\"><title>layer3_residual_res2_act&#45;&gt;layer3_residual_out</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2844.4,-53.0003C2852.39,-53.9083 2861.31,-54.9217 2869.82,-55.8891\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2869.59,-59.3847 2879.92,-57.0362 2870.38,-52.4294 2869.59,-59.3847\"/>\n</g>\n<!-- layer3_residual_out&#45;&gt;layer3_residual_add -->\n<g id=\"edge35\" class=\"edge\"><title>layer3_residual_out&#45;&gt;layer3_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2934.4,-64.5005C2942.39,-65.8625 2951.31,-67.3825 2959.82,-68.8336\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2959.47,-72.3242 2969.92,-70.5544 2960.65,-65.4237 2959.47,-72.3242\"/>\n</g>\n<!-- pool -->\n<g id=\"node34\" class=\"node\"><title>pool</title>\n<g id=\"a_node34\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 4}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M3102,-93C3102,-93 3072,-93 3072,-93 3066,-93 3060,-87 3060,-81 3060,-81 3060,-69 3060,-69 3060,-63 3066,-57 3072,-57 3072,-57 3102,-57 3102,-57 3108,-57 3114,-63 3114,-69 3114,-69 3114,-81 3114,-81 3114,-87 3108,-93 3102,-93\"/>\n<text text-anchor=\"middle\" x=\"3087\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_add&#45;&gt;pool -->\n<g id=\"edge36\" class=\"edge\"><title>layer3_residual_add&#45;&gt;pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3024.4,-75C3032.39,-75 3041.31,-75 3049.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3049.92,-78.5001 3059.92,-75 3049.92,-71.5001 3049.92,-78.5001\"/>\n</g>\n<!-- classifier_flatten -->\n<g id=\"node35\" class=\"node\"><title>classifier_flatten</title>\n<g id=\"a_node35\"><a xlink:title=\"&lt;class &#39;__main__.Flatten&#39;&gt; {}\">\n<path fill=\"#b3de69\" stroke=\"black\" d=\"M3192,-93C3192,-93 3162,-93 3162,-93 3156,-93 3150,-87 3150,-81 3150,-81 3150,-69 3150,-69 3150,-63 3156,-57 3162,-57 3162,-57 3192,-57 3192,-57 3198,-57 3204,-63 3204,-69 3204,-69 3204,-81 3204,-81 3204,-87 3198,-93 3192,-93\"/>\n<text text-anchor=\"middle\" x=\"3177\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">flatten</text>\n</a>\n</g>\n</g>\n<!-- pool&#45;&gt;classifier_flatten -->\n<g id=\"edge37\" class=\"edge\"><title>pool&#45;&gt;classifier_flatten</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3114.4,-75C3122.39,-75 3131.31,-75 3139.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3139.92,-78.5001 3149.92,-75 3139.92,-71.5001 3139.92,-78.5001\"/>\n</g>\n<!-- classifier_conv -->\n<g id=\"node36\" class=\"node\"><title>classifier_conv</title>\n<g id=\"a_node36\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.linear.Linear&#39;&gt; {&#39;in_features&#39;: 512, &#39;out_features&#39;: 10, &#39;bias&#39;: False}\">\n<path fill=\"#fccde5\" stroke=\"black\" d=\"M3282,-93C3282,-93 3252,-93 3252,-93 3246,-93 3240,-87 3240,-81 3240,-81 3240,-69 3240,-69 3240,-63 3246,-57 3252,-57 3252,-57 3282,-57 3282,-57 3288,-57 3294,-63 3294,-69 3294,-69 3294,-81 3294,-81 3294,-87 3288,-93 3282,-93\"/>\n<text text-anchor=\"middle\" x=\"3267\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- classifier_flatten&#45;&gt;classifier_conv -->\n<g id=\"edge38\" class=\"edge\"><title>classifier_flatten&#45;&gt;classifier_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3204.4,-75C3212.39,-75 3221.31,-75 3229.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3229.92,-78.5001 3239.92,-75 3229.92,-71.5001 3229.92,-78.5001\"/>\n</g>\n<!-- classifier_scale -->\n<g id=\"node37\" class=\"node\"><title>classifier_scale</title>\n<g id=\"a_node37\"><a xlink:title=\"&lt;class &#39;__main__.Mul&#39;&gt; {&#39;weight&#39;: 0.0625}\">\n<path fill=\"#bc80bd\" stroke=\"black\" d=\"M3372,-93C3372,-93 3342,-93 3342,-93 3336,-93 3330,-87 3330,-81 3330,-81 3330,-69 3330,-69 3330,-63 3336,-57 3342,-57 3342,-57 3372,-57 3372,-57 3378,-57 3384,-63 3384,-69 3384,-69 3384,-81 3384,-81 3384,-87 3378,-93 3372,-93\"/>\n<text text-anchor=\"middle\" x=\"3357\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">scale</text>\n</a>\n</g>\n</g>\n<!-- classifier_conv&#45;&gt;classifier_scale -->\n<g id=\"edge39\" class=\"edge\"><title>classifier_conv&#45;&gt;classifier_scale</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3294.4,-75C3302.39,-75 3311.31,-75 3319.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3319.92,-78.5001 3329.92,-75 3319.92,-71.5001 3319.92,-78.5001\"/>\n</g>\n<!-- logits -->\n<g id=\"node38\" class=\"node\"><title>logits</title>\n<g id=\"a_node38\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M3462,-93C3462,-93 3432,-93 3432,-93 3426,-93 3420,-87 3420,-81 3420,-81 3420,-69 3420,-69 3420,-63 3426,-57 3432,-57 3432,-57 3462,-57 3462,-57 3468,-57 3474,-63 3474,-69 3474,-69 3474,-81 3474,-81 3474,-87 3468,-93 3462,-93\"/>\n<text text-anchor=\"middle\" x=\"3447\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">logits</text>\n</a>\n</g>\n</g>\n<!-- classifier_scale&#45;&gt;logits -->\n<g id=\"edge40\" class=\"edge\"><title>classifier_scale&#45;&gt;logits</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3384.4,-75C3392.39,-75 3401.31,-75 3409.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3409.92,-78.5001 3419.92,-75 3409.92,-71.5001 3409.92,-78.5001\"/>\n</g>\n<!-- input -->\n<g id=\"node39\" class=\"node\"><title>input</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M42,-92C42,-92 12,-92 12,-92 6,-92 0,-86 0,-80 0,-80 0,-68 0,-68 0,-62 6,-56 12,-56 12,-56 42,-56 42,-56 48,-56 54,-62 54,-68 54,-68 54,-80 54,-80 54,-86 48,-92 42,-92\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">input</text>\n</g>\n<!-- input&#45;&gt;prep_conv -->\n<g id=\"edge1\" class=\"edge\"><title>input&#45;&gt;prep_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.4029,-74C62.3932,-74 71.3106,-74 79.8241,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"79.919,-77.5001 89.919,-74 79.919,-70.5001 79.919,-77.5001\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<__main__.DotGraph at 0x7f4140a82128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "frozen_bn_scale_net = network(conv_pool_block=conv_pool_block_pre, scale=1/16, types={\n",
        "    nn.ReLU: partial(nn.CELU, 0.3),\n",
        "    BatchNorm: partial(GhostBatchNorm, num_splits=16, weight=False)\n",
        "})\n",
        "show(frozen_bn_scale_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe5ByKdin8ZX"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size = 18, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.6, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "opt_params_bias = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.6*16, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size/16), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(frozen_bn_scale_net, label_smoothing_loss(0.2))\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(is_bias[False], opt_params), SGD(is_bias[True], opt_params_bias)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F29j8KyvenL_"
      },
      "source": [
        "Test accuracy improves to 94.2%. Interestingly, had we not increased the learning rate of the batch norm biases, we would have achieved a substantially lower accuracy as the reader can verify. This suggests that the learnable biases are indeed doing something useful - either learning appropriate levels of sparsity or perhaps just adding regularisation noise. Indeed we can improve things slightly by increasing the learning rate of the biases even further:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USqjaYDnpogJ"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size = 18, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.6, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "opt_params_bias = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.6*64, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(frozen_bn_scale_net, label_smoothing_loss(0.2))\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(is_bias[False], opt_params), SGD(is_bias[True], opt_params_bias)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpwXfP3sRIgU"
      },
      "source": [
        "Finally we can use the increased accuracy to reduce training to 17 epochs. The new test accuracy is 94.1% and most importantly we've overtaken the 8 GPUs of BaiduNet9P with a time of 43s, placing us second on the leaderboard!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDGYbRmvQVts"
      },
      "source": [
        "### Input patch whitening (36s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmfdPrAIVhUE"
      },
      "source": [
        "Batch norm does a good job at controlling distributions of individual channels but doesn't tackle covariance between channels and pixels. Controlling the covariance at internal layers, using a 'whitening' version of batch norm, might be helpful but would entail extra computation as well as non-trivial implementation effort. We are going to focus on the easier problem at the input layer.\n",
        "\n",
        "The classic way to remove input correlations is to perform global PCA (or ZCA) whitening. We propose a patch-based approach which is agnostic to the total image size and more in keeping with the structure of a conv net. We are going to apply PCA whitening to 3Ã—3 patches of inputs as an initial 3Ã—3 convolution with fixed (non-learnable) weights. We will follow this with a learnable 1Ã—1 convolution. The 27 input channels to this layer are a transformed version of the original 3Ã—3Ã—3 input patches whose covariance matrix is approximately the identity, which should make optimisation easier.\n",
        "\n",
        "First let's plot the leading eigenvectors of the covariance matrix of 3Ã—3 patches of the input data. The numbers in brackets are the square root of the corresponding eigenvalues to show the relative scales of variation along these directions and we plot the eigenvector with both signs to illustrate the direction of variation. As we might expect, variations in local brightness dominate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "zY1hEAoONY0P",
        "outputId": "7f148b17-14fe-4a7a-a00a-bfd5052c44cc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABgYAAACZCAYAAADkSYGdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xm4FMW9//FPcYADalwwLogL7rtRkyCiiBHFPVFBjZgILlGjxtyYiN6rEoMmKmiSm0TjchWVaDDu0bgQ3EBx119wI7iAW8BEUaOIgFC/P6qOt2mmp7urzszx0u/X85wHZrq+Xf2dru6u7uqeMdZaAQAAAAAAAACAaujU0QsAAAAAAAAAAACah4EBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAq5AsxMGCMWc0Y83djTLcm17uNMWZKE+rpqPzWMMa8ZIxpbUJdy/o63MIY81Sj66lR7zeNMeObUM+ynl9Htc9lPb9mbX+txpgXjTFrNrquVL1NWX++rmV6Hdao9zxjzH90QL23GGP2akI9xxljft3oemrU+0tjzPFNqIf8GlNvU9qnr2tZ3waX9fyW6TZagfwGGWNua3Q9Neo92RhzfhPqWabXn6+r6fsY3x+eZoxZvQl1ddQ6fMIYs2UT6iG/xtTbrPw66hhPfu1TzzJ9jPg/mZ+1tt3/JJ0k6SlJ8yVdXaD8RZJOr/F+D0n/kvRwndjhkhZJ+jjxt2ti+raSJkv6UNJbkkam4u+StH+J3FolXSnpdUkfSXpW0t5l8pN0iKQpkj6R9GBO7DckPSfpA0nvSbpVUq/E9NGS3pT0b79MZ6TiL5H0g4B1+AdJs/x8p0s6poE57ivpYZ/jbElXSPpS6jO/yi/LbEmnxKzDVOzGkj6V9IeccjdL+nZEG/+RX/YPfS6tqek/lDRD0lxJL0naJDHteUnblMzrQZ9X2zbx95L59fBtba5vV0Nz4reXNMnX9Y6kH/r3V5f0R0n/8Lk/ImmHVGzp/Hzct/1nNVfSq5L6l2ifddtUKna4MvYxjcovVdfHvv7fNiK/1Hzul2QldU681677UB/T28e975fvd8k6a5T/gaRLE6+NpAvk9onvye0HTYF6x/r8Nqoxrea+IKJ9bu4/zw8lvSLpwJzyMW10mKSnfdm3/OfROVUmc3spsg6Vs8+TNFDSNLl9/gOS1qszr9UkvS2pe9n4gtvcapKulzumvC/pusS0PpKeLpOfpK6SbpI007efXXM+q65yx+XksXpbv44+8f9umxGb28eQtJzcsf1d/xlMSkzr6evuWiK/vpL+KmmOXJ/rRkk9OyI/SVv45Xzf/02UtEVkfnXn2Z75FdlWEuV+6tvT7nntM3a+7bkN+rLnyPVNP5N0dmpaXr81ZBvs7XNKHhfP6qD88vqspfPL266b3UbrfVaB6+/w1Lr7xK/PrzYov8w+rFz/4QxJb8gdM8dLWrEd1t8hcsfYjyS9KOmArOXz5Z+S1DfVxh/w+U1T/e0377woc1kkdZPrJ6xeMr9j5PoyH0u6R9JaDWyfdc9BFHaML3JsbUo/Jm99K//axghJF5XJT03sxxTYBneVtDiV37BU+725ZH5N68cUaaOJckuddwTm1+x+TFPzS5VtRh+mt7K3v29L+rvccfifkq7RkseIkPXXW83tw9TLb5jqnDOGrj81tw9Tb/9Suo9WYP19kfowPSX9We482ErqnYrNPI/I+ysdUGim0kGSDpD0e+VcNPUr4l1Ja9eYdoXcxca8gYF601+U9HNJLZI2lLvY/c3Uir6zRG7LSzrbb3CdJO3nG1DvjPJL5Sdpd7/RjVT+RfM15Dtffl6jJf05MX1TScv7//eS9IKkgxLTd5L0fMA63FL+4rWkzfyGldX4Y3McKmkvuR3KKpLu1pIXAc+TuzC5itzFttmS9gpdh6m6J/h5Zw4M+A1wjqRugW18T7mL5Vv6HB6UdH5i+jGSpsod9I1vpz0S08+Q9LuSeT2onMGcnPz+KOkGSStI2lluB79lRvyX5Q6ch/u28CVJm/tpG0g6xdfRIulY31ZWiMxvD7kdZV+57bCXEjvfAu2zbptKxQ9Xxj6mUfml6lhe7iC0SyPyS21Hk7T0wEC77kN9zF2SrpY7QV1T7gLMyXXKPy9pp8Tr4+Q6bWv7df+ipONz6tw5kV+tgYGa+4LA9tlZbkD1FP+57SZ3cN8ko3xsG/2+pP5yHZFech2O5CBD3e2lyDpUnX2e3D7gQ0kH+3U6RtJjdeZ1qqQrQuJVbJubLOmXklaS1EXSdql5vCzpayXy6yrpP3wbmqX8E+qDJf01Ff+63ABxq6ST/euuNWJz+xhyA/fj5U4cWpQ6NsudHA8pkd/efplXlDsOXyXpno7IT9LKfprxuZ0saWpkfrnzbK/88pYlUWZDuf3eP5Q6+a3VPttjvu21Dfryw3y7uV1LXziv228N3AZ7K3VsytkGG5lf3T5rSH5FtutmtdEin1VIfqmyw+UGqGsO6LdDfpl9WL9up0lax0+/XdI1ke2zl6QFvs0YuQsTnyh18T1R/uuSXk6996jccau7pMFyFzVWy4jP7B8UWRa58+uflMhvgFw/f0u/Ln4v6aEGts+65yAKO8bnHXua1o/JW9/Kv7axtly/p7VEfk3rxxTYBneV9FadurvJnZf2LJFf0/oxRdqoL1PzvCMwv2b3Y5qaX6Jcs/ow9ba/dSR92f9/BUnXSfpN5Prrreb2Yerll3fOGLT+1Nw+TL39S+k+WpH8UvHD1XF9mDUknSBpR9UYGKiVX9G/UoVLz1w6V/mdw10kvVLj/R19oz5ScQMDn2jJu81ulPSfide9JM1T6g7uknlOlTS4TH5+2jHKuWieKt8q1xl8MWN6L7md6YjEe539Z7BeRH6bynUiDml0jj7mIEnPJV6/LWlQ4vU5ksbHrkO5EeE/+R1BvYGBIyRNzJhWpI1fL+kXidcDJc32/+8kN6I4sE78TpJmlMztQRUfGFgiP7md4wIt+dTCOCUGM1Lxv5A0rsSy/VuJg0VgflMkHV2w7FLtM69NpcrW3cc0Ir/U/IZJek3ZB5+o/Pz0leQuZPfV0gMD7b4PlbuTbZ/E6zGSLssou66ff3KZpkg6NvH6aNXvIHWWG/3fRjUGBurtCwLb51Zygzkm8d4ESec0ah2mYk+RdEfq88rcXsqsQ9XY58ldnJ+SeL28n99mGfO4X9J3QuNrzO/zbU7SILk74lrqlL9C0k+L5pea/pbyT6ivknRm4vUgvz6T7eEN5QzWJcp+3seQOx7/W4m7l2qUP0PS2JD8fJntJX3UEfml3u8s6URJn7RXflnzbER+9ZZF7kRlH99W0wMDme0zZr6pcu2yDcqdCJ5dZ3rNfmvZbVDlT6qbkp8vs0SfNTC/3O26WW20yGdVNr8aZR7IaePB+SmnDyt31/SpiWn95J4WXC5i/e0g6Z+p9/4laceMeYyU9D+J15vI3amfvKtxsjJueFCd/kGRZZG7GeCBEvldKOnixOu1/Pa4YQesv6hjfKpc8tjatH5M3vpWgfMOuQs/A4rml3q/of2YAutwV9UZGPBl/qrEUwRl8vPTGtaPycvPv8477wjOTw3ux3RkfmpCHyZv+0uVXUHStZLuislPTezDlMnPT1vinDEwv6b1YYq0z1T5Un20eu0zMa3D+jCJ9zore2Cg0DEw/fdF+I2BreXu/PycMaZF0sVyjzXaAvPYzhjzrjFmujHmLGNM58S0X0s6whjTxRizqdyAw8S2idbatyUtlGvQpRlj1pDbAF/IKLJUfgF1rGuM+UBuB/ATubuvktNPN8Z8LHegX17uQrQkyVr7mdyjn18JqPcSY0zbI0iz5O7yrSU6x5Rd5D9PY8wqch3QvyWm/03urhVJYevQGLOipFGSflygeGx+W2rp5V/DGLOq3F0fa0vayhjzpjFmhjHmZ8aY5Lb5kqTefpnLOM9vF48YY3atUy6d3yaSFllrp6eWOev75vpKmmOMmWKM+acx5g5jzLq1ChpjtpUbNX0l8Xap/Pz+4WuSVjPGvGKMecsY8ztjTPeMkCXyK9Kmaqi3j0kuW3R+NQyTdK31e/oa2iO/X8jdATa7xrRG7EP/W9K3jTHLGWN6yd3pc09G2a0lveb3ZW1qbVP18vuR3CONU9MTCuwLQtafyXhvq4zy7bEOk5L70NztJfY4qNT6sNa2fV1R1vKm9zll4z9XY5vr6+d9jTHmPWPMk8aYAamwlxRwTCyhVn5TU9vwVBXLL93H2EHuLpOf+X3Sc8aYwamw2Pw+bz8ZGplf2/sfyF2s+63c/ikpKL+ceSa1W34Zy3GwpAXW2qw+VWh+efNNardtMGNZ6vZbFd5GX/f7sLHGmC/XKdfQ/FJqbS9l8yuyXSc1so0W+ayC9zHGmPXkPrNr6xSLyS+vD2u05DHayA1gbZx4r2x+T0l6ybjfJWoxxhwgd2FmqT6HVyu/16y1H2Us8/8ubH7/oMiylM2v1mcmFezTqH3XX7sc42sce5rZjymyvvPOO+rmWOD6RJ5GrkNJWt0Y844/7/2VMWb51Dxi82tkP6ZIfpnnHV5Qfk3qx3RIfk3sw+Ruf8aYnY0xH8rdNT5Y7lw4KbR9NqMPU/h44pXuwwSemyQ1un0mtUd+yWkd3YcpIqiP9kUYGFhZbqNLOlnS49bapwvET5LrmKwut+EeJvf4TZs7JQ2ROzmZJulKa+2TqXl85JejFGNMF7nHi66x1k7LKFYrv1KstW9Ya1eWe4zoTLk8ktPPl/v6lu3lRpQ+TM0iKD9r7Ql+vv0l3SLXsawlOsc2xpg95C6EjvRvreD/Teb0oV+upLI5niPXFt4sUDY2vxW09PJLLoe1/f8Hye1EviHXho9OlG+ru0x+p8l95UYvSZdLusMYs2FG2XR+6eVtW+b0Z95mbbl19kO5u7tnyD0CtQR/YXWcpJ9Za5PzL5vfGnKPDg+Ra5vbStpObtuopVZ+Un6bapO3j5HUrvkl57mu3CPc19QpFpWfMeZrcnfF/zZj/o3Yhz4kd4Br+37DpyRl/Qhfre2v1ja1gjFmqQvyxph15L56aGR6mpe3LwhZf9PkHrs/1Q+oDJJbj8tllI9to58zxhwpNxBwoX+r6PYSdJxILG+ZfUbsPkdS5ja3ttz+9AG5r6m6SNLtqQ54TK5FtFd+tfoYa8vtjz6Uuzh0ktwFks0TocH5GWO2kdtWltrHJTQyP0mS7/OsJJffs6nQ0D5NvXkmtUt+tRhjVpA7ma/3g3Kl8ys436SG5Sjl91tVPsd35b5+ZT1JX/XLdV2d8g3Nr02NPmubsvkV2a6TGplfkXnF7EOPkDTZWjujTpmY/PLK3i3pGGNMb2PMSnL9ZWnJ43Op/Ky1i+QuElwvd650vaTj/MWbWmLza5u+VNmCy/KR3L6wqLskHWKM2cbfVDBS7sa9Mn2a9lp/0cf4jGNPM/sxeWWLnHdk5ljw+kSeRuY3Ta4v2lPuqza/Kve1J0nB+TWhH1O3bIHzDikwvyb1Y5qeX5P7MLllrbUPW2tXktvfjJF7giGp7PprZh+mcNka54xtyubXzD5MmfxK99EK7D87ug9TRFAf7YswMPC+EokaY9aSGxg4o0iwtfY1a+0Ma+1ia+1zcnd+DvHz6iF3F+ooue/LWkfSnsaYE1Kz+ZLcd28VZtwd3ePkHvU4qU7RJfKLYa2dI3eB8Pb0nQPWeVbu4t3PUqGl80vMd5G19mG5Df77GcXaJUdjTF+5DuyQxCjZx/7f5N26K2rpC4WFc/R3mO4u6VcFFy02v4+19PJLLod5/v+jrbUfWGtnSrpM7jG6Nm11F16H1trHrbUfWWvnW2uvkfuBzn0yiqfzSy9v2zJnDY7Mk3SrtfZJa+2ncu2vnz/hkiT5E4k75L7u5bxUfNn82j6z31prZ1lr35XrUJbJT8pvU5Lq72PatHN+SUfIPU5c7+ATnJ/fj10i92PRn9WY3u77UF/nvXKDjcvLXThaRe7HhGuptf3V2qY+zniq4teSRqUGa9qWpci+IGT7Wyj3Xb37yj2F8WO5ryp6KyMkqo228XcGni/3g0nv+reLbi/BxwmV32fE7nPqbXPzJM201l5prV1orR0v93VtOyXKxORaRHvkl9XHmCf3dMe51toF1tqH5C6QDEqUCcrPGLOR3EWzH1prJ9cp2sj8PucvZF0q6VpjzOqJSTF9mqx5JkXnV8fP5L56r94+PSS/IvNNamSOn6vTby2Vo7X2Y2vtU9baz6y178i1mUEm+0muhueX0WdtU3YdFtmukxqZX5F5xexDj1D9mx2kuPzyyl4ld/PKg3J3Az7g308en0vlZ4zZXe6pmF3lnmAbIOl/fB+jltj82qYvVbbgsnxJS194yGStvU/uB0FvlrsrdKavr0yfpr3WX9Qxvs6xp5n9mLpli5x3KCPHEtcn8jQyv9nW2hd9fjPkfky5XfJrUj8mr2zmeUdC8PprQj+mI/JrZh+mcFnrnqi+R+6785NK5dfkPkyhshnnjG3Krr9m9mGK5le6j1Zw/9nRfZgigvpoX4SBgalyj0y06SM3gvyiMWa23FdO9DHGzDbuKxHyWP3vI44byD2Kca3fEN+S27A/vyDiByK6qsRXxfi7Uq+UuxNzsL8IlCWdX6zOcncQZO1IOsv9cIskyZ+IbaQlHzkNrTfrjvPoHI0x28n9wvZRvgMqSbLWvi/3NUbJx2G+oiUfPSu7DneV+663N3wb+4mkwcaYZzLKx+b3gpZe/neste/JLfMC1f/KrM3lOsH/jliG5HaRls5vuqTOxpjkY9VLfOY14pPL3/Z/I0nGmFa5u8HflrvDIK1Ufr5NvKX6n1l6+T7Pr0ibylsEJT7L9s4vpcjBJya/FeXuFLjBbwttTwK8ZYzpr8bsQ3vIDTD8zg9cvSdprLIHdqZK2iB1UanWNpW1/gZKGuOPIW1flfSoMWaoiu0LgtaftXaqtXaAtXZVa+2ecp/lExnFo9uoMWYvue8U3N+fSCbnVXd7CTkOpiyxPox7JHzDOsub3ueUis/Z5tL7o1o2V/wxsZ5a+W2TeqJlG2XnV6+PkfXYdlLp/PyjsRPlfgdjXE7xRuaX1knurtReifdi11+teSZF5ZdjoKSTE/ujdST9yRhzWqJMSH5F5psUtQ2WVKvfGrsOl+hn1NDQ/LL6rAll8yuyXafLN6qNFvmsgtafMWYnubsJb8opGpNf3T6svxj5U2ttb2vt2v79t/1fm7L5bSv3tRpP+fk/KelxuRsPaqmV3wbGmOSFhJrH/AL9gyLLUnr9WWsvttZubK1dXW6AoLOk5zOKN2z9KeIYn3PsaWY/pvD69mqdwy2VY8lja55GrsO0dsmvif2YvPzqnXcE55fSyH5MR+TXzD5M2e2v1jWw2PXXyD5Mbn5Z54wJZfNrZh8md/8S0kcrsv6+CH2YgsL62LbkjxIU+ZPbgLrJ/eDYOP//mj+2IXcx4l+SevnXrXKPBrb9/VCuQ7NmRvzektbw/99MrpPyU/96RbnRkqFyO9A15X7Q+OeJ+KFK/aBIgfwulfSYpBUKlF0iP/9ei/9Mjpd7XLCbpC4Z8QfJfe9zJ7lf+f6TpGf8tE5yF0VWkdux9JHrLJ6ciO+njB8rrrPMq8v9GOcKfln3lDRX0rcalONWkt6RdGjG9PPlvnpkFb+OZynxYx1l16HcgTTZxi6U28BXyyi/hqT3JHULbON7yd01vIXP4X4t+QM+18p9XUvbVwtNU+KHQiX9l6RLSuS3sl9n3fxyHu7X36Yl8hsvd0fV8nJ34Xwo/2voNeJ3kxsZ3VbuK0t+JfeIlfzrO+Qu4mV9PqXy8zGj5C5ir+4/08nK/mHXWu2zbptKxdfbxzQkv8S2O1eJHw9q7/zk9hvJbeHrcp2VXn6+jdqHvibpdN8+V5Z0q6Tr6pSfKqlf4vXxct+f10vuAP2Csn9UafVUjlbuO2q7q8C+IGL9bSO3DS4nN+AwQxk/7tsObXQ3uW14l5Dtpcg6VJ19ntyx6UO5R967yT39Ue/HoE+RdHnideF45WxzcgNP78s9OtoidxfaHElfTpSZLqlP0fz89Fb/3ltyd8B0kzJ/EPwgSRNS6/d1uf5Mq9xdKK9L6poRn9nH8Pm/Iuksv8w7yd1Fkvxh0AmSDimx/nrJfVfpqbWWp8n57SH3VVctcvuf30j6h5Y8PpXNL3ee7ZxfvWVZVUvuc96UdHDys6jVPttjvu21DSbaYTe5u7HO9f9vSXx+Nfutodug3PfXts1zVUk3KOPHU5uQX90+a2B+udt1E9to7mdVNr9EmcvlfjOp0fuYzD6s3DFiQ7n+zxZyfbpjI9ffALmvitjWv95O7pg8KGP5tpc0PfXeY3J9kG6SDpTre2Wdl2T2D4osi18PI0rk102u3Ru5rwx9UNIvOnD9lT7G+/frHXua1o/JW9+qc97h3+vl12lr0fz89Gb2Y+qtw119OzJyF4AfkDQ2tZxzJK1VYv01ux9TL7/M846I/Jrdj2l2fs3uw9Tb/g5PtM/15Pa1t0S2z2b3Yerll3fOGJJfs/sw9dpnaB8t9/quvgB9GD+9m59mfbvqlpdfkb9ShQvPVDrbL2jy7+w65cdIOi1j2nC5r9Joe72u3CMW6/rXF/qVP1fuYtMoJS5A+8b/pP9AZ8uNji2XmP4XSd8skdt6Pp9P/XK0/R1eND+fU/rzuTox/WNJ/f3/fyB3QWmuX/7xktbz0zrJPd40x8dMl7uIlfzF64uVGCgomONqcjvBD+S+A/w5Sd/LiYnJcaykxanP84VE2Va5R3//7df1Kam6S63DjPb6h5wyNyqxg6nXxtNt1L93il/2f/t8WxPTVvTr9SO5A+HI1Dp8TtJXSq6/J/38PpDbye1RMr8echfe5sr9avrQxLT+cl/bkoz/vtzdVu/LXbRbx78/wH82n6TWb//Q/HxMF7mvwPnAbxe/UUbnKKN9Zrap9PpTnX1Mo/LzcZfJPVZZpGxwfqn59Pb5JC+Itus+1MdsK3di+b7cCeyNklavU/5ESb9PvDZyj8rP8X+jU9vMEusgNS8raaOMaWcrtS+IWH9jfH4fyz3WXLPOdmqjD0j6LNUG7y66vRRZh8o5rsvdjThN7nHSByX1rjOvL8udmHYvEi/XWbu0xDbX36+3j+V+vyI57euSng3Ib2aN6TVz9J/3G0p0quVO6J72+T0jabvEtP9qW18q0MeQ+32OR+X2SS9KOjAxraf/bLsWzU/uKyJsqr6Pa+XW6PzkTgSn+ff+Jffd1ttE5ld3nu2ZX5G2lKprpqTd89pn7Hzbcxv0r6+usSzD/bTMfmvoNij3Hdtt85wld0NFzRuGmpDfWNXvs4buYzK362a30ZzPKjS/bnLHoIFZebVjfvX6sJvIPR33idyJefqcIjS/k+QujHwk11f8cU6OT0raIfG6t/+s5/nlS+4XDle586LMZdH/Xhheo8T2t7LcDRpt2/R58gNlzV5/fnrIMb7IsbUp/ZgC6zvv2sapkn4ZkN/MGuu4Zo6NXIdy58Rvy22Db8r9xtmXEtMPVuJCbJH81MR+TJE2mqrLKnEOEJhfs/sxTc2vxjxnqrF9mN7K3v5+7uc91/97uaRVI9dfs/sw9fJ7QPXPGYPWn5rbh6m3fxmrkn20gvl9IfowiW1uib96+RX9M34GHcoYs5rcHYzbWWvn5ZVvx3q3lhuN27HB9XRUfqvLXeDfzrrvfm9kXcv6OtxC7itd+tgmbjTGmP0lfddae0iD61nW8+uo9rms59es7a9V7ke2BlprZzWyrlS9TVl/vq5leh3WqPcXkv5prf11k+u9We7Hpu9qcD3HStrCWlv0h9Taq96LJL1qrb2kwfWQX2PqbUr79HUt69vgsp7fMt1GK5DfIEknWGsPaGQ9Ner9gdzNOyMaXM8yvf58XU3fx/j+8N/k7vb9Z4Pr6qh1+Ljck/NZX1XVXvWQX2PqbVZ+HXWMJ7/2qWeZPkb8X8zvCzEwAAAAAAAAAAAAmuOL8OPDAAAAAAAAAACgSRgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQjo3YqbXXH1p1C8aG2OCY2N/THnY8ONzKx//X9+Iyy8idnFMxZIO+8UDudXf9McrovLr0rlLcOzChQtjqtaQod/Lze+GP/5PVH4vPP9ieOwLL8VUrZtvuzs3v0v3/WVUfnZReLhpiWnd0vF/OSV3BnffeUNUfo8+9kRw7OMRsZJ078TJuflddeWvo/KbPfud4NhevdaKqVrDhv+gUAP40cnHRuW4x6Ddg2MnTrgvpmr98jeX5eZ4+Le/FZXfkUcOC4695ppxMVVr3PW35ua3989vjMpvoW0Jju302byYqjXh7MNz8xt31X9H5Tdzxszg2OnTX46pWuNuuDM3vz+f2j8qvye79gmPbd0hpmrdM/KQ3PyO+953ovLbZUD/4NhJD02OqVqXXfGH3PwuG3B5VH6dPgvv+n/WbUFM1fr+ffl9bEk65o5vRuXYa8om4bGPbhxTtY598LjcHPuee29UfoPm3Rscu9unccfAXS/6W25+Q0bFnUdsP/+Z4NgnIvcxt40cnpvff555elR++879S3DsncvvF1O1zj/3vNz8hj62a1R+G92xfXDs+vd+JaZqHfnUsNz8TvjjIVH5rTN5s+DYGXtMjalalx94W6F96MgzfhyV44x9ng6OXf/u8PUvSaPO/WVujoN/dmVUfqd/MDo4dvTKP4mpWjf+NP9axZBRcdcqNp8/LTh264XPx1StQ0bfk5vf8TcdFJXfTiMPDY599MybY6rWJUP/VGAfc3BUfqtMDz8f7/l4XB/mpLtOys3vyFFjovI7YM7E4NjbeoRfA5CksSNPzc3vgiMuiMrvjQMfCo5d59YBMVXr9GtPC74YxxMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAB7Q0EFAAAPa0lEQVQAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUCAMDAAAAAAAAAABUSOdGzNQY06HxjdbSKW48pVNEeottVNWFPP/8i3EzsE1YyAxDCpRpbW1t+HJkmT9/fsPrMIsjZ7AofP01Y9udMuWxqPiY1vn1Hb4WVXcRDzzwUFT8Cy+8FBy75ZabR9U9bPgPCpV77dUZUfXM+ses4NhXX30tqu4iNt5446j4SZMeDo5dr3fvqLqL2PajR6Liu7SEH0NjYp3Dc0tcdfX1UTX0779TcOx3jzoqqu4iutm445CJ2IsuMF2i6i6if/+do+Ivu+Sy4Nhjjz82qu4i/tFvelS87RTeSbAtzenfLVwuro3G5NiyoCGnRksYNO/eqPgWLQqOvb/7wKi6dy1QZo9PJkbVsee8vwbHrrpoTlTd0vDcEpO7h+/jJWnVxf8Kjn24e7+ouovY6M9xfd1OC1uCY2cMei6q7iLWfmiLqPjNxoevA2sj+zAHFiu2/l3bRVVjO4Xv69f/y1ej6ta5+UW2WvB8VBV3LL9vcOymC6dF1V3EiPcvjIq/c/l9gmPPW/nUqLoPKVBmzSc2iqrj5QMfD47t8XLPqLqL6HtOkStS2V458Ing2Cnn/DGq7pN0Um6Zb865L6qOEeuNCI49//UxUXVL+e37zW9Njqph+xEnBsc+e/7vo+qWTguO5IkBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqhIEBAAAAAAAAAAAqpHMjZrp48eKoeGNMcKy1NqruQjq1NL6OLOEfTWEvPP9CVPyCBQuDY7t06RJVdxHz58+Pm0FE+2xtbY2ruwDTEjneF5Gf6dT4BvrEE09Fxe/Qt09wbN8dw2OLGrj7blHxm2++WXDsGmuuEVV3URtutEFUfM+eawbHbrBhXN1FrNe7d1T8NVePC4498qhhUXUX0d/+v6j4lk/nBccusI0//n73qKOi4l+e/nJw7Nirr4+qe9CeB+WWWWC6RtURcYhQq/00qu4iJk96OCr+uO8fFxw76aHJUXV/54jjc8u83e/vUXUs6r4gOLbTwub0f1sWxJ2e2Jbwc4GFy4V/PkUN/PS+qPj7ug0Mjp3QfVBU3aMKlLl7ub2j6ni3ZdXg2Gdbt4uqu8jWv/O8KVF19P30ieDY9zqtFlW3tH9uifUnbB1Vw4w9pwbHvrr/01F1FzFzz7g+jOkUfp3jrZ2nRdVd1Ix9n4mL3+vZ4NiIj8c7PLfEZgvjPsfRK40Ijj39wwui6i7i/JVPjYrfesHzwbGnvX9hVN1S/vGpx8s9o2qYctbNwbE7n3VoVN1FTDlnfFR8z8c3Do7d4ZyDo+rWbflF7ugR3geRpAteHx0ce3uP3aPqPrBAmXVu7x9VxzOjLwmOXffWXaLq1uDwUJ4YAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQhgYAAAAAAAAAACgQoy1tqOXAQAAAAAAAAAANAlPDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCEMDAAAAAAAAAAAUCGdGzHTww/d38bEWxseboyJqVrX3XBH7gyGnPWHuPwiYuOyk2465zu5szjggH2i8lv42cLg2C5dusRUrdtuvSs3v8EH7B2V35ZbbREeu+XmMVXr0MOOyc3v6JsOjcrPtIS3MrsoqmpdOeSG3Mr3GLhzVCV9+/aJiP16TNXad//DcvM7YujBUfmtueYawbFvv/2PmKp13Q23FGo8Yy4YGZXjhAn3BcfuMWhgTNUacdqo3ByvHntxVH5Xj702OHbYsO/GVK0jjz4pN787TukXlV8Xsyg4dlHn7jFVa98LHszN79uHDYnKb/31ewfHbrLJRjFV68jhx+fmt8+o8VH5fX3B4+Gx85+IqVr7jXkkN7+Lf3NBVH6THpocHLvLgP4xVevEk0/LP8bfE9dHW9Q5fPvr8mnXmKp1xX75fWxJunT330fl+Ha/6eGxO74cU7Wu2vvO3Bwn/3iruGNgtz2DY+/vHncMfOTMfXLzG39qXBt9pnX74Ng+88P3T5I0ZMxfc/M754yTovK7c/n9gmP3n3tnTNU68+e/y83vmq9cF5XfK/s/HRw7c8+pMVVrXP+Jufn95oBfReX3Zv9pwbHr/3WbmKp1wj0nFtqHjjzzlLh+zF1fDY6dsfczMVVr1M8vys3xhhF7ReV3/sojgmNHfHBhTNU6bHT+tYrxp8Zdq3ipdbPg2Oe6bBVTtW756dG5+f1u399G5ffIqBuCY/ueOzimav3w1h/l5vffB/w6Kr/3Nwk/H5+9Q1wf5tLBt+bmd/mpQ6Pyu73H7sGx35ozMaZqHTvm+tz8fnzZd6LyW+fWXYJj3zxwUkzVuui4PwRfyOOJAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKoSBAQAAAAAAAAAAKqRzI2ZqrY2KX2wXB8d2asJYx6LF4csnSYsjPp5OJqrqQrbcaouoeGOasJAR5s+fHzmH8BXY2to1su4CYjeBlojYuE2/kH79+kbFx7TOJx5/Kqruffc/LLfMN74xIKqOLbbcPDj2xRdeiqq7qA023CAqfq21XgyO3TCy7iJeefmVqPhddtk5OHbmzJlRdRfx7Jd2ior/bFH4MXThZ3HH330LlDlq+NCoOh6e9HBw7LVXjY2q+8jhx+eW+dS0RtVhI/aiXe3CqLqLmDw5/POXpONOODY49vJLr4iq+8STT8st03PKplF1dFoc0UlYFNm/269Ysc6fxPWVzOLw5VzUtfFtdEL3PaPiF0d01Habd19U3dI+uSUmLLd7VA0Tuu8RHPteS4+ouocUKNN/3pSoOt7rtFpw7M6RdRfxyjfj+rqLuywKju09YeuoutU/v8hbu4T3ISVp2mHh68CYuD5MUTP2eSYqPmYfOmPfp6PqLuL5rltFxe8/987g2GldNouqu4jRq/wkKn6/uX8Jjj39gzFRdUtH55aY3SfuPGnjW3YIjn1/41lRdRfx2Fk3RcVvfEuf4Nh+Z+Vfa6hrcH6RO1YZGFXF6NcvCI49bb0RUXUX6eGvc3uBA0kdz46+ODh2u9NPiKpbx4WH8sQAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAVwsAAAAAAAAAAAAAV0rkRMzWd4sYbOtmIuk0TxjoWL2p8HVkiPpuittpqi6j4rl27BscuXLgwqu4iuraGL5+kqHUwf/6CuLoLsIsWx8VbEx68uPENtE+fr0XFP/bYE8Gxjz8aHlvUxIn3R8W/9NK04NjZs9+JqvvI751UqNyrr7wWVc+sWbODY197Na7uIl6fOTMqftjw7wbHjr3qmqi6i3jYbBsVv6hb9+DYLqbxx99xV10VFb/JJhsFxx41fGhU3UV0tXHHoZhjxKemW1TdRfTfZaeo+Et/f1lw7C4D+kfVXcTaUzaNim+ZF94HWtylOf3fxV3j6jGLws8FunwS2Ucs4P5uA6Pid/v0vuDYPedNiKq7iH0+uTsq/suL3guO3X7+M1F1S6fklni4e7+oGh7rtkNw7KqL342qe9cCZWYOei6qjvXv3SY4dsM7vhpVt87JL9J7Qlwfxtrw/UuvhzeLqrtA85Qkrf+XuM+x9z3bBccujrmQI0kFNq9pXeI+xxEfjg6OPX+l06LqLuL0D8KXT5Ke67p1cOwFq/wkqu6bC5SZs/GsqDp2PGdwcOwj59wQVXcRO511aFT8rB1eCY59/Kwbo+o+Qofkltn//fA+iCSNWC98G/rWnIlRdUtH55Z481uTo2rYfsSJwbFvHDgpqm7puOBInhgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCGBgAAAAAAAAAAKBCjLW2o5cBAAAAAAAAAAA0CU8MAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIQwMAAAAAAAAAABQIf8frpnNXocsUqEAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1944x144 with 54 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def cov(X):\n",
        "    X = X/np.sqrt(X.size(0) - 1)\n",
        "    return X.t() @ X\n",
        "\n",
        "def patches(data, patch_size=(3, 3), dtype=torch.float32):\n",
        "    h, w = patch_size\n",
        "    c = data.size(1)\n",
        "    return data.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1, c, h, w).to(dtype)\n",
        "\n",
        "def eigens(patches):\n",
        "    n,c,h,w = patches.shape\n",
        "    Î£ = cov(patches.reshape(n, c*h*w))\n",
        "    Î›, V = torch.symeig(Î£, eigenvectors=True)\n",
        "    return Î›.flip(0), V.t().reshape(c*h*w, c, h, w).flip(0)\n",
        "\n",
        "Î›, V = eigens(patches(train_set['data'][:10000,:,4:-4,4:-4])) #center crop to remove padding\n",
        "\n",
        "layout([\n",
        "    [partial(image_plot, img=V[i].to(torch.float16)*3, title=f'{i+1} ({torch.sqrt(Î›[i]):.2f})') for i in range(len(V))],\n",
        "    [partial(image_plot, img=-V[i].to(torch.float16)*3, title='') for i in range(len(V))],\n",
        "], col_width=1.0, row_height=1.0\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6qlex4zYaGm"
      },
      "source": [
        "Now let's replace the first 3Ã—3 convolution of the network with a fixed 3Ã—3 whitening convolution to equalise the scales of the eigenpatches above, followed by a learnable 1Ã—1 convolution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "ckzKy37AY1gq",
        "outputId": "b63310fe-2ae7-4fce-883e-c99f2ac3204c"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"1080pt\" height=\"58pt\"\n viewBox=\"0.00 0.00 1080.00 57.75\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.302352 0.302352) rotate(0) translate(4 187)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-187 3568,-187 3568,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\"><title>cluster_prep</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M94,-48C94,-48 410,-48 410,-48 416,-48 422,-54 422,-60 422,-60 422,-111 422,-111 422,-117 416,-123 410,-123 410,-123 94,-123 94,-123 88,-123 82,-117 82,-111 82,-111 82,-60 82,-60 82,-54 88,-48 94,-48\"/>\n<text text-anchor=\"middle\" x=\"252\" y=\"-107.8\" font-family=\"Times,serif\" font-size=\"14.00\">prep</text>\n</g>\n<g id=\"clust2\" class=\"cluster\"><title>cluster_layer1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M454,-12C454,-12 1588,-12 1588,-12 1594,-12 1600,-18 1600,-24 1600,-24 1600,-163 1600,-163 1600,-169 1594,-175 1588,-175 1588,-175 454,-175 454,-175 448,-175 442,-169 442,-163 442,-163 442,-24 442,-24 442,-18 448,-12 454,-12\"/>\n<text text-anchor=\"middle\" x=\"1021\" y=\"-159.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer1</text>\n</g>\n<g id=\"clust3\" class=\"cluster\"><title>cluster_layer1_residual</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M814,-20C814,-20 1580,-20 1580,-20 1586,-20 1592,-26 1592,-32 1592,-32 1592,-132 1592,-132 1592,-138 1586,-144 1580,-144 1580,-144 814,-144 814,-144 808,-144 802,-138 802,-132 802,-132 802,-32 802,-32 802,-26 808,-20 814,-20\"/>\n<text text-anchor=\"middle\" x=\"1197\" y=\"-128.8\" font-family=\"Times,serif\" font-size=\"14.00\">residual</text>\n</g>\n<g id=\"clust4\" class=\"cluster\"><title>cluster_layer1_residual_res1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M904,-28C904,-28 1130,-28 1130,-28 1136,-28 1142,-34 1142,-40 1142,-40 1142,-91 1142,-91 1142,-97 1136,-103 1130,-103 1130,-103 904,-103 904,-103 898,-103 892,-97 892,-91 892,-91 892,-40 892,-40 892,-34 898,-28 904,-28\"/>\n<text text-anchor=\"middle\" x=\"1017\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">res1</text>\n</g>\n<g id=\"clust5\" class=\"cluster\"><title>cluster_layer1_residual_res2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1174,-28C1174,-28 1400,-28 1400,-28 1406,-28 1412,-34 1412,-40 1412,-40 1412,-91 1412,-91 1412,-97 1406,-103 1400,-103 1400,-103 1174,-103 1174,-103 1168,-103 1162,-97 1162,-91 1162,-91 1162,-40 1162,-40 1162,-34 1168,-28 1174,-28\"/>\n<text text-anchor=\"middle\" x=\"1287\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">res2</text>\n</g>\n<g id=\"clust6\" class=\"cluster\"><title>cluster_layer2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1624,-53C1624,-53 1940,-53 1940,-53 1946,-53 1952,-59 1952,-65 1952,-65 1952,-116 1952,-116 1952,-122 1946,-128 1940,-128 1940,-128 1624,-128 1624,-128 1618,-128 1612,-122 1612,-116 1612,-116 1612,-65 1612,-65 1612,-59 1618,-53 1624,-53\"/>\n<text text-anchor=\"middle\" x=\"1782\" y=\"-112.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer2</text>\n</g>\n<g id=\"clust7\" class=\"cluster\"><title>cluster_layer3</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M1984,-8C1984,-8 3118,-8 3118,-8 3124,-8 3130,-14 3130,-20 3130,-20 3130,-159 3130,-159 3130,-165 3124,-171 3118,-171 3118,-171 1984,-171 1984,-171 1978,-171 1972,-165 1972,-159 1972,-159 1972,-20 1972,-20 1972,-14 1978,-8 1984,-8\"/>\n<text text-anchor=\"middle\" x=\"2551\" y=\"-155.8\" font-family=\"Times,serif\" font-size=\"14.00\">layer3</text>\n</g>\n<g id=\"clust8\" class=\"cluster\"><title>cluster_layer3_residual</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2344,-16C2344,-16 3110,-16 3110,-16 3116,-16 3122,-22 3122,-28 3122,-28 3122,-128 3122,-128 3122,-134 3116,-140 3110,-140 3110,-140 2344,-140 2344,-140 2338,-140 2332,-134 2332,-128 2332,-128 2332,-28 2332,-28 2332,-22 2338,-16 2344,-16\"/>\n<text text-anchor=\"middle\" x=\"2727\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">residual</text>\n</g>\n<g id=\"clust9\" class=\"cluster\"><title>cluster_layer3_residual_res1</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2434,-24C2434,-24 2660,-24 2660,-24 2666,-24 2672,-30 2672,-36 2672,-36 2672,-87 2672,-87 2672,-93 2666,-99 2660,-99 2660,-99 2434,-99 2434,-99 2428,-99 2422,-93 2422,-87 2422,-87 2422,-36 2422,-36 2422,-30 2428,-24 2434,-24\"/>\n<text text-anchor=\"middle\" x=\"2547\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\">res1</text>\n</g>\n<g id=\"clust10\" class=\"cluster\"><title>cluster_layer3_residual_res2</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M2704,-24C2704,-24 2930,-24 2930,-24 2936,-24 2942,-30 2942,-36 2942,-36 2942,-87 2942,-87 2942,-93 2936,-99 2930,-99 2930,-99 2704,-99 2704,-99 2698,-99 2692,-93 2692,-87 2692,-87 2692,-36 2692,-36 2692,-30 2698,-24 2704,-24\"/>\n<text text-anchor=\"middle\" x=\"2817\" y=\"-83.8\" font-family=\"Times,serif\" font-size=\"14.00\">res2</text>\n</g>\n<g id=\"clust11\" class=\"cluster\"><title>cluster_classifier</title>\n<path fill=\"#777777\" fill-opacity=\"0.266667\" stroke=\"black\" d=\"M3244,-49C3244,-49 3470,-49 3470,-49 3476,-49 3482,-55 3482,-61 3482,-61 3482,-112 3482,-112 3482,-118 3476,-124 3470,-124 3470,-124 3244,-124 3244,-124 3238,-124 3232,-118 3232,-112 3232,-112 3232,-61 3232,-61 3232,-55 3238,-49 3244,-49\"/>\n<text text-anchor=\"middle\" x=\"3357\" y=\"-108.8\" font-family=\"Times,serif\" font-size=\"14.00\">classifier</text>\n</g>\n<!-- prep_whiten -->\n<g id=\"node1\" class=\"node\"><title>prep_whiten</title>\n<g id=\"a_node1\"><a xlink:title=\"&lt;function identity at 0x7f414a26c6a8&gt; {&#39;value&#39;: Conv2d(3, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)}\">\n<path fill=\"#dea05e\" stroke=\"black\" d=\"M132,-92C132,-92 102,-92 102,-92 96,-92 90,-86 90,-80 90,-80 90,-68 90,-68 90,-62 96,-56 102,-56 102,-56 132,-56 132,-56 138,-56 144,-62 144,-68 144,-68 144,-80 144,-80 144,-86 138,-92 132,-92\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">whiten</text>\n</a>\n</g>\n</g>\n<!-- prep_conv -->\n<g id=\"node2\" class=\"node\"><title>prep_conv</title>\n<g id=\"a_node2\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 27, &#39;out_channels&#39;: 64, &#39;kernel_size&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M222,-92C222,-92 192,-92 192,-92 186,-92 180,-86 180,-80 180,-80 180,-68 180,-68 180,-62 186,-56 192,-56 192,-56 222,-56 222,-56 228,-56 234,-62 234,-68 234,-68 234,-80 234,-80 234,-86 228,-92 222,-92\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- prep_whiten&#45;&gt;prep_conv -->\n<g id=\"edge2\" class=\"edge\"><title>prep_whiten&#45;&gt;prep_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M144.403,-74C152.393,-74 161.311,-74 169.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"169.919,-77.5001 179.919,-74 169.919,-70.5001 169.919,-77.5001\"/>\n</g>\n<!-- prep_norm -->\n<g id=\"node3\" class=\"node\"><title>prep_norm</title>\n<g id=\"a_node3\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 64}\">\n<path fill=\"#d0c281\" stroke=\"black\" d=\"M312,-92C312,-92 282,-92 282,-92 276,-92 270,-86 270,-80 270,-80 270,-68 270,-68 270,-62 276,-56 282,-56 282,-56 312,-56 312,-56 318,-56 324,-62 324,-68 324,-68 324,-80 324,-80 324,-86 318,-92 312,-92\"/>\n<text text-anchor=\"middle\" x=\"297\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- prep_conv&#45;&gt;prep_norm -->\n<g id=\"edge3\" class=\"edge\"><title>prep_conv&#45;&gt;prep_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M234.403,-74C242.393,-74 251.311,-74 259.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"259.919,-77.5001 269.919,-74 259.919,-70.5001 259.919,-77.5001\"/>\n</g>\n<!-- prep_act -->\n<g id=\"node4\" class=\"node\"><title>prep_act</title>\n<g id=\"a_node4\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#f0e189\" stroke=\"black\" d=\"M402,-92C402,-92 372,-92 372,-92 366,-92 360,-86 360,-80 360,-80 360,-68 360,-68 360,-62 366,-56 372,-56 372,-56 402,-56 402,-56 408,-56 414,-62 414,-68 414,-68 414,-80 414,-80 414,-86 408,-92 402,-92\"/>\n<text text-anchor=\"middle\" x=\"387\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- prep_norm&#45;&gt;prep_act -->\n<g id=\"edge4\" class=\"edge\"><title>prep_norm&#45;&gt;prep_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M324.403,-74C332.393,-74 341.311,-74 349.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"349.919,-77.5001 359.919,-74 349.919,-70.5001 349.919,-77.5001\"/>\n</g>\n<!-- layer1_conv -->\n<g id=\"node5\" class=\"node\"><title>layer1_conv</title>\n<g id=\"a_node5\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 64, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M492,-92C492,-92 462,-92 462,-92 456,-92 450,-86 450,-80 450,-80 450,-68 450,-68 450,-62 456,-56 462,-56 462,-56 492,-56 492,-56 498,-56 504,-62 504,-68 504,-68 504,-80 504,-80 504,-86 498,-92 492,-92\"/>\n<text text-anchor=\"middle\" x=\"477\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- prep_act&#45;&gt;layer1_conv -->\n<g id=\"edge5\" class=\"edge\"><title>prep_act&#45;&gt;layer1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M414.403,-74C422.393,-74 431.311,-74 439.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"439.919,-77.5001 449.919,-74 439.919,-70.5001 439.919,-77.5001\"/>\n</g>\n<!-- layer1_pool -->\n<g id=\"node6\" class=\"node\"><title>layer1_pool</title>\n<g id=\"a_node6\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M582,-92C582,-92 552,-92 552,-92 546,-92 540,-86 540,-80 540,-80 540,-68 540,-68 540,-62 546,-56 552,-56 552,-56 582,-56 582,-56 588,-56 594,-62 594,-68 594,-68 594,-80 594,-80 594,-86 588,-92 582,-92\"/>\n<text text-anchor=\"middle\" x=\"567\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer1_conv&#45;&gt;layer1_pool -->\n<g id=\"edge6\" class=\"edge\"><title>layer1_conv&#45;&gt;layer1_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M504.403,-74C512.393,-74 521.311,-74 529.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"529.919,-77.5001 539.919,-74 529.919,-70.5001 529.919,-77.5001\"/>\n</g>\n<!-- layer1_norm -->\n<g id=\"node7\" class=\"node\"><title>layer1_norm</title>\n<g id=\"a_node7\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 128}\">\n<path fill=\"#d0c281\" stroke=\"black\" d=\"M672,-92C672,-92 642,-92 642,-92 636,-92 630,-86 630,-80 630,-80 630,-68 630,-68 630,-62 636,-56 642,-56 642,-56 672,-56 672,-56 678,-56 684,-62 684,-68 684,-68 684,-80 684,-80 684,-86 678,-92 672,-92\"/>\n<text text-anchor=\"middle\" x=\"657\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_pool&#45;&gt;layer1_norm -->\n<g id=\"edge7\" class=\"edge\"><title>layer1_pool&#45;&gt;layer1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M594.403,-74C602.393,-74 611.311,-74 619.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"619.919,-77.5001 629.919,-74 619.919,-70.5001 619.919,-77.5001\"/>\n</g>\n<!-- layer1_act -->\n<g id=\"node8\" class=\"node\"><title>layer1_act</title>\n<g id=\"a_node8\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#f0e189\" stroke=\"black\" d=\"M762,-92C762,-92 732,-92 732,-92 726,-92 720,-86 720,-80 720,-80 720,-68 720,-68 720,-62 726,-56 732,-56 732,-56 762,-56 762,-56 768,-56 774,-62 774,-68 774,-68 774,-80 774,-80 774,-86 768,-92 762,-92\"/>\n<text text-anchor=\"middle\" x=\"747\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_norm&#45;&gt;layer1_act -->\n<g id=\"edge8\" class=\"edge\"><title>layer1_norm&#45;&gt;layer1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M684.403,-74C692.393,-74 701.311,-74 709.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"709.919,-77.5001 719.919,-74 709.919,-70.5001 709.919,-77.5001\"/>\n</g>\n<!-- layer1_residual_in -->\n<g id=\"node9\" class=\"node\"><title>layer1_residual_in</title>\n<g id=\"a_node9\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M852,-92C852,-92 822,-92 822,-92 816,-92 810,-86 810,-80 810,-80 810,-68 810,-68 810,-62 816,-56 822,-56 822,-56 852,-56 852,-56 858,-56 864,-62 864,-68 864,-68 864,-80 864,-80 864,-86 858,-92 852,-92\"/>\n<text text-anchor=\"middle\" x=\"837\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">in</text>\n</a>\n</g>\n</g>\n<!-- layer1_act&#45;&gt;layer1_residual_in -->\n<g id=\"edge9\" class=\"edge\"><title>layer1_act&#45;&gt;layer1_residual_in</title>\n<path fill=\"none\" stroke=\"black\" d=\"M774.403,-74C782.393,-74 791.311,-74 799.824,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"799.919,-77.5001 809.919,-74 799.919,-70.5001 799.919,-77.5001\"/>\n</g>\n<!-- layer1_residual_res1_conv -->\n<g id=\"node10\" class=\"node\"><title>layer1_residual_res1_conv</title>\n<g id=\"a_node10\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M942,-72C942,-72 912,-72 912,-72 906,-72 900,-66 900,-60 900,-60 900,-48 900,-48 900,-42 906,-36 912,-36 912,-36 942,-36 942,-36 948,-36 954,-42 954,-48 954,-48 954,-60 954,-60 954,-66 948,-72 942,-72\"/>\n<text text-anchor=\"middle\" x=\"927\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_in&#45;&gt;layer1_residual_res1_conv -->\n<g id=\"edge10\" class=\"edge\"><title>layer1_residual_in&#45;&gt;layer1_residual_res1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M864.403,-67.9993C872.481,-66.1634 881.507,-64.1121 890.105,-62.158\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"890.943,-65.5568 899.919,-59.9275 889.392,-58.7308 890.943,-65.5568\"/>\n</g>\n<!-- layer1_residual_add -->\n<g id=\"node17\" class=\"node\"><title>layer1_residual_add</title>\n<g id=\"a_node17\"><a xlink:title=\"&lt;class &#39;__main__.Add&#39;&gt; {}\">\n<path fill=\"#fdb462\" stroke=\"black\" d=\"M1572,-97C1572,-97 1542,-97 1542,-97 1536,-97 1530,-91 1530,-85 1530,-85 1530,-73 1530,-73 1530,-67 1536,-61 1542,-61 1542,-61 1572,-61 1572,-61 1578,-61 1584,-67 1584,-73 1584,-73 1584,-85 1584,-85 1584,-91 1578,-97 1572,-97\"/>\n<text text-anchor=\"middle\" x=\"1557\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">add</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_in&#45;&gt;layer1_residual_add -->\n<g id=\"edge17\" class=\"edge\"><title>layer1_residual_in&#45;&gt;layer1_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M861.737,-92.1018C870.735,-97.9639 881.381,-103.78 892,-107 944.782,-123.006 960.844,-112 1016,-112 1016,-112 1016,-112 1378,-112 1427.77,-112 1484.08,-99.3711 1519.8,-89.7384\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1521.04,-93.0265 1529.74,-86.9884 1519.17,-86.2796 1521.04,-93.0265\"/>\n</g>\n<!-- layer1_residual_res1_norm -->\n<g id=\"node11\" class=\"node\"><title>layer1_residual_res1_norm</title>\n<g id=\"a_node11\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 128}\">\n<path fill=\"#d0c281\" stroke=\"black\" d=\"M1032,-72C1032,-72 1002,-72 1002,-72 996,-72 990,-66 990,-60 990,-60 990,-48 990,-48 990,-42 996,-36 1002,-36 1002,-36 1032,-36 1032,-36 1038,-36 1044,-42 1044,-48 1044,-48 1044,-60 1044,-60 1044,-66 1038,-72 1032,-72\"/>\n<text text-anchor=\"middle\" x=\"1017\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_conv&#45;&gt;layer1_residual_res1_norm -->\n<g id=\"edge11\" class=\"edge\"><title>layer1_residual_res1_conv&#45;&gt;layer1_residual_res1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M954.403,-54C962.393,-54 971.311,-54 979.824,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"979.919,-57.5001 989.919,-54 979.919,-50.5001 979.919,-57.5001\"/>\n</g>\n<!-- layer1_residual_res1_act -->\n<g id=\"node12\" class=\"node\"><title>layer1_residual_res1_act</title>\n<g id=\"a_node12\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#f0e189\" stroke=\"black\" d=\"M1122,-72C1122,-72 1092,-72 1092,-72 1086,-72 1080,-66 1080,-60 1080,-60 1080,-48 1080,-48 1080,-42 1086,-36 1092,-36 1092,-36 1122,-36 1122,-36 1128,-36 1134,-42 1134,-48 1134,-48 1134,-60 1134,-60 1134,-66 1128,-72 1122,-72\"/>\n<text text-anchor=\"middle\" x=\"1107\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_norm&#45;&gt;layer1_residual_res1_act -->\n<g id=\"edge12\" class=\"edge\"><title>layer1_residual_res1_norm&#45;&gt;layer1_residual_res1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1044.4,-54C1052.39,-54 1061.31,-54 1069.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1069.92,-57.5001 1079.92,-54 1069.92,-50.5001 1069.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_conv -->\n<g id=\"node13\" class=\"node\"><title>layer1_residual_res2_conv</title>\n<g id=\"a_node13\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 128, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M1212,-72C1212,-72 1182,-72 1182,-72 1176,-72 1170,-66 1170,-60 1170,-60 1170,-48 1170,-48 1170,-42 1176,-36 1182,-36 1182,-36 1212,-36 1212,-36 1218,-36 1224,-42 1224,-48 1224,-48 1224,-60 1224,-60 1224,-66 1218,-72 1212,-72\"/>\n<text text-anchor=\"middle\" x=\"1197\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res1_act&#45;&gt;layer1_residual_res2_conv -->\n<g id=\"edge13\" class=\"edge\"><title>layer1_residual_res1_act&#45;&gt;layer1_residual_res2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1134.4,-54C1142.39,-54 1151.31,-54 1159.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1159.92,-57.5001 1169.92,-54 1159.92,-50.5001 1159.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_norm -->\n<g id=\"node14\" class=\"node\"><title>layer1_residual_res2_norm</title>\n<g id=\"a_node14\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 128}\">\n<path fill=\"#d0c281\" stroke=\"black\" d=\"M1302,-72C1302,-72 1272,-72 1272,-72 1266,-72 1260,-66 1260,-60 1260,-60 1260,-48 1260,-48 1260,-42 1266,-36 1272,-36 1272,-36 1302,-36 1302,-36 1308,-36 1314,-42 1314,-48 1314,-48 1314,-60 1314,-60 1314,-66 1308,-72 1302,-72\"/>\n<text text-anchor=\"middle\" x=\"1287\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_conv&#45;&gt;layer1_residual_res2_norm -->\n<g id=\"edge14\" class=\"edge\"><title>layer1_residual_res2_conv&#45;&gt;layer1_residual_res2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1224.4,-54C1232.39,-54 1241.31,-54 1249.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1249.92,-57.5001 1259.92,-54 1249.92,-50.5001 1249.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_res2_act -->\n<g id=\"node15\" class=\"node\"><title>layer1_residual_res2_act</title>\n<g id=\"a_node15\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#f0e189\" stroke=\"black\" d=\"M1392,-72C1392,-72 1362,-72 1362,-72 1356,-72 1350,-66 1350,-60 1350,-60 1350,-48 1350,-48 1350,-42 1356,-36 1362,-36 1362,-36 1392,-36 1392,-36 1398,-36 1404,-42 1404,-48 1404,-48 1404,-60 1404,-60 1404,-66 1398,-72 1392,-72\"/>\n<text text-anchor=\"middle\" x=\"1377\" y=\"-50.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_norm&#45;&gt;layer1_residual_res2_act -->\n<g id=\"edge15\" class=\"edge\"><title>layer1_residual_res2_norm&#45;&gt;layer1_residual_res2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1314.4,-54C1322.39,-54 1331.31,-54 1339.82,-54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1339.92,-57.5001 1349.92,-54 1339.92,-50.5001 1339.92,-57.5001\"/>\n</g>\n<!-- layer1_residual_out -->\n<g id=\"node16\" class=\"node\"><title>layer1_residual_out</title>\n<g id=\"a_node16\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M1482,-82C1482,-82 1452,-82 1452,-82 1446,-82 1440,-76 1440,-70 1440,-70 1440,-58 1440,-58 1440,-52 1446,-46 1452,-46 1452,-46 1482,-46 1482,-46 1488,-46 1494,-52 1494,-58 1494,-58 1494,-70 1494,-70 1494,-76 1488,-82 1482,-82\"/>\n<text text-anchor=\"middle\" x=\"1467\" y=\"-60.3\" font-family=\"Times,serif\" font-size=\"14.00\">out</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_res2_act&#45;&gt;layer1_residual_out -->\n<g id=\"edge16\" class=\"edge\"><title>layer1_residual_res2_act&#45;&gt;layer1_residual_out</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1404.4,-57.0003C1412.39,-57.9083 1421.31,-58.9217 1429.82,-59.8891\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1429.59,-63.3847 1439.92,-61.0362 1430.38,-56.4294 1429.59,-63.3847\"/>\n</g>\n<!-- layer1_residual_out&#45;&gt;layer1_residual_add -->\n<g id=\"edge18\" class=\"edge\"><title>layer1_residual_out&#45;&gt;layer1_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1494.4,-68.5005C1502.39,-69.8625 1511.31,-71.3825 1519.82,-72.8336\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1519.47,-76.3242 1529.92,-74.5544 1520.65,-69.4237 1519.47,-76.3242\"/>\n</g>\n<!-- layer2_conv -->\n<g id=\"node18\" class=\"node\"><title>layer2_conv</title>\n<g id=\"a_node18\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 128, &#39;out_channels&#39;: 256, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M1662,-97C1662,-97 1632,-97 1632,-97 1626,-97 1620,-91 1620,-85 1620,-85 1620,-73 1620,-73 1620,-67 1626,-61 1632,-61 1632,-61 1662,-61 1662,-61 1668,-61 1674,-67 1674,-73 1674,-73 1674,-85 1674,-85 1674,-91 1668,-97 1662,-97\"/>\n<text text-anchor=\"middle\" x=\"1647\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer1_residual_add&#45;&gt;layer2_conv -->\n<g id=\"edge19\" class=\"edge\"><title>layer1_residual_add&#45;&gt;layer2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1584.4,-79C1592.39,-79 1601.31,-79 1609.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1609.92,-82.5001 1619.92,-79 1609.92,-75.5001 1609.92,-82.5001\"/>\n</g>\n<!-- layer2_pool -->\n<g id=\"node19\" class=\"node\"><title>layer2_pool</title>\n<g id=\"a_node19\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M1752,-97C1752,-97 1722,-97 1722,-97 1716,-97 1710,-91 1710,-85 1710,-85 1710,-73 1710,-73 1710,-67 1716,-61 1722,-61 1722,-61 1752,-61 1752,-61 1758,-61 1764,-67 1764,-73 1764,-73 1764,-85 1764,-85 1764,-91 1758,-97 1752,-97\"/>\n<text text-anchor=\"middle\" x=\"1737\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer2_conv&#45;&gt;layer2_pool -->\n<g id=\"edge20\" class=\"edge\"><title>layer2_conv&#45;&gt;layer2_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1674.4,-79C1682.39,-79 1691.31,-79 1699.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1699.92,-82.5001 1709.92,-79 1699.92,-75.5001 1699.92,-82.5001\"/>\n</g>\n<!-- layer2_norm -->\n<g id=\"node20\" class=\"node\"><title>layer2_norm</title>\n<g id=\"a_node20\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 256}\">\n<path fill=\"#d0c281\" stroke=\"black\" d=\"M1842,-97C1842,-97 1812,-97 1812,-97 1806,-97 1800,-91 1800,-85 1800,-85 1800,-73 1800,-73 1800,-67 1806,-61 1812,-61 1812,-61 1842,-61 1842,-61 1848,-61 1854,-67 1854,-73 1854,-73 1854,-85 1854,-85 1854,-91 1848,-97 1842,-97\"/>\n<text text-anchor=\"middle\" x=\"1827\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer2_pool&#45;&gt;layer2_norm -->\n<g id=\"edge21\" class=\"edge\"><title>layer2_pool&#45;&gt;layer2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1764.4,-79C1772.39,-79 1781.31,-79 1789.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1789.92,-82.5001 1799.92,-79 1789.92,-75.5001 1789.92,-82.5001\"/>\n</g>\n<!-- layer2_act -->\n<g id=\"node21\" class=\"node\"><title>layer2_act</title>\n<g id=\"a_node21\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#f0e189\" stroke=\"black\" d=\"M1932,-97C1932,-97 1902,-97 1902,-97 1896,-97 1890,-91 1890,-85 1890,-85 1890,-73 1890,-73 1890,-67 1896,-61 1902,-61 1902,-61 1932,-61 1932,-61 1938,-61 1944,-67 1944,-73 1944,-73 1944,-85 1944,-85 1944,-91 1938,-97 1932,-97\"/>\n<text text-anchor=\"middle\" x=\"1917\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer2_norm&#45;&gt;layer2_act -->\n<g id=\"edge22\" class=\"edge\"><title>layer2_norm&#45;&gt;layer2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1854.4,-79C1862.39,-79 1871.31,-79 1879.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1879.92,-82.5001 1889.92,-79 1879.92,-75.5001 1879.92,-82.5001\"/>\n</g>\n<!-- layer3_conv -->\n<g id=\"node22\" class=\"node\"><title>layer3_conv</title>\n<g id=\"a_node22\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 256, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M2022,-97C2022,-97 1992,-97 1992,-97 1986,-97 1980,-91 1980,-85 1980,-85 1980,-73 1980,-73 1980,-67 1986,-61 1992,-61 1992,-61 2022,-61 2022,-61 2028,-61 2034,-67 2034,-73 2034,-73 2034,-85 2034,-85 2034,-91 2028,-97 2022,-97\"/>\n<text text-anchor=\"middle\" x=\"2007\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer2_act&#45;&gt;layer3_conv -->\n<g id=\"edge23\" class=\"edge\"><title>layer2_act&#45;&gt;layer3_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1944.4,-79C1952.39,-79 1961.31,-79 1969.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1969.92,-82.5001 1979.92,-79 1969.92,-75.5001 1969.92,-82.5001\"/>\n</g>\n<!-- layer3_pool -->\n<g id=\"node23\" class=\"node\"><title>layer3_pool</title>\n<g id=\"a_node23\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 2}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M2112,-97C2112,-97 2082,-97 2082,-97 2076,-97 2070,-91 2070,-85 2070,-85 2070,-73 2070,-73 2070,-67 2076,-61 2082,-61 2082,-61 2112,-61 2112,-61 2118,-61 2124,-67 2124,-73 2124,-73 2124,-85 2124,-85 2124,-91 2118,-97 2112,-97\"/>\n<text text-anchor=\"middle\" x=\"2097\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer3_conv&#45;&gt;layer3_pool -->\n<g id=\"edge24\" class=\"edge\"><title>layer3_conv&#45;&gt;layer3_pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2034.4,-79C2042.39,-79 2051.31,-79 2059.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2059.92,-82.5001 2069.92,-79 2059.92,-75.5001 2059.92,-82.5001\"/>\n</g>\n<!-- layer3_norm -->\n<g id=\"node24\" class=\"node\"><title>layer3_norm</title>\n<g id=\"a_node24\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 512}\">\n<path fill=\"#d0c281\" stroke=\"black\" d=\"M2202,-97C2202,-97 2172,-97 2172,-97 2166,-97 2160,-91 2160,-85 2160,-85 2160,-73 2160,-73 2160,-67 2166,-61 2172,-61 2172,-61 2202,-61 2202,-61 2208,-61 2214,-67 2214,-73 2214,-73 2214,-85 2214,-85 2214,-91 2208,-97 2202,-97\"/>\n<text text-anchor=\"middle\" x=\"2187\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_pool&#45;&gt;layer3_norm -->\n<g id=\"edge25\" class=\"edge\"><title>layer3_pool&#45;&gt;layer3_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2124.4,-79C2132.39,-79 2141.31,-79 2149.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2149.92,-82.5001 2159.92,-79 2149.92,-75.5001 2149.92,-82.5001\"/>\n</g>\n<!-- layer3_act -->\n<g id=\"node25\" class=\"node\"><title>layer3_act</title>\n<g id=\"a_node25\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#f0e189\" stroke=\"black\" d=\"M2292,-97C2292,-97 2262,-97 2262,-97 2256,-97 2250,-91 2250,-85 2250,-85 2250,-73 2250,-73 2250,-67 2256,-61 2262,-61 2262,-61 2292,-61 2292,-61 2298,-61 2304,-67 2304,-73 2304,-73 2304,-85 2304,-85 2304,-91 2298,-97 2292,-97\"/>\n<text text-anchor=\"middle\" x=\"2277\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_norm&#45;&gt;layer3_act -->\n<g id=\"edge26\" class=\"edge\"><title>layer3_norm&#45;&gt;layer3_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2214.4,-79C2222.39,-79 2231.31,-79 2239.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2239.92,-82.5001 2249.92,-79 2239.92,-75.5001 2239.92,-82.5001\"/>\n</g>\n<!-- layer3_residual_in -->\n<g id=\"node26\" class=\"node\"><title>layer3_residual_in</title>\n<g id=\"a_node26\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M2382,-97C2382,-97 2352,-97 2352,-97 2346,-97 2340,-91 2340,-85 2340,-85 2340,-73 2340,-73 2340,-67 2346,-61 2352,-61 2352,-61 2382,-61 2382,-61 2388,-61 2394,-67 2394,-73 2394,-73 2394,-85 2394,-85 2394,-91 2388,-97 2382,-97\"/>\n<text text-anchor=\"middle\" x=\"2367\" y=\"-75.3\" font-family=\"Times,serif\" font-size=\"14.00\">in</text>\n</a>\n</g>\n</g>\n<!-- layer3_act&#45;&gt;layer3_residual_in -->\n<g id=\"edge27\" class=\"edge\"><title>layer3_act&#45;&gt;layer3_residual_in</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2304.4,-79C2312.39,-79 2321.31,-79 2329.82,-79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2329.92,-82.5001 2339.92,-79 2329.92,-75.5001 2329.92,-82.5001\"/>\n</g>\n<!-- layer3_residual_res1_conv -->\n<g id=\"node27\" class=\"node\"><title>layer3_residual_res1_conv</title>\n<g id=\"a_node27\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 512, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M2472,-68C2472,-68 2442,-68 2442,-68 2436,-68 2430,-62 2430,-56 2430,-56 2430,-44 2430,-44 2430,-38 2436,-32 2442,-32 2442,-32 2472,-32 2472,-32 2478,-32 2484,-38 2484,-44 2484,-44 2484,-56 2484,-56 2484,-62 2478,-68 2472,-68\"/>\n<text text-anchor=\"middle\" x=\"2457\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_in&#45;&gt;layer3_residual_res1_conv -->\n<g id=\"edge28\" class=\"edge\"><title>layer3_residual_in&#45;&gt;layer3_residual_res1_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2394.4,-70.299C2402.57,-67.608 2411.7,-64.5979 2420.38,-61.7368\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2421.52,-65.049 2429.92,-58.5949 2419.33,-58.4007 2421.52,-65.049\"/>\n</g>\n<!-- layer3_residual_add -->\n<g id=\"node34\" class=\"node\"><title>layer3_residual_add</title>\n<g id=\"a_node34\"><a xlink:title=\"&lt;class &#39;__main__.Add&#39;&gt; {}\">\n<path fill=\"#fdb462\" stroke=\"black\" d=\"M3102,-93C3102,-93 3072,-93 3072,-93 3066,-93 3060,-87 3060,-81 3060,-81 3060,-69 3060,-69 3060,-63 3066,-57 3072,-57 3072,-57 3102,-57 3102,-57 3108,-57 3114,-63 3114,-69 3114,-69 3114,-81 3114,-81 3114,-87 3108,-93 3102,-93\"/>\n<text text-anchor=\"middle\" x=\"3087\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">add</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_in&#45;&gt;layer3_residual_add -->\n<g id=\"edge35\" class=\"edge\"><title>layer3_residual_in&#45;&gt;layer3_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2394.22,-93.0359C2402.83,-96.9904 2412.61,-100.808 2422,-103 2475.71,-115.535 2490.84,-108 2546,-108 2546,-108 2546,-108 2908,-108 2957.77,-108 3014.08,-95.3711 3049.8,-85.7384\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3051.04,-89.0265 3059.74,-82.9884 3049.17,-82.2796 3051.04,-89.0265\"/>\n</g>\n<!-- layer3_residual_res1_norm -->\n<g id=\"node28\" class=\"node\"><title>layer3_residual_res1_norm</title>\n<g id=\"a_node28\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 512}\">\n<path fill=\"#d0c281\" stroke=\"black\" d=\"M2562,-68C2562,-68 2532,-68 2532,-68 2526,-68 2520,-62 2520,-56 2520,-56 2520,-44 2520,-44 2520,-38 2526,-32 2532,-32 2532,-32 2562,-32 2562,-32 2568,-32 2574,-38 2574,-44 2574,-44 2574,-56 2574,-56 2574,-62 2568,-68 2562,-68\"/>\n<text text-anchor=\"middle\" x=\"2547\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_conv&#45;&gt;layer3_residual_res1_norm -->\n<g id=\"edge29\" class=\"edge\"><title>layer3_residual_res1_conv&#45;&gt;layer3_residual_res1_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2484.4,-50C2492.39,-50 2501.31,-50 2509.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2509.92,-53.5001 2519.92,-50 2509.92,-46.5001 2509.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res1_act -->\n<g id=\"node29\" class=\"node\"><title>layer3_residual_res1_act</title>\n<g id=\"a_node29\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#f0e189\" stroke=\"black\" d=\"M2652,-68C2652,-68 2622,-68 2622,-68 2616,-68 2610,-62 2610,-56 2610,-56 2610,-44 2610,-44 2610,-38 2616,-32 2622,-32 2622,-32 2652,-32 2652,-32 2658,-32 2664,-38 2664,-44 2664,-44 2664,-56 2664,-56 2664,-62 2658,-68 2652,-68\"/>\n<text text-anchor=\"middle\" x=\"2637\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_norm&#45;&gt;layer3_residual_res1_act -->\n<g id=\"edge30\" class=\"edge\"><title>layer3_residual_res1_norm&#45;&gt;layer3_residual_res1_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2574.4,-50C2582.39,-50 2591.31,-50 2599.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2599.92,-53.5001 2609.92,-50 2599.92,-46.5001 2599.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_conv -->\n<g id=\"node30\" class=\"node\"><title>layer3_residual_res2_conv</title>\n<g id=\"a_node30\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.conv.Conv2d&#39;&gt; {&#39;in_channels&#39;: 512, &#39;out_channels&#39;: 512, &#39;kernel_size&#39;: (3, 3), &#39;stride&#39;: (1, 1), &#39;padding&#39;: (1, 1), &#39;bias&#39;: False}\">\n<path fill=\"#bebada\" stroke=\"black\" d=\"M2742,-68C2742,-68 2712,-68 2712,-68 2706,-68 2700,-62 2700,-56 2700,-56 2700,-44 2700,-44 2700,-38 2706,-32 2712,-32 2712,-32 2742,-32 2742,-32 2748,-32 2754,-38 2754,-44 2754,-44 2754,-56 2754,-56 2754,-62 2748,-68 2742,-68\"/>\n<text text-anchor=\"middle\" x=\"2727\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res1_act&#45;&gt;layer3_residual_res2_conv -->\n<g id=\"edge31\" class=\"edge\"><title>layer3_residual_res1_act&#45;&gt;layer3_residual_res2_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2664.4,-50C2672.39,-50 2681.31,-50 2689.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2689.92,-53.5001 2699.92,-50 2689.92,-46.5001 2689.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_norm -->\n<g id=\"node31\" class=\"node\"><title>layer3_residual_res2_norm</title>\n<g id=\"a_node31\"><a xlink:title=\"functools.partial(&lt;class &#39;__main__.GhostBatchNorm&#39;&gt;, num_splits=16, weight=False) {&#39;num_features&#39;: 512}\">\n<path fill=\"#d0c281\" stroke=\"black\" d=\"M2832,-68C2832,-68 2802,-68 2802,-68 2796,-68 2790,-62 2790,-56 2790,-56 2790,-44 2790,-44 2790,-38 2796,-32 2802,-32 2802,-32 2832,-32 2832,-32 2838,-32 2844,-38 2844,-44 2844,-44 2844,-56 2844,-56 2844,-62 2838,-68 2832,-68\"/>\n<text text-anchor=\"middle\" x=\"2817\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">norm</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_conv&#45;&gt;layer3_residual_res2_norm -->\n<g id=\"edge32\" class=\"edge\"><title>layer3_residual_res2_conv&#45;&gt;layer3_residual_res2_norm</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2754.4,-50C2762.39,-50 2771.31,-50 2779.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2779.92,-53.5001 2789.92,-50 2779.92,-46.5001 2779.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_res2_act -->\n<g id=\"node32\" class=\"node\"><title>layer3_residual_res2_act</title>\n<g id=\"a_node32\"><a xlink:title=\"functools.partial(&lt;class &#39;torch.nn.modules.activation.CELU&#39;&gt;, 0.3) {}\">\n<path fill=\"#f0e189\" stroke=\"black\" d=\"M2922,-68C2922,-68 2892,-68 2892,-68 2886,-68 2880,-62 2880,-56 2880,-56 2880,-44 2880,-44 2880,-38 2886,-32 2892,-32 2892,-32 2922,-32 2922,-32 2928,-32 2934,-38 2934,-44 2934,-44 2934,-56 2934,-56 2934,-62 2928,-68 2922,-68\"/>\n<text text-anchor=\"middle\" x=\"2907\" y=\"-46.3\" font-family=\"Times,serif\" font-size=\"14.00\">act</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_norm&#45;&gt;layer3_residual_res2_act -->\n<g id=\"edge33\" class=\"edge\"><title>layer3_residual_res2_norm&#45;&gt;layer3_residual_res2_act</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2844.4,-50C2852.39,-50 2861.31,-50 2869.82,-50\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2869.92,-53.5001 2879.92,-50 2869.92,-46.5001 2869.92,-53.5001\"/>\n</g>\n<!-- layer3_residual_out -->\n<g id=\"node33\" class=\"node\"><title>layer3_residual_out</title>\n<g id=\"a_node33\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M3012,-78C3012,-78 2982,-78 2982,-78 2976,-78 2970,-72 2970,-66 2970,-66 2970,-54 2970,-54 2970,-48 2976,-42 2982,-42 2982,-42 3012,-42 3012,-42 3018,-42 3024,-48 3024,-54 3024,-54 3024,-66 3024,-66 3024,-72 3018,-78 3012,-78\"/>\n<text text-anchor=\"middle\" x=\"2997\" y=\"-56.3\" font-family=\"Times,serif\" font-size=\"14.00\">out</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_res2_act&#45;&gt;layer3_residual_out -->\n<g id=\"edge34\" class=\"edge\"><title>layer3_residual_res2_act&#45;&gt;layer3_residual_out</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2934.4,-53.0003C2942.39,-53.9083 2951.31,-54.9217 2959.82,-55.8891\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2959.59,-59.3847 2969.92,-57.0362 2960.38,-52.4294 2959.59,-59.3847\"/>\n</g>\n<!-- layer3_residual_out&#45;&gt;layer3_residual_add -->\n<g id=\"edge36\" class=\"edge\"><title>layer3_residual_out&#45;&gt;layer3_residual_add</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3024.4,-64.5005C3032.39,-65.8625 3041.31,-67.3825 3049.82,-68.8336\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3049.47,-72.3242 3059.92,-70.5544 3050.65,-65.4237 3049.47,-72.3242\"/>\n</g>\n<!-- pool -->\n<g id=\"node35\" class=\"node\"><title>pool</title>\n<g id=\"a_node35\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.pooling.MaxPool2d&#39;&gt; {&#39;kernel_size&#39;: 4}\">\n<path fill=\"#8dd3c7\" stroke=\"black\" d=\"M3192,-93C3192,-93 3162,-93 3162,-93 3156,-93 3150,-87 3150,-81 3150,-81 3150,-69 3150,-69 3150,-63 3156,-57 3162,-57 3162,-57 3192,-57 3192,-57 3198,-57 3204,-63 3204,-69 3204,-69 3204,-81 3204,-81 3204,-87 3198,-93 3192,-93\"/>\n<text text-anchor=\"middle\" x=\"3177\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">pool</text>\n</a>\n</g>\n</g>\n<!-- layer3_residual_add&#45;&gt;pool -->\n<g id=\"edge37\" class=\"edge\"><title>layer3_residual_add&#45;&gt;pool</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3114.4,-75C3122.39,-75 3131.31,-75 3139.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3139.92,-78.5001 3149.92,-75 3139.92,-71.5001 3139.92,-78.5001\"/>\n</g>\n<!-- classifier_flatten -->\n<g id=\"node36\" class=\"node\"><title>classifier_flatten</title>\n<g id=\"a_node36\"><a xlink:title=\"&lt;class &#39;__main__.Flatten&#39;&gt; {}\">\n<path fill=\"#b3de69\" stroke=\"black\" d=\"M3282,-93C3282,-93 3252,-93 3252,-93 3246,-93 3240,-87 3240,-81 3240,-81 3240,-69 3240,-69 3240,-63 3246,-57 3252,-57 3252,-57 3282,-57 3282,-57 3288,-57 3294,-63 3294,-69 3294,-69 3294,-81 3294,-81 3294,-87 3288,-93 3282,-93\"/>\n<text text-anchor=\"middle\" x=\"3267\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">flatten</text>\n</a>\n</g>\n</g>\n<!-- pool&#45;&gt;classifier_flatten -->\n<g id=\"edge38\" class=\"edge\"><title>pool&#45;&gt;classifier_flatten</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3204.4,-75C3212.39,-75 3221.31,-75 3229.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3229.92,-78.5001 3239.92,-75 3229.92,-71.5001 3229.92,-78.5001\"/>\n</g>\n<!-- classifier_conv -->\n<g id=\"node37\" class=\"node\"><title>classifier_conv</title>\n<g id=\"a_node37\"><a xlink:title=\"&lt;class &#39;torch.nn.modules.linear.Linear&#39;&gt; {&#39;in_features&#39;: 512, &#39;out_features&#39;: 10, &#39;bias&#39;: False}\">\n<path fill=\"#fccde5\" stroke=\"black\" d=\"M3372,-93C3372,-93 3342,-93 3342,-93 3336,-93 3330,-87 3330,-81 3330,-81 3330,-69 3330,-69 3330,-63 3336,-57 3342,-57 3342,-57 3372,-57 3372,-57 3378,-57 3384,-63 3384,-69 3384,-69 3384,-81 3384,-81 3384,-87 3378,-93 3372,-93\"/>\n<text text-anchor=\"middle\" x=\"3357\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">conv</text>\n</a>\n</g>\n</g>\n<!-- classifier_flatten&#45;&gt;classifier_conv -->\n<g id=\"edge39\" class=\"edge\"><title>classifier_flatten&#45;&gt;classifier_conv</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3294.4,-75C3302.39,-75 3311.31,-75 3319.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3319.92,-78.5001 3329.92,-75 3319.92,-71.5001 3319.92,-78.5001\"/>\n</g>\n<!-- classifier_scale -->\n<g id=\"node38\" class=\"node\"><title>classifier_scale</title>\n<g id=\"a_node38\"><a xlink:title=\"&lt;class &#39;__main__.Mul&#39;&gt; {&#39;weight&#39;: 0.0625}\">\n<path fill=\"#bc80bd\" stroke=\"black\" d=\"M3462,-93C3462,-93 3432,-93 3432,-93 3426,-93 3420,-87 3420,-81 3420,-81 3420,-69 3420,-69 3420,-63 3426,-57 3432,-57 3432,-57 3462,-57 3462,-57 3468,-57 3474,-63 3474,-69 3474,-69 3474,-81 3474,-81 3474,-87 3468,-93 3462,-93\"/>\n<text text-anchor=\"middle\" x=\"3447\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">scale</text>\n</a>\n</g>\n</g>\n<!-- classifier_conv&#45;&gt;classifier_scale -->\n<g id=\"edge40\" class=\"edge\"><title>classifier_conv&#45;&gt;classifier_scale</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3384.4,-75C3392.39,-75 3401.31,-75 3409.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3409.92,-78.5001 3419.92,-75 3409.92,-71.5001 3409.92,-78.5001\"/>\n</g>\n<!-- logits -->\n<g id=\"node39\" class=\"node\"><title>logits</title>\n<g id=\"a_node39\"><a xlink:title=\"&lt;class &#39;__main__.Identity&#39;&gt; {}\">\n<path fill=\"#80b1d3\" stroke=\"black\" d=\"M3552,-93C3552,-93 3522,-93 3522,-93 3516,-93 3510,-87 3510,-81 3510,-81 3510,-69 3510,-69 3510,-63 3516,-57 3522,-57 3522,-57 3552,-57 3552,-57 3558,-57 3564,-63 3564,-69 3564,-69 3564,-81 3564,-81 3564,-87 3558,-93 3552,-93\"/>\n<text text-anchor=\"middle\" x=\"3537\" y=\"-71.3\" font-family=\"Times,serif\" font-size=\"14.00\">logits</text>\n</a>\n</g>\n</g>\n<!-- classifier_scale&#45;&gt;logits -->\n<g id=\"edge41\" class=\"edge\"><title>classifier_scale&#45;&gt;logits</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3474.4,-75C3482.39,-75 3491.31,-75 3499.82,-75\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3499.92,-78.5001 3509.92,-75 3499.92,-71.5001 3499.92,-78.5001\"/>\n</g>\n<!-- input -->\n<g id=\"node40\" class=\"node\"><title>input</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M42,-92C42,-92 12,-92 12,-92 6,-92 0,-86 0,-80 0,-80 0,-68 0,-68 0,-62 6,-56 12,-56 12,-56 42,-56 42,-56 48,-56 54,-62 54,-68 54,-68 54,-80 54,-80 54,-86 48,-92 42,-92\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-70.3\" font-family=\"Times,serif\" font-size=\"14.00\">input</text>\n</g>\n<!-- input&#45;&gt;prep_whiten -->\n<g id=\"edge1\" class=\"edge\"><title>input&#45;&gt;prep_whiten</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.4029,-74C62.3932,-74 71.3106,-74 79.8241,-74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"79.919,-77.5001 89.919,-74 79.919,-70.5001 79.919,-77.5001\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<__main__.DotGraph at 0x7f4140167cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def whitening_block(c_in, c_out, Î›=None, V=None, eps=1e-2):\n",
        "    filt = nn.Conv2d(3, 27, kernel_size=(3,3), padding=(1,1), bias=False)\n",
        "    filt.weight.data = (V/torch.sqrt(Î›+eps)[:,None,None,None])\n",
        "    filt.weight.requires_grad = False \n",
        "                                   \n",
        "    return {\n",
        "        'whiten': (identity, {'value': filt}),\n",
        "        'conv': conv(27, c_out, kernel_size=(1, 1), bias=False),\n",
        "        'norm': batch_norm(c_out), \n",
        "        'act':  relu(),\n",
        "    }\n",
        "\n",
        "input_whitening_net = network(conv_pool_block=conv_pool_block_pre, prep_block=partial(whitening_block, Î›=Î›, V=V), scale=1/16, types={\n",
        "    nn.ReLU: partial(nn.CELU, 0.3),\n",
        "    BatchNorm: partial(GhostBatchNorm, num_splits=16, weight=False)\n",
        "})\n",
        "show(input_whitening_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqAvZkzucX9W"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size = 17, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(8, 8))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.6, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "opt_params_bias = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 0.6*64, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(input_whitening_net, label_smoothing_loss(0.2))\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(is_bias[False], opt_params), SGD(is_bias[True], opt_params_bias)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfjLj-HORxWe"
      },
      "source": [
        "17 epoch test accuracy jumps to 94.4% allowing a further 2 epoch reduction in training time. 15 epochs brings a test accuracy of 94.1% in 39s, closing in on the 4-GPU, test-time-augmentation assisted DAWNBench leader! If we increase the maximum learning rate by a further ~50% and reduce the amount of cutout augmentation, from 8Ã—8 to 5Ã—5 patches, to compensate for the extra regularisation that the higher learning rate brings, we can remove a further epoch and reach 94.1% test accuracy in 36s, moving us narrowly into top spot on the leaderboard!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TouhGnnKIBn"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size = 14, 512\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(5, 5))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 1.0, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "opt_params_bias = {'lr': lr_schedule([0, epochs/5, epochs], [0.0, 1.0*64, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(input_whitening_net, label_smoothing_loss(0.2))\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    state, timer = {MODEL: model, OPTS: [SGD(is_bias[False], opt_params), SGD(is_bias[True], opt_params_bias)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz_9Q9MapvH7"
      },
      "source": [
        "### Exponential moving averages (33.5s)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD_F0A7fpzBb"
      },
      "source": [
        "High learning rates are necessary for rapid training since they allow stochastic gradient descent to traverse the necessary distances in parameter space in a limited amount of time. On the other hand, learning rates need to be annealed towards the end of training to enable optimisation along the steeper and noisier directions in parameter space. Parameter averaging methods allow training to continue at a higher rate whilst potentially approaching minima along noisy or oscillatory directions by averaging over multiple iterates. \n",
        "\n",
        "We shall investigate exponential moving averaging of parameters which is a standard approach. For efficiency reasons we update the moving average every 5 batches since we find that more frequent updates don't improve things. We need to choose a new learning rate schedule with higher learning rates towards the end of training, and a momentum for the moving average. For the learning rate, a simple choice is to stick with the piecewise linear schedule that we've been using throughout, floored at a low fixed value for the last 2 epochs and we choose a momentum of 0.99 so that averaging takes place over a timescale of roughly the last epoch.\n",
        "\n",
        "Test accuracy improves to 94.3% allowing us to trim a further epoch. 13 epoch training reaches a test accuracy of 94.1%, achieving a training time below 34s and a 10Ã— improvement over the single-GPU state-of-the-art at the outset of the series!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jef-6wWUcpH"
      },
      "outputs": [],
      "source": [
        "#Final training setup\n",
        "epochs, batch_size, ema_epochs=13, 512, 2\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(5, 5))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs - ema_epochs], [0.0, 1.0, 0.1], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "opt_params_bias = {'lr': lr_schedule([0, epochs/5, epochs - ema_epochs], [0.0, 1.0*64, 0.1*64], batch_size), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(input_whitening_net, label_smoothing_loss(0.2))\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    state, timer = {MODEL: model, VALID_MODEL: copy.deepcopy(model), OPTS: [SGD(is_bias[False], opt_params), SGD(is_bias[True], opt_params_bias)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size), train_steps=(*train_steps, update_ema(momentum=0.99, update_freq=5)))))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv-Q0jUA-oHi"
      },
      "source": [
        "### Test-time augmentation (26s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-2DG-xvVr2g"
      },
      "source": [
        "Suppose that you'd like your network to classify images the same way under horizontal flips of the input. One possibility, that we've been using till now, is to present the network with a large amount of data, possibly augmented by label preserving left-right flips, and hope that the network will eventually learn the invariance through extensive training. \n",
        "\n",
        "A second approach, which doesn't leave things to chance, is to present both the input image and its horizontally flipped version and come to a consensus by averaging network outputs for the two versions, thus guaranteeing invariance. This eminently sensible approach goes by the name of test-time augmentation.\n",
        "\n",
        "At training time, we still present the network with a single version of each image - potentially subject to random flipping as a data augmentation so that different versions are presented on different training epochs. An alternative, would be to use the same procedure at training time as at test time and present each image along with its mirror. In this case, we could claim to have changed the network by splitting into two identical branches, one of which sees the flipped image, and then merging at the end. Through this lens, the original training can be viewed as a stochastic training procedure for a weight-tied, two branch network in which a single branch is 'dropped-out' for each training example.\n",
        "\n",
        "This dropout-training viewpoint makes it clear that any attempt to introduce a rule disallowing TTA from a benchmark is going to be fraught with difficulties. From this point of view, we have just introduced a larger network for which we have an efficient stochastic training methodology. On the other hand, if we don't limit the amount of work that we are prepared to do at test time then there are some obvious degenerate solutions in which training takes as little time as is required to store the dataset!\n",
        "\n",
        "These arguments are not only relevant to artificial benchmarks but also to end use-cases. In some applications, classification accuracy is all that is desired and in that case TTA should most definitely be used. In other cases, inference time is also a constraint and a sensible approach is to maximise accuracy subject to such constraints. This is probably a good approach for training benchmarks too. \n",
        "\n",
        "In the case at hand, the Kakao Brain team has applied the simple form of TTA described here - presenting an image and its left-right mirror at inference time, thus doubling the computational load. More extensive forms of TTA are of course possible for other symmetries (such as translational symmetry, variations in brightness/colour etc.) but these would come at a higher computational cost. \n",
        "\n",
        "Now because these entries are based of a computationally light 9-layer ResNet _total inference time including TTA_ is likely to be much lower for these entries than for some of the 100+ layer networks that have been entered at earlier stages of the competition! According to our discussion above, any reasonable rule to limit this kind of approach should be based on inference time constraints and not an arbitrary feature of the implementation and so from this point-of-view, we should accept the approach.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GcFJZJeg__W"
      },
      "source": [
        "Let's see what improvement TTA brings. We shall restrict ourselves to horizontal flip TTA for consistency with the current DAWNBench submissions and because this seems a sweet spot between accuracy and inference cost. With our current network and 13 epoch training setup, the test accuracy with TTA rises to 94.6%, making this the largest individual effect we've studied today. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjS1RJ11oLef"
      },
      "outputs": [],
      "source": [
        "valid_steps_tta = (forward_tta([identity, flip_lr]), log_activations(('loss', 'acc')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3r9fcUEnlk_"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size, ema_epochs=13, 512, 2\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(5, 5))\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs - ema_epochs], [0.0, 1.0, 0.1], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "opt_params_bias = {'lr': lr_schedule([0, epochs/5, epochs - ema_epochs], [0.0, 1.0*64, 0.1*64], batch_size), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(input_whitening_net, label_smoothing_loss(0.2))\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    state, timer = {MODEL: model, VALID_MODEL: copy.deepcopy(model), OPTS: [SGD(is_bias[False], opt_params), SGD(is_bias[True], opt_params_bias)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size), \n",
        "                                                                        train_steps=(*train_steps, update_ema(momentum=0.99, update_freq=5)),\n",
        "                                                                        valid_steps=valid_steps_tta)))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH4hq224nqQh"
      },
      "source": [
        "If we remove the remaining cutout data augmentation - which is getting in the way on such a short training schedule - we can reduce training to 10 epochs (!) and achieve a TTA test accuracy of 94.1% in 26s!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oewy20lcoBVK"
      },
      "outputs": [],
      "source": [
        "epochs, batch_size, ema_epochs=10, 512, 2\n",
        "transforms = (Crop(32, 32), FlipLR())\n",
        "opt_params = {'lr': lr_schedule([0, epochs/5, epochs - ema_epochs], [0.0, 1.0, 0.1], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "opt_params_bias = {'lr': lr_schedule([0, epochs/5, epochs - ema_epochs], [0.0, 1.0*64, 0.1*64], batch_size), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)}\n",
        "\n",
        "logs = Table(report=every(epochs,'epoch'))\n",
        "for run in range(N_RUNS):\n",
        "    model = build_model(input_whitening_net, label_smoothing_loss(0.2))\n",
        "    is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "    state, timer = {MODEL: model, VALID_MODEL: copy.deepcopy(model), OPTS: [SGD(is_bias[False], opt_params), SGD(is_bias[True], opt_params_bias)]}, Timer(torch.cuda.synchronize)\n",
        "    for epoch in range(epochs):\n",
        "        logs.append(union({'run': run+1, 'epoch': epoch+1}, train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size), \n",
        "                                                                        train_steps=(*train_steps, update_ema(momentum=0.99, update_freq=5)),\n",
        "                                                                        valid_steps=valid_steps_tta)))\n",
        "summary(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD7-fE8Xoq_r"
      },
      "source": [
        "### Training to convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO8eWpWOdFS-"
      },
      "source": [
        "Here is a simple experiment to investigate whether the gains in training speed that we have collected also translate into gains in final accuracy for the model if it is trained to convergence. We have every reason to believe that this should be the case, if only because many of the techniques that we have been using today were originally proposed as techniques to improve converged accuracy on ImageNet! If it is the case that the same techniques which speed up training time to 94% accuracy on CIFAR10 also improve converged accuracy on ImageNet, then this suggests a rather effective way to accelerate research on the latter problem!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9G4gjGueEAZ"
      },
      "source": [
        "Unlike the previous experiments, this is going to be very rough and ready and we leave it to future work to do this experiment more carefully. We are going to pick a fixed learning rate schedule with lower learning rates appropriate for longer training and increase the amount of cutout augmentation to 12Ã—12 patches to allow training for longer without overfitting. We will fix the other hyperparameters as they were above and train both the baseline network and the final network for a range of different times from 24 to 100 epochs. Finally we're going to break all the rules and only run each experiment 5 times! Here are the results: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HShJN2xh22e"
      },
      "source": [
        "#### Convergence experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2LQrVTMgTSq"
      },
      "outputs": [],
      "source": [
        "valid_steps_tta = (forward_tta([identity, flip_lr]), log_activations(('loss', 'acc')))\n",
        "\n",
        "def train_epoch_tta(state, timer, train_batches, valid_batches, train_steps=train_steps, \n",
        "                    valid_steps=valid_steps, valid_steps_tta=valid_steps_tta, on_epoch_end=identity):\n",
        "    train_summary, train_time = epoch_stats(on_epoch_end(reduce(train_batches, state, train_steps))), timer()\n",
        "    valid_summary, valid_time = epoch_stats(reduce(valid_batches, state, valid_steps)), timer(update_total=False) #DAWNBench rules\n",
        "    valid_summary_tta, valid_time_tta = epoch_stats(reduce(valid_batches, state, valid_steps_tta)), timer(update_total=False) #DAWNBench rules\n",
        "    return {\n",
        "        'train': union({'time': train_time}, train_summary), \n",
        "        'valid': union({'time': valid_time}, valid_summary), \n",
        "        'tta': union({'time': valid_time}, valid_summary_tta), \n",
        "        'total time': timer.total_time\n",
        "    }\n",
        "\n",
        "#baseline model\n",
        "transforms = (Crop(32, 32), FlipLR(), Cutout(12, 12))\n",
        "logs = Table()\n",
        "for run in range(5):\n",
        "    for epochs in [24, 40, 60, 80, 100]:\n",
        "        opt_params = {'lr': lr_schedule([0, 5, epochs], [0.0, 0.4, 0.0], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "\n",
        "        model = build_model(network(), x_ent_loss)\n",
        "        state, timer = {MODEL: model, OPTS: [SGD(trainable_params(model).values(), opt_params)]}, Timer(torch.cuda.synchronize)\n",
        "        for epoch in range(epochs-1):\n",
        "            train_epoch(state, timer, train_batches(batch_size, transforms),  valid_batches(batch_size)) \n",
        "        logs.append(union({'run': run+1, 'epoch': epochs, 'experiment': baseline}, \n",
        "                          train_epoch_tta(state, timer, train_batches(batch_size, transforms),  valid_batches(batch_size), \n",
        "                                          valid_steps_tta=valid_steps_tta)))   \n",
        "#final model\n",
        "ema_epochs=2\n",
        "for run in range(5):\n",
        "    for epochs in [24, 40, 60, 80, 100]:\n",
        "        opt_params = {'lr': lr_schedule([0, epochs/5, epochs - ema_epochs], [0.0, 0.4, 0.04], batch_size), 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)}\n",
        "        opt_params_bias = {'lr': lr_schedule([0, epochs/5, epochs - ema_epochs], [0.0, 0.4*64, 0.04\n",
        "                                                                          *64], batch_size), 'weight_decay': Const(5e-4*batch_size/64), 'momentum': Const(0.9)}\n",
        "\n",
        "        model = build_model(input_whitening_net, label_smoothing_loss(0.2))\n",
        "        is_bias = group_by_key(('bias' in k, v) for k, v in trainable_params(model).items())\n",
        "        state, timer = {MODEL: model, VALID_MODEL: copy.deepcopy(model), OPTS: [SGD(is_bias[False], opt_params), SGD(is_bias[True], opt_params_bias)]}, Timer(torch.cuda.synchronize)\n",
        "        for epoch in range(epochs-1):\n",
        "            train_epoch(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size), \n",
        "                                                                            train_steps=(*train_steps, update_ema(momentum=0.99, update_freq=5)))\n",
        "        logs.append(union({'run': run+1, 'epoch': epochs, 'experiment': final}, \n",
        "                          train_epoch_tta(state, timer, train_batches(batch_size, transforms), valid_batches(batch_size), \n",
        "                                          train_steps=(*train_steps, update_ema(momentum=0.99, update_freq=5)))))  \n",
        "        \n",
        "data = logs.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI_eqtvSh6Ng"
      },
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "t-5yQ-E3g-xt",
        "outputId": "ce602892-b006-466a-e743-d6ce2314a463"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega@5\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-lite@3.4.0\"></script>\n",
              "  <script type=\"text/javascript\" src=\"https://cdn.jsdelivr.net/npm//vega-embed@4\"></script>\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"altair-viz\"></div>\n",
              "  <script>\n",
              "    (function(vegaEmbed) {\n",
              "      var spec = {\"config\": {\"view\": {\"width\": 400, \"height\": 300}, \"mark\": {\"tooltip\": null}}, \"hconcat\": [{\"layer\": [{\"mark\": \"point\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"experiment\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\", \"scale\": {\"zero\": false}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"valid acc\"}, \"field\": \"valid_acc\", \"scale\": {\"zero\": false}}}}, {\"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"experiment\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\", \"scale\": {\"zero\": false}}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"valid_acc\", \"scale\": {\"zero\": false}}}}]}, {\"layer\": [{\"mark\": \"point\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"experiment\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\", \"scale\": {\"zero\": false}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"valid acc (tta)\"}, \"field\": \"tta_acc\", \"scale\": {\"zero\": false}}}}, {\"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"experiment\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\", \"scale\": {\"zero\": false}}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"tta_acc\", \"scale\": {\"zero\": false}}}}]}], \"data\": {\"name\": \"data-14c72c6d62a735a120867cb9d5a4c575\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v3.4.0.json\", \"datasets\": {\"data-14c72c6d62a735a120867cb9d5a4c575\": [{\"epoch\": 40, \"run\": 1, \"total time\": 115.77539189700474, \"train_acc\": 0.9724750322164949, \"train_loss\": 0.08945317443498631, \"train_time\": 2.8944502609992924, \"tta_acc\": 0.9522, \"tta_loss\": 0.14731700954437255, \"tta_time\": 0.191972672000702, \"valid_acc\": 0.9454, \"valid_loss\": 0.1637589175224304, \"valid_time\": 0.191972672000702, \"experiment\": \"baseline\"}, {\"epoch\": 60, \"run\": 1, \"total time\": 173.7991279079979, \"train_acc\": 0.9833279639175257, \"train_loss\": 0.05970388147787949, \"train_time\": 2.892037823999999, \"tta_acc\": 0.9561, \"tta_loss\": 0.13223285465240478, \"tta_time\": 0.191634595998039, \"valid_acc\": 0.9526, \"valid_loss\": 0.14667956314086913, \"valid_time\": 0.191634595998039, \"experiment\": \"baseline\"}, {\"epoch\": 80, \"run\": 1, \"total time\": 231.56872692101751, \"train_acc\": 0.9870127255154639, \"train_loss\": 0.04798114148074204, \"train_time\": 2.8922985280005378, \"tta_acc\": 0.9575, \"tta_loss\": 0.13690186977386476, \"tta_time\": 0.1917704199986474, \"valid_acc\": 0.9522, \"valid_loss\": 0.1517398440361023, \"valid_time\": 0.1917704199986474, \"experiment\": \"baseline\"}, {\"epoch\": 100, \"run\": 1, \"total time\": 289.0958471929989, \"train_acc\": 0.9906974871134021, \"train_loss\": 0.03765406883945785, \"train_time\": 2.886209833999601, \"tta_acc\": 0.957, \"tta_loss\": 0.13574402532577515, \"tta_time\": 0.1920333970010688, \"valid_acc\": 0.9535, \"valid_loss\": 0.14849681997299194, \"valid_time\": 0.1920333970010688, \"experiment\": \"baseline\"}, {\"epoch\": 40, \"run\": 2, \"total time\": 115.93349109901101, \"train_acc\": 0.973401256443299, \"train_loss\": 0.08912419904138624, \"train_time\": 2.893640766000317, \"tta_acc\": 0.9515, \"tta_loss\": 0.1469169704437256, \"tta_time\": 0.19260841499635717, \"valid_acc\": 0.9459, \"valid_loss\": 0.161656810092926, \"valid_time\": 0.19260841499635717, \"experiment\": \"baseline\"}, {\"epoch\": 60, \"run\": 2, \"total time\": 173.81213349894824, \"train_acc\": 0.9840125644329897, \"train_loss\": 0.05957732916108726, \"train_time\": 2.892742789001204, \"tta_acc\": 0.9551, \"tta_loss\": 0.13827357215881347, \"tta_time\": 0.19220788899838226, \"valid_acc\": 0.9498, \"valid_loss\": 0.15355262699127198, \"valid_time\": 0.19220788899838226, \"experiment\": \"baseline\"}, {\"epoch\": 80, \"run\": 2, \"total time\": 231.54911988700042, \"train_acc\": 0.98828125, \"train_loss\": 0.04477034353641505, \"train_time\": 2.891720998995879, \"tta_acc\": 0.9562, \"tta_loss\": 0.1326014790534973, \"tta_time\": 0.19183066400000826, \"valid_acc\": 0.9508, \"valid_loss\": 0.15004254236221312, \"valid_time\": 0.19183066400000826, \"experiment\": \"baseline\"}, {\"epoch\": 100, \"run\": 2, \"total time\": 289.1528977750204, \"train_acc\": 0.990576675257732, \"train_loss\": 0.03862031384073582, \"train_time\": 2.885488786996575, \"tta_acc\": 0.9563, \"tta_loss\": 0.1359883903503418, \"tta_time\": 0.1906894510029815, \"valid_acc\": 0.9539, \"valid_loss\": 0.1490915186882019, \"valid_time\": 0.1906894510029815, \"experiment\": \"baseline\"}, {\"epoch\": 40, \"run\": 3, \"total time\": 115.94290988198918, \"train_acc\": 0.9740455863402062, \"train_loss\": 0.08699826053199694, \"train_time\": 2.8944724870016216, \"tta_acc\": 0.9513, \"tta_loss\": 0.14736389656066895, \"tta_time\": 0.19228573099826463, \"valid_acc\": 0.9476, \"valid_loss\": 0.16150463714599608, \"valid_time\": 0.19228573099826463, \"experiment\": \"baseline\"}, {\"epoch\": 60, \"run\": 3, \"total time\": 173.79642055099248, \"train_acc\": 0.9826634987113402, \"train_loss\": 0.060447668176643626, \"train_time\": 2.8918151920006494, \"tta_acc\": 0.9554, \"tta_loss\": 0.13378720598220825, \"tta_time\": 0.19201715199596947, \"valid_acc\": 0.9523, \"valid_loss\": 0.14677832469940186, \"valid_time\": 0.19201715199596947, \"experiment\": \"baseline\"}, {\"epoch\": 80, \"run\": 3, \"total time\": 231.33222012402257, \"train_acc\": 0.9876167847938144, \"train_loss\": 0.04650055932015488, \"train_time\": 2.8850283520005178, \"tta_acc\": 0.9571, \"tta_loss\": 0.1345288269996643, \"tta_time\": 0.19103913399885641, \"valid_acc\": 0.9514, \"valid_loss\": 0.15048525075912475, \"valid_time\": 0.19103913399885641, \"experiment\": \"baseline\"}, {\"epoch\": 100, \"run\": 3, \"total time\": 289.1790519290007, \"train_acc\": 0.9903753221649485, \"train_loss\": 0.03815095111266854, \"train_time\": 2.886819564999314, \"tta_acc\": 0.9571, \"tta_loss\": 0.13442925338745118, \"tta_time\": 0.19067772800190141, \"valid_acc\": 0.9519, \"valid_loss\": 0.15329569854736327, \"valid_time\": 0.19067772800190141, \"experiment\": \"baseline\"}, {\"epoch\": 40, \"run\": 4, \"total time\": 115.98809706900647, \"train_acc\": 0.974206668814433, \"train_loss\": 0.08665592160037498, \"train_time\": 2.898000020002655, \"tta_acc\": 0.9534, \"tta_loss\": 0.14771631813049316, \"tta_time\": 0.1922089979998418, \"valid_acc\": 0.949, \"valid_loss\": 0.16354228534698487, \"valid_time\": 0.1922089979998418, \"experiment\": \"baseline\"}, {\"epoch\": 60, \"run\": 4, \"total time\": 173.8017946649852, \"train_acc\": 0.9843145940721649, \"train_loss\": 0.05849805401311707, \"train_time\": 2.8925928979952005, \"tta_acc\": 0.9566, \"tta_loss\": 0.1356637414932251, \"tta_time\": 0.19205911600147374, \"valid_acc\": 0.9513, \"valid_loss\": 0.1493300030708313, \"valid_time\": 0.19205911600147374, \"experiment\": \"baseline\"}, {\"epoch\": 80, \"run\": 4, \"total time\": 231.53127032201883, \"train_acc\": 0.9885430090206185, \"train_loss\": 0.04646093713254044, \"train_time\": 2.8897476560014184, \"tta_acc\": 0.9569, \"tta_loss\": 0.1331675503730774, \"tta_time\": 0.19233000699750846, \"valid_acc\": 0.9516, \"valid_loss\": 0.15248397092819213, \"valid_time\": 0.19233000699750846, \"experiment\": \"baseline\"}, {\"epoch\": 100, \"run\": 4, \"total time\": 289.1731662430029, \"train_acc\": 0.9904558634020618, \"train_loss\": 0.039665708952036104, \"train_time\": 2.887360422006168, \"tta_acc\": 0.9576, \"tta_loss\": 0.13489235219955445, \"tta_time\": 0.19174574600037886, \"valid_acc\": 0.9536, \"valid_loss\": 0.1465499900817871, \"valid_time\": 0.19174574600037886, \"experiment\": \"baseline\"}, {\"epoch\": 40, \"run\": 5, \"total time\": 115.90960084103426, \"train_acc\": 0.9747905927835051, \"train_loss\": 0.08495002355157714, \"train_time\": 2.8942276980014867, \"tta_acc\": 0.9505, \"tta_loss\": 0.14900790967941285, \"tta_time\": 0.19179014800465666, \"valid_acc\": 0.9475, \"valid_loss\": 0.1620381199836731, \"valid_time\": 0.19179014800465666, \"experiment\": \"baseline\"}, {\"epoch\": 60, \"run\": 5, \"total time\": 173.73591956799646, \"train_acc\": 0.9835293170103093, \"train_loss\": 0.0589248721593434, \"train_time\": 2.8926016470068134, \"tta_acc\": 0.9554, \"tta_loss\": 0.1400909294128418, \"tta_time\": 0.19176351399801206, \"valid_acc\": 0.9514, \"valid_loss\": 0.1536110694885254, \"valid_time\": 0.19176351399801206, \"experiment\": \"baseline\"}, {\"epoch\": 80, \"run\": 5, \"total time\": 231.572428033076, \"train_acc\": 0.9887040914948454, \"train_loss\": 0.045570830766534065, \"train_time\": 2.891090276003524, \"tta_acc\": 0.9574, \"tta_loss\": 0.1329388027191162, \"tta_time\": 0.19153811800060794, \"valid_acc\": 0.9542, \"valid_loss\": 0.14818038806915282, \"valid_time\": 0.19153811800060794, \"experiment\": \"baseline\"}, {\"epoch\": 100, \"run\": 5, \"total time\": 289.2042186130275, \"train_acc\": 0.9903753221649485, \"train_loss\": 0.03801192454609674, \"train_time\": 2.8860927780042402, \"tta_acc\": 0.9573, \"tta_loss\": 0.1373170760154724, \"tta_time\": 0.19177757999568712, \"valid_acc\": 0.9522, \"valid_loss\": 0.15495181436538696, \"valid_time\": 0.19177757999568712, \"experiment\": \"baseline\"}, {\"epoch\": 24, \"run\": 1, \"total time\": 69.47972413498792, \"train_acc\": 0.9537290592783505, \"train_loss\": 0.14411357749906398, \"train_time\": 2.897183038003277, \"tta_acc\": 0.9469, \"tta_loss\": 0.1606788829803467, \"tta_time\": 0.1921295049978653, \"valid_acc\": 0.9401, \"valid_loss\": 0.17633453397750853, \"valid_time\": 0.1921295049978653, \"experiment\": \"baseline\"}, {\"epoch\": 24, \"run\": 2, \"total time\": 69.62557950800692, \"train_acc\": 0.953870006443299, \"train_loss\": 0.14425174651917108, \"train_time\": 2.8987395690055564, \"tta_acc\": 0.9435, \"tta_loss\": 0.16524562730789186, \"tta_time\": 0.19240747199364705, \"valid_acc\": 0.9397, \"valid_loss\": 0.18035643215179442, \"valid_time\": 0.19240747199364705, \"experiment\": \"baseline\"}, {\"epoch\": 24, \"run\": 3, \"total time\": 69.65048073300568, \"train_acc\": 0.9536887886597938, \"train_loss\": 0.14596862908567965, \"train_time\": 2.90208780499961, \"tta_acc\": 0.9424, \"tta_loss\": 0.16533652591705322, \"tta_time\": 0.1919627120005316, \"valid_acc\": 0.9384, \"valid_loss\": 0.1796941129684448, \"valid_time\": 0.1919627120005316, \"experiment\": \"baseline\"}, {\"epoch\": 24, \"run\": 4, \"total time\": 69.65110485800687, \"train_acc\": 0.9530645940721649, \"train_loss\": 0.1461100154124277, \"train_time\": 2.9011953369990806, \"tta_acc\": 0.9432, \"tta_loss\": 0.17059293451309204, \"tta_time\": 0.19200909399660304, \"valid_acc\": 0.9396, \"valid_loss\": 0.18514128446578978, \"valid_time\": 0.19200909399660304, \"experiment\": \"baseline\"}, {\"epoch\": 24, \"run\": 5, \"total time\": 69.64399432497885, \"train_acc\": 0.9548163659793815, \"train_loss\": 0.1410928838646289, \"train_time\": 2.900024989998201, \"tta_acc\": 0.9445, \"tta_loss\": 0.16654890432357788, \"tta_time\": 0.19193982199794846, \"valid_acc\": 0.9384, \"valid_loss\": 0.1841355944633484, \"valid_time\": 0.19193982199794846, \"experiment\": \"baseline\"}, {\"epoch\": 40, \"run\": 1, \"total time\": 102.84597889299039, \"train_acc\": 0.9693741945876289, \"train_loss\": 0.9459850665220281, \"train_time\": 2.5670782279994455, \"tta_acc\": 0.9579, \"tta_loss\": 0.9583810546875, \"tta_time\": 0.181817176999175, \"valid_acc\": 0.9538, \"valid_loss\": 0.96737177734375, \"valid_time\": 0.181817176999175, \"experiment\": \"final\"}, {\"epoch\": 60, \"run\": 1, \"total time\": 154.31354360000114, \"train_acc\": 0.9786767074742269, \"train_loss\": 0.928253222986595, \"train_time\": 2.57236493799428, \"tta_acc\": 0.9587, \"tta_loss\": 0.951898486328125, \"tta_time\": 0.1827594560018042, \"valid_acc\": 0.9546, \"valid_loss\": 0.96043193359375, \"valid_time\": 0.1827594560018042, \"experiment\": \"final\"}, {\"epoch\": 80, \"run\": 1, \"total time\": 205.7615784409645, \"train_acc\": 0.9815157860824743, \"train_loss\": 0.9220649384960686, \"train_time\": 2.569626550997782, \"tta_acc\": 0.9609, \"tta_loss\": 0.949437255859375, \"tta_time\": 0.18310099600057583, \"valid_acc\": 0.9568, \"valid_loss\": 0.95888212890625, \"valid_time\": 0.18310099600057583, \"experiment\": \"final\"}, {\"epoch\": 100, \"run\": 1, \"total time\": 257.2018751499709, \"train_acc\": 0.9832675579896907, \"train_loss\": 0.9179677373355197, \"train_time\": 2.5681756339981803, \"tta_acc\": 0.9601, \"tta_loss\": 0.947637109375, \"tta_time\": 0.18255041600059485, \"valid_acc\": 0.9575, \"valid_loss\": 0.956150048828125, \"valid_time\": 0.18255041600059485, \"experiment\": \"final\"}, {\"epoch\": 40, \"run\": 2, \"total time\": 102.95762568798818, \"train_acc\": 0.9720521907216495, \"train_loss\": 0.9434325228032377, \"train_time\": 2.577212516000145, \"tta_acc\": 0.958, \"tta_loss\": 0.957646435546875, \"tta_time\": 0.18276126800628845, \"valid_acc\": 0.954, \"valid_loss\": 0.965418359375, \"valid_time\": 0.18276126800628845, \"experiment\": \"final\"}, {\"epoch\": 60, \"run\": 2, \"total time\": 154.3870944369919, \"train_acc\": 0.9785357603092784, \"train_loss\": 0.9288256733687883, \"train_time\": 2.5691759129986167, \"tta_acc\": 0.9574, \"tta_loss\": 0.953997314453125, \"tta_time\": 0.18221270800131606, \"valid_acc\": 0.9556, \"valid_loss\": 0.962770068359375, \"valid_time\": 0.18221270800131606, \"experiment\": \"final\"}, {\"epoch\": 80, \"run\": 2, \"total time\": 205.821977417967, \"train_acc\": 0.9823413337628866, \"train_loss\": 0.9203307948161646, \"train_time\": 2.5703018869971856, \"tta_acc\": 0.9611, \"tta_loss\": 0.950356884765625, \"tta_time\": 0.18253108699718723, \"valid_acc\": 0.9566, \"valid_loss\": 0.958939599609375, \"valid_time\": 0.18253108699718723, \"experiment\": \"final\"}, {\"epoch\": 100, \"run\": 2, \"total time\": 257.2657502310103, \"train_acc\": 0.9837910760309279, \"train_loss\": 0.9158867904820394, \"train_time\": 2.568709031002072, \"tta_acc\": 0.9623, \"tta_loss\": 0.945064013671875, \"tta_time\": 0.182770311999775, \"valid_acc\": 0.9593, \"valid_loss\": 0.955058447265625, \"valid_time\": 0.182770311999775, \"experiment\": \"final\"}, {\"epoch\": 40, \"run\": 3, \"total time\": 102.91727831303433, \"train_acc\": 0.9696560889175257, \"train_loss\": 0.9443699256661012, \"train_time\": 2.5694828940031584, \"tta_acc\": 0.9568, \"tta_loss\": 0.958073681640625, \"tta_time\": 0.18202030599786667, \"valid_acc\": 0.9535, \"valid_loss\": 0.966338623046875, \"valid_time\": 0.18202030599786667, \"experiment\": \"final\"}, {\"epoch\": 60, \"run\": 3, \"total time\": 154.38148195800022, \"train_acc\": 0.9781934600515464, \"train_loss\": 0.9290023587413669, \"train_time\": 2.572302505999687, \"tta_acc\": 0.9601, \"tta_loss\": 0.952961572265625, \"tta_time\": 0.18237990399939008, \"valid_acc\": 0.9547, \"valid_loss\": 0.962862646484375, \"valid_time\": 0.18237990399939008, \"experiment\": \"final\"}, {\"epoch\": 80, \"run\": 3, \"total time\": 205.85524166001414, \"train_acc\": 0.9814352448453608, \"train_loss\": 0.9220564045856908, \"train_time\": 2.5693484250004985, \"tta_acc\": 0.96, \"tta_loss\": 0.9477572265625, \"tta_time\": 0.18291645099816378, \"valid_acc\": 0.9566, \"valid_loss\": 0.956994482421875, \"valid_time\": 0.18291645099816378, \"experiment\": \"final\"}, {\"epoch\": 100, \"run\": 3, \"total time\": 257.400613299993, \"train_acc\": 0.9848179768041238, \"train_loss\": 0.9156462285936493, \"train_time\": 2.571818030999566, \"tta_acc\": 0.9618, \"tta_loss\": 0.947703662109375, \"tta_time\": 0.18251981300272746, \"valid_acc\": 0.9584, \"valid_loss\": 0.95703037109375, \"valid_time\": 0.18251981300272746, \"experiment\": \"final\"}, {\"epoch\": 40, \"run\": 4, \"total time\": 102.93862039600208, \"train_acc\": 0.9711259664948454, \"train_loss\": 0.9436643639790643, \"train_time\": 2.5738141760011786, \"tta_acc\": 0.9581, \"tta_loss\": 0.957923681640625, \"tta_time\": 0.18292383100197185, \"valid_acc\": 0.9539, \"valid_loss\": 0.96743447265625, \"valid_time\": 0.18292383100197185, \"experiment\": \"final\"}, {\"epoch\": 60, \"run\": 4, \"total time\": 154.41663919502753, \"train_acc\": 0.9784954896907216, \"train_loss\": 0.9284879841755346, \"train_time\": 2.5739044319998357, \"tta_acc\": 0.9602, \"tta_loss\": 0.95122548828125, \"tta_time\": 0.18250545299815712, \"valid_acc\": 0.9561, \"valid_loss\": 0.96009091796875, \"valid_time\": 0.18250545299815712, \"experiment\": \"final\"}, {\"epoch\": 80, \"run\": 4, \"total time\": 205.85838152302313, \"train_acc\": 0.9814352448453608, \"train_loss\": 0.9217412496350476, \"train_time\": 2.572255806000612, \"tta_acc\": 0.9619, \"tta_loss\": 0.947907763671875, \"tta_time\": 0.18321384899900295, \"valid_acc\": 0.9578, \"valid_loss\": 0.95641865234375, \"valid_time\": 0.18321384899900295, \"experiment\": \"final\"}, {\"epoch\": 100, \"run\": 4, \"total time\": 257.2927283469835, \"train_acc\": 0.9829655283505154, \"train_loss\": 0.9180972797354472, \"train_time\": 2.5725200929955463, \"tta_acc\": 0.9601, \"tta_loss\": 0.946186474609375, \"tta_time\": 0.18265706300007878, \"valid_acc\": 0.9571, \"valid_loss\": 0.95430537109375, \"valid_time\": 0.18265706300007878, \"experiment\": \"final\"}, {\"epoch\": 40, \"run\": 5, \"total time\": 102.95283322800242, \"train_acc\": 0.9719313788659794, \"train_loss\": 0.9427000517697678, \"train_time\": 2.5722945279994747, \"tta_acc\": 0.957, \"tta_loss\": 0.958058056640625, \"tta_time\": 0.18345675599994138, \"valid_acc\": 0.9541, \"valid_loss\": 0.9679720703125, \"valid_time\": 0.18345675599994138, \"experiment\": \"final\"}, {\"epoch\": 60, \"run\": 5, \"total time\": 154.39701501696254, \"train_acc\": 0.9778108891752577, \"train_loss\": 0.9301550560390827, \"train_time\": 2.57575754699792, \"tta_acc\": 0.9584, \"tta_loss\": 0.9511826171875, \"tta_time\": 0.18216746600228362, \"valid_acc\": 0.9553, \"valid_loss\": 0.96147744140625, \"valid_time\": 0.18216746600228362, \"experiment\": \"final\"}, {\"epoch\": 80, \"run\": 5, \"total time\": 205.8281666639814, \"train_acc\": 0.9823211984536082, \"train_loss\": 0.9212127567566547, \"train_time\": 2.570514618004381, \"tta_acc\": 0.9624, \"tta_loss\": 0.947673046875, \"tta_time\": 0.18234977099928074, \"valid_acc\": 0.958, \"valid_loss\": 0.955671728515625, \"valid_time\": 0.18234977099928074, \"experiment\": \"final\"}, {\"epoch\": 100, \"run\": 5, \"total time\": 257.22457723898697, \"train_acc\": 0.9840729703608248, \"train_loss\": 0.9173678270320302, \"train_time\": 2.5702690160032944, \"tta_acc\": 0.9618, \"tta_loss\": 0.946293212890625, \"tta_time\": 0.18217301399999997, \"valid_acc\": 0.9569, \"valid_loss\": 0.95438818359375, \"valid_time\": 0.18217301399999997, \"experiment\": \"final\"}, {\"epoch\": 24, \"run\": 1, \"total time\": 61.77687859902653, \"train_acc\": 0.9563667847938144, \"train_loss\": 0.9728790656807497, \"train_time\": 2.5735945090054884, \"tta_acc\": 0.9521, \"tta_loss\": 0.973640478515625, \"tta_time\": 0.18347028499556473, \"valid_acc\": 0.9473, \"valid_loss\": 0.982691748046875, \"valid_time\": 0.18347028499556473, \"experiment\": \"final\"}, {\"epoch\": 24, \"run\": 2, \"total time\": 61.786338885976875, \"train_acc\": 0.953990818298969, \"train_loss\": 0.9761515450231808, \"train_time\": 2.572059215999616, \"tta_acc\": 0.9539, \"tta_loss\": 0.97068408203125, \"tta_time\": 0.18265191599493846, \"valid_acc\": 0.9471, \"valid_loss\": 0.9804400390625, \"valid_time\": 0.18265191599493846, \"experiment\": \"final\"}, {\"epoch\": 24, \"run\": 3, \"total time\": 61.781463586994505, \"train_acc\": 0.9569507087628866, \"train_loss\": 0.9721518742669489, \"train_time\": 2.5763632460002555, \"tta_acc\": 0.9506, \"tta_loss\": 0.971709228515625, \"tta_time\": 0.18320887300069444, \"valid_acc\": 0.9482, \"valid_loss\": 0.9808876953125, \"valid_time\": 0.18320887300069444, \"experiment\": \"final\"}, {\"epoch\": 24, \"run\": 4, \"total time\": 61.80873339098616, \"train_acc\": 0.9562661082474226, \"train_loss\": 0.9727197332480519, \"train_time\": 2.573691127996426, \"tta_acc\": 0.949, \"tta_loss\": 0.97357724609375, \"tta_time\": 0.1826382819999708, \"valid_acc\": 0.9451, \"valid_loss\": 0.982364697265625, \"valid_time\": 0.1826382819999708, \"experiment\": \"final\"}, {\"epoch\": 24, \"run\": 5, \"total time\": 61.78838591801468, \"train_acc\": 0.9554002899484536, \"train_loss\": 0.973067352452229, \"train_time\": 2.5750719900024706, \"tta_acc\": 0.9506, \"tta_loss\": 0.97138369140625, \"tta_time\": 0.1822957199983648, \"valid_acc\": 0.9461, \"valid_loss\": 0.98204267578125, \"valid_time\": 0.1822957199983648, \"experiment\": \"final\"}]}};\n",
              "      var embedOpt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "      function showError(el, error){\n",
              "          el.innerHTML = ('<div class=\"error\" style=\"color:red;\">'\n",
              "                          + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                          + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                          + \"See the javascript console for the full traceback.</p>\"\n",
              "                          + '</div>');\n",
              "          throw error;\n",
              "      }\n",
              "      const el = document.getElementById('altair-viz');\n",
              "      vegaEmbed(\"#altair-viz\", spec, embedOpt)\n",
              "        .catch(error => showError(el, error));\n",
              "    })(vegaEmbed);\n",
              "\n",
              "  </script>\n",
              "</body>\n",
              "</html>"
            ],
            "text/plain": [
              "alt.HConcatChart(...)"
            ]
          },
          "execution_count": 21,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scale = alt.Scale(zero=False)\n",
        "c = alt.Chart(data).encode(x=alt.X('epoch', scale=scale), color='experiment')\n",
        "\n",
        "c1=(c.mark_point().encode(y=alt.Y('valid_acc',scale=scale, axis=alt.Axis(title='valid acc'))) + \n",
        "c.mark_line().encode(y=alt.Y('mean(valid_acc)',scale=scale)))\n",
        "\n",
        "c2=(c.mark_point().encode(y=alt.Y('tta_acc',scale=scale, axis=alt.Axis(title='valid acc (tta)'))) + \n",
        "c.mark_line().encode(y=alt.Y('mean(tta_acc)',scale=scale)))\n",
        "\n",
        "c1 | c2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tExlB_cQiADd"
      },
      "source": [
        "Despite the lack of tuning of the various extra hyperparameters of the final training setup for longer runs, it appears to maintain a healthy lead over the baseline even out to 100 epochs of training and approximate convergence. The final TTA accuracy of our little 9-layer ResNet at 80 epochs is 96.1% even though we never optimised anything for training above 94% accuracy! We could presumably go quite a bit higher with proper hyperparameter optimisation. \n",
        "\n",
        "It appears that 96% accuracy is reached in about 70 epochs and 3 minutes of total training time, answering a question that I've been asked several times by people who (perhaps rightly) believe that the 94% threshold of DAWNBench is too low. Note that we have made almost no attempt to optimise the 96% time and we would expect it to come down considerably from here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXPU8vZJlgpC"
      },
      "source": [
        "### Final thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9qabx_UlkoN"
      },
      "source": [
        "Thanks to everyone who contributed to, supported or provided feedback on the project. Special thanks to Sam Davis, to Thomas Read for his work last summer on what became the post on weight decay and to everyone at Myrtle.\n",
        "\n",
        "It has been tremendous fun working on this project, exploring dynamics of neural network training and extending the work of others to bring training times to a level where rapid experimentation becomes possible. I hope that the reader will find this useful in their work and believe that training times have a long way to fall yet (or accuracies improve if that's your thing!) through further algorithmic developments.\n",
        "\n",
        "At the outset of the series I half joked that if we could achieve 100% compute efficiency, training should take 40s. I would have been surprised to find that target surpassed by the end of the series with compute efficiency little better than it ever was! There is much scope for improvement on that front as well."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JZ_zmZhTyOSN",
        "ZiaIqUQKWHgT",
        "5HShJN2xh22e"
      ],
      "name": "bag-of-tricks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
